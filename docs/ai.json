{
  "vercel_ai_sdk_v6": {
    "foundations": [
      {
        "title": "Overview",
        "title_citation": "https://ai-sdk.dev/docs/foundations/overview",
        "content": "# [Overview](https://ai-sdk.dev/docs/foundations/overview\\#overview)\n\nThis page is a beginner-friendly introduction to high-level artificial\nintelligence (AI) concepts. To dive right into implementing the AI SDK, feel\nfree to skip ahead to our [quickstarts](https://ai-sdk.dev/docs/getting-started) or learn about\nour [supported models and providers](https://ai-sdk.dev/docs/foundations/providers-and-models).\n\nThe AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](https://ai-sdk.dev/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.\n\nFor example, here’s how you can generate text with various models using the AI SDK:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```typescript\n1import { generateText } from \"ai\";\n\n2\n\n3const { text } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: \"What is love?\",\n\n6});\n```\n\nLove is a complex and multifaceted emotion that can be felt and expressed in many different ways. It involves deep affection, care, compassion, and connection towards another person or thing.\n\nTo effectively leverage the AI SDK, it helps to familiarize yourself with the following concepts:\n\n## [Generative Artificial Intelligence](https://ai-sdk.dev/docs/foundations/overview\\#generative-artificial-intelligence)\n\n**Generative artificial intelligence** refers to models that predict and generate various types of outputs (such as text, images, or audio) based on what’s statistically likely, pulling from patterns they’ve learned from their training data. For example:\n\n- Given a photo, a generative model can generate a caption.\n- Given an audio file, a generative model can generate a transcription.\n- Given a text description, a generative model can generate an image.\n\n## [Large Language Models](https://ai-sdk.dev/docs/foundations/overview\\#large-language-models)\n\nA **large language model (LLM)** is a subset of generative models focused primarily on **text**. An LLM takes a sequence of words as input and aims to predict the most likely sequence to follow. It assigns probabilities to potential next sequences and then selects one. The model continues to generate sequences until it meets a specified stopping criterion.\n\nLLMs learn by training on massive collections of written text, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well.\n\nHowever, it's crucial to understand LLMs' limitations. When asked about less known or absent information, like the birthday of a personal relative, LLMs might \"hallucinate\" or make up information. It's essential to consider how well-represented the information you need is in the model.\n\n## [Embedding Models](https://ai-sdk.dev/docs/foundations/overview\\#embedding-models)\n\nAn **embedding model** is used to convert complex data (like words or images) into a dense vector (a list of numbers) representation, known as an embedding. Unlike generative models, embedding models do not generate new text or data. Instead, they provide representations of semantic and syntactic relationships between entities that can be used as input for other models or other natural language processing tasks.\n\nIn the next section, you will learn about the difference between models providers and models, and which ones are available in the AI SDK.\n\nOn this page\n\n[Overview](https://ai-sdk.dev/docs/foundations/overview#overview)\n\n[Generative Artificial Intelligence](https://ai-sdk.dev/docs/foundations/overview#generative-artificial-intelligence)\n\n[Large Language Models](https://ai-sdk.dev/docs/foundations/overview#large-language-models)\n\n[Embedding Models](https://ai-sdk.dev/docs/foundations/overview#embedding-models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/foundations/overview",
        "url": "https://ai-sdk.dev/docs/foundations/overview",
        "url_citation": "https://ai-sdk.dev/docs/foundations/overview"
      },
      {
        "title": "Providers and Models",
        "title_citation": "https://ai-sdk.dev/docs/foundations/providers-and-models",
        "content": "# [Providers and Models](https://ai-sdk.dev/docs/foundations/providers-and-models\\#providers-and-models)\n\nCompanies such as OpenAI and Anthropic (providers) offer access to a range of large language models (LLMs) with differing strengths and capabilities through their own APIs.\n\nEach provider typically has its own unique method for interfacing with their models, complicating the process of switching providers and increasing the risk of vendor lock-in.\n\nTo solve these challenges, AI SDK Core offers a standardized approach to interacting with LLMs through a [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) that abstracts differences between providers. This unified interface allows you to switch between providers with ease while using the same API for all providers.\n\nHere is an overview of the AI SDK Provider Architecture:\n\n![](https://ai-sdk.dev/_next/image?url=%2Fimages%2Fai-sdk-diagram.png&w=1920&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![](https://ai-sdk.dev/_next/image?url=%2Fimages%2Fai-sdk-diagram-dark.png&w=1920&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n## [AI SDK Providers](https://ai-sdk.dev/docs/foundations/providers-and-models\\#ai-sdk-providers)\n\nThe AI SDK comes with a wide range of providers that you can use to interact with different language models:\n\n- [xAI Grok Provider](https://ai-sdk.dev/providers/ai-sdk-providers/xai) (`@ai-sdk/xai`)\n- [OpenAI Provider](https://ai-sdk.dev/providers/ai-sdk-providers/openai) (`@ai-sdk/openai`)\n- [Azure OpenAI Provider](https://ai-sdk.dev/providers/ai-sdk-providers/azure) (`@ai-sdk/azure`)\n- [Anthropic Provider](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) (`@ai-sdk/anthropic`)\n- [Amazon Bedrock Provider](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock) (`@ai-sdk/amazon-bedrock`)\n- [Google Generative AI Provider](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai) (`@ai-sdk/google`)\n- [Google Vertex Provider](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex) (`@ai-sdk/google-vertex`)\n- [Mistral Provider](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) (`@ai-sdk/mistral`)\n- [Together.ai Provider](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai) (`@ai-sdk/togetherai`)\n- [Cohere Provider](https://ai-sdk.dev/providers/ai-sdk-providers/cohere) (`@ai-sdk/cohere`)\n- [Fireworks Provider](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks) (`@ai-sdk/fireworks`)\n- [DeepInfra Provider](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra) (`@ai-sdk/deepinfra`)\n- [DeepSeek Provider](https://ai-sdk.dev/providers/ai-sdk-providers/deepseek) (`@ai-sdk/deepseek`)\n- [Cerebras Provider](https://ai-sdk.dev/providers/ai-sdk-providers/cerebras) (`@ai-sdk/cerebras`)\n- [Groq Provider](https://ai-sdk.dev/providers/ai-sdk-providers/groq) (`@ai-sdk/groq`)\n- [Perplexity Provider](https://ai-sdk.dev/providers/ai-sdk-providers/perplexity) (`@ai-sdk/perplexity`)\n- [ElevenLabs Provider](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs) (`@ai-sdk/elevenlabs`)\n- [LMNT Provider](https://ai-sdk.dev/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\n- [Hume Provider](https://ai-sdk.dev/providers/ai-sdk-providers/hume) (`@ai-sdk/hume`)\n- [Rev.ai Provider](https://ai-sdk.dev/providers/ai-sdk-providers/revai) (`@ai-sdk/revai`)\n- [Deepgram Provider](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram) (`@ai-sdk/deepgram`)\n- [Gladia Provider](https://ai-sdk.dev/providers/ai-sdk-providers/gladia) (`@ai-sdk/gladia`)\n- [LMNT Provider](https://ai-sdk.dev/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\n- [AssemblyAI Provider](https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai) (`@ai-sdk/assemblyai`)\n- [Baseten Provider](https://ai-sdk.dev/providers/ai-sdk-providers/baseten) (`@ai-sdk/baseten`)\n\nYou can also use the [OpenAI Compatible provider](https://ai-sdk.dev/providers/openai-compatible-providers) with OpenAI-compatible APIs:\n\n- [LM Studio](https://ai-sdk.dev/providers/openai-compatible-providers/lmstudio)\n- [Heroku](https://ai-sdk.dev/providers/openai-compatible-providers/heroku)\n\nOur [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) is published as an open-source package, which you can use to create [custom providers](https://ai-sdk.dev/providers/community-providers/custom-providers).\n\nThe open-source community has created the following providers:\n\n- [Ollama Provider](https://ai-sdk.dev/providers/community-providers/ollama) (`ollama-ai-provider`)\n- [FriendliAI Provider](https://ai-sdk.dev/providers/community-providers/friendliai) (`@friendliai/ai-provider`)\n- [Portkey Provider](https://ai-sdk.dev/providers/community-providers/portkey) (`@portkey-ai/vercel-provider`)\n- [Cloudflare Workers AI Provider](https://ai-sdk.dev/providers/community-providers/cloudflare-workers-ai) (`workers-ai-provider`)\n- [OpenRouter Provider](https://ai-sdk.dev/providers/community-providers/openrouter) (`@openrouter/ai-sdk-provider`)\n- [Aihubmix Provider](https://ai-sdk.dev/providers/community-providers/aihubmix) (`@aihubmix/ai-sdk-provider`)\n- [Requesty Provider](https://ai-sdk.dev/providers/community-providers/requesty) (`@requesty/ai-sdk`)\n- [Crosshatch Provider](https://ai-sdk.dev/providers/community-providers/crosshatch) (`@crosshatch/ai-provider`)\n- [Mixedbread Provider](https://ai-sdk.dev/providers/community-providers/mixedbread) (`mixedbread-ai-provider`)\n- [Voyage AI Provider](https://ai-sdk.dev/providers/community-providers/voyage-ai) (`voyage-ai-provider`)\n- [Mem0 Provider](https://ai-sdk.dev/providers/community-providers/mem0)(`@mem0/vercel-ai-provider`)\n- [Letta Provider](https://ai-sdk.dev/providers/community-providers/letta)(`@letta-ai/vercel-ai-sdk-provider`)\n- [Supermemory Provider](https://ai-sdk.dev/providers/community-providers/supermemory)(`@supermemory/tools`)\n- [Spark Provider](https://ai-sdk.dev/providers/community-providers/spark) (`spark-ai-provider`)\n- [AnthropicVertex Provider](https://ai-sdk.dev/providers/community-providers/anthropic-vertex-ai) (`anthropic-vertex-ai`)\n- [LangDB Provider](https://ai-sdk.dev/providers/community-providers/langdb) (`@langdb/vercel-provider`)\n- [Dify Provider](https://ai-sdk.dev/providers/community-providers/dify) (`dify-ai-provider`)\n- [Sarvam Provider](https://ai-sdk.dev/providers/community-providers/sarvam) (`sarvam-ai-provider`)\n- [Claude Code Provider](https://ai-sdk.dev/providers/community-providers/claude-code) (`ai-sdk-provider-claude-code`)\n- [Built-in AI Provider](https://ai-sdk.dev/providers/community-providers/built-in-ai) (`built-in-ai`)\n- [Gemini CLI Provider](https://ai-sdk.dev/providers/community-providers/gemini-cli) (`ai-sdk-provider-gemini-cli`)\n- [A2A Provider](https://ai-sdk.dev/providers/community-providers/a2a) (`a2a-ai-provider`)\n- [SAP-AI Provider](https://ai-sdk.dev/providers/community-providers/sap-ai) (`@mymediset/sap-ai-provider`)\n- [AI/ML API Provider](https://ai-sdk.dev/providers/community-providers/aimlapi) (`@ai-ml.api/aimlapi-vercel-ai`)\n- [MCP Sampling Provider](https://ai-sdk.dev/providers/community-providers/mcp-sampling) (`@mcpc-tech/mcp-sampling-ai-provider`)\n- [ACP Provider](https://ai-sdk.dev/providers/community-providers/acp) (`@mcpc-tech/acp-ai-provider`)\n\n## [Self-Hosted Models](https://ai-sdk.dev/docs/foundations/providers-and-models\\#self-hosted-models)\n\nYou can access self-hosted models with the following providers:\n\n- [Ollama Provider](https://ai-sdk.dev/providers/community-providers/ollama)\n- [LM Studio](https://ai-sdk.dev/providers/openai-compatible-providers/lmstudio)\n- [Baseten](https://ai-sdk.dev/providers/ai-sdk-providers/baseten)\n- [Built-in AI](https://ai-sdk.dev/providers/community-providers/built-in-ai)\n\nAdditionally, any self-hosted provider that supports the OpenAI specification can be used with the [OpenAI Compatible Provider](https://ai-sdk.dev/providers/openai-compatible-providers).\n\n## [Model Capabilities](https://ai-sdk.dev/docs/foundations/providers-and-models\\#model-capabilities)\n\nThe AI providers support different language models with various capabilities.\nHere are the capabilities of popular models:\n\n| Provider | Model | Image Input | Object Generation | Tool Usage | Tool Streaming |\n| --- | --- | --- | --- | --- | --- |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-4` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-3` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-3-fast` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-3-mini` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-3-mini-fast` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-2-1212` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-2-vision-1212` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-beta` |  |  |  |  |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai) | `grok-vision-beta` |  |  |  |  |\n| [Vercel](https://ai-sdk.dev/providers/ai-sdk-providers/vercel) | `v0-1.0-md` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.2-pro` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.2-chat-latest` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.2` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5-mini` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5-nano` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.1-chat-latest` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.1-codex-mini` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.1-codex` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5.1` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5-codex` |  |  |  |  |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) | `gpt-5-chat-latest` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-opus-4-5` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-opus-4-1` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-opus-4-0` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-sonnet-4-0` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-3-7-sonnet-latest` |  |  |  |  |\n| [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic) | `claude-3-5-haiku-latest` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `pixtral-large-latest` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `mistral-large-latest` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `mistral-medium-latest` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `mistral-medium-2505` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `mistral-small-latest` |  |  |  |  |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral) | `pixtral-12b-2409` |  |  |  |  |\n| [Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai) | `gemini-2.0-flash-exp` |  |  |  |  |\n| [Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-flash` |  |  |  |  |\n| [Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-pro` |  |  |  |  |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex) | `gemini-2.0-flash-exp` |  |  |  |  |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex) | `gemini-1.5-flash` |  |  |  |  |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex) | `gemini-1.5-pro` |  |  |  |  |\n| [DeepSeek](https://ai-sdk.dev/providers/ai-sdk-providers/deepseek) | `deepseek-chat` |  |  |  |  |\n| [DeepSeek](https://ai-sdk.dev/providers/ai-sdk-providers/deepseek) | `deepseek-reasoner` |  |  |  |  |\n| [Cerebras](https://ai-sdk.dev/providers/ai-sdk-providers/cerebras) | `llama3.1-8b` |  |  |  |  |\n| [Cerebras](https://ai-sdk.dev/providers/ai-sdk-providers/cerebras) | `llama3.1-70b` |  |  |  |  |\n| [Cerebras](https://ai-sdk.dev/providers/ai-sdk-providers/cerebras) | `llama3.3-70b` |  |  |  |  |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq) | `meta-llama/llama-4-scout-17b-16e-instruct` |  |  |  |  |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq) | `llama-3.3-70b-versatile` |  |  |  |  |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq) | `llama-3.1-8b-instant` |  |  |  |  |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq) | `mixtral-8x7b-32768` |  |  |  |  |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq) | `gemma2-9b-it` |  |  |  |  |\n\nThis table is not exhaustive. Additional models can be found in the provider\ndocumentation pages and on the provider websites.\n\nOn this page\n\n[Providers and Models](https://ai-sdk.dev/docs/foundations/providers-and-models#providers-and-models)\n\n[AI SDK Providers](https://ai-sdk.dev/docs/foundations/providers-and-models#ai-sdk-providers)\n\n[Self-Hosted Models](https://ai-sdk.dev/docs/foundations/providers-and-models#self-hosted-models)\n\n[Model Capabilities](https://ai-sdk.dev/docs/foundations/providers-and-models#model-capabilities)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/foundations/providers-and-models",
        "url": "https://ai-sdk.dev/docs/foundations/providers-and-models",
        "url_citation": "https://ai-sdk.dev/docs/foundations/providers-and-models"
      },
      {
        "title": "Prompts",
        "title_citation": "https://ai-sdk.dev/docs/foundations/prompts",
        "content": "# [Prompts](https://ai-sdk.dev/docs/foundations/prompts\\#prompts)\n\nPrompts are instructions that you give a [large language model (LLM)](https://ai-sdk.dev/docs/foundations/overview#large-language-models) to tell it what to do.\nIt's like when you ask someone for directions; the clearer your question, the better the directions you'll get.\n\nMany LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types.\nWhile these interfaces are powerful, they can be hard to use and understand.\n\nIn order to simplify prompting, the AI SDK supports text, message, and system prompts.\n\n## [Text Prompts](https://ai-sdk.dev/docs/foundations/prompts\\#text-prompts)\n\nText prompts are strings.\nThey are ideal for simple generation use cases,\ne.g. repeatedly generating content for variants of the same prompt text.\n\nYou can set text prompts using the `prompt` property made available by AI SDK functions like [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) or [`generateObject`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object).\nYou can structure the text in any way and inject variables, e.g. using a template literal.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Invent a new holiday and describe its traditions.',\n\n4});\n```\n\nYou can also use template literals to provide dynamic data to your prompt.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt:\n\n4    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n\n5    `Please suggest the best tourist activities for me to do.`,\n\n6});\n```\n\n## [System Prompts](https://ai-sdk.dev/docs/foundations/prompts\\#system-prompts)\n\nSystem prompts are the initial set of instructions given to models that help guide and constrain the models' behaviors and responses.\nYou can set system prompts using the `system` property.\nSystem prompts work with both the `prompt` and the `messages` properties.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  system:\n\n4    `You help planning travel itineraries. ` +\n\n5    `Respond to the users' request with a list ` +\n\n6    `of the best stops to make in their destination.`,\n\n7  prompt:\n\n8    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\n\n9    `Please suggest the best tourist activities for me to do.`,\n\n10});\n```\n\nWhen you use a message prompt, you can also use system messages instead of a\nsystem prompt.\n\n## [Message Prompts](https://ai-sdk.dev/docs/foundations/prompts\\#message-prompts)\n\nA message prompt is an array of user, assistant, and tool messages.\nThey are great for chat interfaces and more complex, multi-modal prompts.\nYou can use the `messages` property to set message prompts.\n\nEach message has a `role` and a `content` property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'user', content: 'Hi!' },\\\n\\\n5    { role: 'assistant', content: 'Hello, how can I help?' },\\\n\\\n6    { role: 'user', content: 'Where can I buy the best Currywurst in Berlin?' },\\\n\\\n7  ],\n\n8});\n```\n\nInstead of sending a text in the `content` property, you can send an array of parts that includes a mix of text and other content parts.\n\nNot all language models support all message and content types. For example,\nsome models might not be capable of handling multi-modal inputs or tool\nmessages. [Learn more about the capabilities of select\\\\\nmodels](https://ai-sdk.dev/docs/foundations/providers-and-models#model-capabilities).\n\n### [Provider Options](https://ai-sdk.dev/docs/foundations/prompts\\#provider-options)\n\nYou can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.\n\n#### [Function Call Level](https://ai-sdk.dev/docs/foundations/prompts\\#function-call-level)\n\nFunctions like [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#provider-options) or [`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text#provider-options) accept a `providerOptions` property.\n\nAdding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.\n\n```ts\n1const { text } = await generateText({\n\n2  model: azure('your-deployment-name'),\n\n3  providerOptions: {\n\n4    openai: {\n\n5      reasoningEffort: 'low',\n\n6    },\n\n7  },\n\n8});\n```\n\n#### [Message Level](https://ai-sdk.dev/docs/foundations/prompts\\#message-level)\n\nFor granular control over applying provider options at the message level, you can pass `providerOptions` to the message object:\n\n```ts\n1import { ModelMessage } from 'ai';\n\n2\n\n3const messages: ModelMessage[] = [\\\n\\\n4  {\\\n\\\n5    role: 'system',\\\n\\\n6    content: 'Cached system message',\\\n\\\n7    providerOptions: {\\\n\\\n8      // Sets a cache control breakpoint on the system message\\\n\\\n9      anthropic: { cacheControl: { type: 'ephemeral' } },\\\n\\\n10    },\\\n\\\n11  },\\\n\\\n12];\n```\n\n#### [Message Part Level](https://ai-sdk.dev/docs/foundations/prompts\\#message-part-level)\n\nCertain provider-specific options require configuration at the message part level:\n\n```ts\n1import { ModelMessage } from 'ai';\n\n2\n\n3const messages: ModelMessage[] = [\\\n\\\n4  {\\\n\\\n5    role: 'user',\\\n\\\n6    content: [\\\n\\\n7      {\\\n\\\n8        type: 'text',\\\n\\\n9        text: 'Describe the image in detail.',\\\n\\\n10        providerOptions: {\\\n\\\n11          openai: { imageDetail: 'low' },\\\n\\\n12        },\\\n\\\n13      },\\\n\\\n14      {\\\n\\\n15        type: 'image',\\\n\\\n16        image:\\\n\\\n17          'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\\\n\\\n18        // Sets image detail configuration for image part:\\\n\\\n19        providerOptions: {\\\n\\\n20          openai: { imageDetail: 'low' },\\\n\\\n21        },\\\n\\\n22      },\\\n\\\n23    ],\\\n\\\n24  },\\\n\\\n25];\n```\n\nAI SDK UI hooks like [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) return\narrays of `UIMessage` objects, which do not support provider options. We\nrecommend using the\n[`convertToModelMessages`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/convert-to-core-messages)\nfunction to convert `UIMessage` objects to\n[`ModelMessage`](https://ai-sdk.dev/docs/reference/ai-sdk-core/model-message) objects before\napplying or appending message(s) or message parts with `providerOptions`.\n\n### [User Messages](https://ai-sdk.dev/docs/foundations/prompts\\#user-messages)\n\n#### [Text Parts](https://ai-sdk.dev/docs/foundations/prompts\\#text-parts)\n\nText content is the most common type of content. It is a string that is passed to the model.\n\nIf you only need to send text content in a message, the `content` property can be a string,\nbut you can also use it to send multiple content parts.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    {\\\n\\\n5      role: 'user',\\\n\\\n6      content: [\\\n\\\n7        {\\\n\\\n8          type: 'text',\\\n\\\n9          text: 'Where can I buy the best Currywurst in Berlin?',\\\n\\\n10        },\\\n\\\n11      ],\\\n\\\n12    },\\\n\\\n13  ],\n\n14});\n```\n\n#### [Image Parts](https://ai-sdk.dev/docs/foundations/prompts\\#image-parts)\n\nUser messages can include image parts. An image can be one of the following:\n\n- base64-encoded image:\n  - `string` with base-64 encoded content\n  - data URL `string`, e.g. `data:image/png;base64,...`\n- binary image:\n  - `ArrayBuffer`\n  - `Uint8Array`\n  - `Buffer`\n- URL:\n  - http(s) URL `string`, e.g. `https://example.com/image.png`\n  - `URL` object, e.g. `new URL('https://example.com/image.png')`\n\n##### [Example: Binary image (Buffer)](https://ai-sdk.dev/docs/foundations/prompts\\#example-binary-image-buffer)\n\n```ts\n1const result = await generateText({\n\n2  model,\n\n3  messages: [\\\n\\\n4    {\\\n\\\n5      role: 'user',\\\n\\\n6      content: [\\\n\\\n7        { type: 'text', text: 'Describe the image in detail.' },\\\n\\\n8        {\\\n\\\n9          type: 'image',\\\n\\\n10          image: fs.readFileSync('./data/comic-cat.png'),\\\n\\\n11        },\\\n\\\n12      ],\\\n\\\n13    },\\\n\\\n14  ],\n\n15});\n```\n\n##### [Example: Base-64 encoded image (string)](https://ai-sdk.dev/docs/foundations/prompts\\#example-base-64-encoded-image-string)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    {\\\n\\\n5      role: 'user',\\\n\\\n6      content: [\\\n\\\n7        { type: 'text', text: 'Describe the image in detail.' },\\\n\\\n8        {\\\n\\\n9          type: 'image',\\\n\\\n10          image: fs.readFileSync('./data/comic-cat.png').toString('base64'),\\\n\\\n11        },\\\n\\\n12      ],\\\n\\\n13    },\\\n\\\n14  ],\n\n15});\n```\n\n##### [Example: Image URL (string)](https://ai-sdk.dev/docs/foundations/prompts\\#example-image-url-string)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    {\\\n\\\n5      role: 'user',\\\n\\\n6      content: [\\\n\\\n7        { type: 'text', text: 'Describe the image in detail.' },\\\n\\\n8        {\\\n\\\n9          type: 'image',\\\n\\\n10          image:\\\n\\\n11            'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',\\\n\\\n12        },\\\n\\\n13      ],\\\n\\\n14    },\\\n\\\n15  ],\n\n16});\n```\n\n#### [File Parts](https://ai-sdk.dev/docs/foundations/prompts\\#file-parts)\n\nOnly a few providers and models currently support file parts: [Google\\\\\nGenerative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai), [Google\\\\\nVertex AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex),\n[OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) (for `wav` and `mp3` audio with\n`gpt-4o-audio-preview`), [Anthropic](https://ai-sdk.dev/providers/ai-sdk-providers/anthropic),\n[OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai) (for `pdf`).\n\nUser messages can include file parts. A file can be one of the following:\n\n- base64-encoded file:\n  - `string` with base-64 encoded content\n  - data URL `string`, e.g. `data:image/png;base64,...`\n- binary data:\n  - `ArrayBuffer`\n  - `Uint8Array`\n  - `Buffer`\n- URL:\n  - http(s) URL `string`, e.g. `https://example.com/some.pdf`\n  - `URL` object, e.g. `new URL('https://example.com/some.pdf')`\n\nYou need to specify the MIME type of the file you are sending.\n\n##### [Example: PDF file from Buffer](https://ai-sdk.dev/docs/foundations/prompts\\#example-pdf-file-from-buffer)\n\n```ts\n1import { google } from '@ai-sdk/google';\n\n2import { generateText } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: google('gemini-1.5-flash'),\n\n6  messages: [\\\n\\\n7    {\\\n\\\n8      role: 'user',\\\n\\\n9      content: [\\\n\\\n10        { type: 'text', text: 'What is the file about?' },\\\n\\\n11        {\\\n\\\n12          type: 'file',\\\n\\\n13          mediaType: 'application/pdf',\\\n\\\n14          data: fs.readFileSync('./data/example.pdf'),\\\n\\\n15          filename: 'example.pdf', // optional, not used by all providers\\\n\\\n16        },\\\n\\\n17      ],\\\n\\\n18    },\\\n\\\n19  ],\n\n20});\n```\n\n##### [Example: mp3 audio file from Buffer](https://ai-sdk.dev/docs/foundations/prompts\\#example-mp3-audio-file-from-buffer)\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateText } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: openai('gpt-4o-audio-preview'),\n\n6  messages: [\\\n\\\n7    {\\\n\\\n8      role: 'user',\\\n\\\n9      content: [\\\n\\\n10        { type: 'text', text: 'What is the audio saying?' },\\\n\\\n11        {\\\n\\\n12          type: 'file',\\\n\\\n13          mediaType: 'audio/mpeg',\\\n\\\n14          data: fs.readFileSync('./data/galileo.mp3'),\\\n\\\n15        },\\\n\\\n16      ],\\\n\\\n17    },\\\n\\\n18  ],\n\n19});\n```\n\n#### [Custom Download Function (Experimental)](https://ai-sdk.dev/docs/foundations/prompts\\#custom-download-function-experimental)\n\nYou can use custom download functions to implement throttling, retries, authentication, caching, and more.\n\nThe default download implementation automatically downloads files in parallel when they are not supported by the model.\n\nCustom download function can be passed via the `experimental_download` property:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  experimental_download: async (\n\n4    requestedDownloads: Array<{\n\n5      url: URL;\n\n6      isUrlSupportedByModel: boolean;\n\n7    }>,\n\n8  ): PromiseLike<\n\n9    Array<{\n\n10      data: Uint8Array;\n\n11      mediaType: string | undefined;\n\n12    } | null>\n\n13  > => {\n\n14    // ... download the files and return an array with similar order\n\n15  },\n\n16  messages: [\\\n\\\n17    {\\\n\\\n18      role: 'user',\\\n\\\n19      content: [\\\n\\\n20        {\\\n\\\n21          type: 'file',\\\n\\\n22          data: new URL('https://api.company.com/private/document.pdf'),\\\n\\\n23          mediaType: 'application/pdf',\\\n\\\n24        },\\\n\\\n25      ],\\\n\\\n26    },\\\n\\\n27  ],\n\n28});\n```\n\nThe `experimental_download` option is experimental and may change in future\nreleases.\n\n### [Assistant Messages](https://ai-sdk.dev/docs/foundations/prompts\\#assistant-messages)\n\nAssistant messages are messages that have a role of `assistant`.\nThey are typically previous responses from the assistant\nand can contain text, reasoning, and tool call parts.\n\n#### [Example: Assistant message with text content](https://ai-sdk.dev/docs/foundations/prompts\\#example-assistant-message-with-text-content)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'user', content: 'Hi!' },\\\n\\\n5    { role: 'assistant', content: 'Hello, how can I help?' },\\\n\\\n6  ],\n\n7});\n```\n\n#### [Example: Assistant message with text content in array](https://ai-sdk.dev/docs/foundations/prompts\\#example-assistant-message-with-text-content-in-array)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'user', content: 'Hi!' },\\\n\\\n5    {\\\n\\\n6      role: 'assistant',\\\n\\\n7      content: [{ type: 'text', text: 'Hello, how can I help?' }],\\\n\\\n8    },\\\n\\\n9  ],\n\n10});\n```\n\n#### [Example: Assistant message with tool call content](https://ai-sdk.dev/docs/foundations/prompts\\#example-assistant-message-with-tool-call-content)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'user', content: 'How many calories are in this block of cheese?' },\\\n\\\n5    {\\\n\\\n6      role: 'assistant',\\\n\\\n7      content: [\\\n\\\n8        {\\\n\\\n9          type: 'tool-call',\\\n\\\n10          toolCallId: '12345',\\\n\\\n11          toolName: 'get-nutrition-data',\\\n\\\n12          input: { cheese: 'Roquefort' },\\\n\\\n13        },\\\n\\\n14      ],\\\n\\\n15    },\\\n\\\n16  ],\n\n17});\n```\n\n#### [Example: Assistant message with file content](https://ai-sdk.dev/docs/foundations/prompts\\#example-assistant-message-with-file-content)\n\nThis content part is for model-generated files. Only a few models support\nthis, and only for file types that they can generate.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'user', content: 'Generate an image of a roquefort cheese!' },\\\n\\\n5    {\\\n\\\n6      role: 'assistant',\\\n\\\n7      content: [\\\n\\\n8        {\\\n\\\n9          type: 'file',\\\n\\\n10          mediaType: 'image/png',\\\n\\\n11          data: fs.readFileSync('./data/roquefort.jpg'),\\\n\\\n12        },\\\n\\\n13      ],\\\n\\\n14    },\\\n\\\n15  ],\n\n16});\n```\n\n### [Tool messages](https://ai-sdk.dev/docs/foundations/prompts\\#tool-messages)\n\n[Tools](https://ai-sdk.dev/docs/foundations/tools) (also known as function calling) are programs\nthat you can provide an LLM to extend its built-in functionality. This can be\nanything from calling an external API to calling functions within your UI.\nLearn more about Tools in [the next section](https://ai-sdk.dev/docs/foundations/tools).\n\nFor models that support [tool](https://ai-sdk.dev/docs/foundations/tools) calls, assistant messages can contain tool call parts, and tool messages can contain tool output parts.\nA single assistant message can call multiple tools, and a single tool message can contain multiple tool results.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    {\\\n\\\n5      role: 'user',\\\n\\\n6      content: [\\\n\\\n7        {\\\n\\\n8          type: 'text',\\\n\\\n9          text: 'How many calories are in this block of cheese?',\\\n\\\n10        },\\\n\\\n11        { type: 'image', image: fs.readFileSync('./data/roquefort.jpg') },\\\n\\\n12      ],\\\n\\\n13    },\\\n\\\n14    {\\\n\\\n15      role: 'assistant',\\\n\\\n16      content: [\\\n\\\n17        {\\\n\\\n18          type: 'tool-call',\\\n\\\n19          toolCallId: '12345',\\\n\\\n20          toolName: 'get-nutrition-data',\\\n\\\n21          input: { cheese: 'Roquefort' },\\\n\\\n22        },\\\n\\\n23        // there could be more tool calls here (parallel calling)\\\n\\\n24      ],\\\n\\\n25    },\\\n\\\n26    {\\\n\\\n27      role: 'tool',\\\n\\\n28      content: [\\\n\\\n29        {\\\n\\\n30          type: 'tool-result',\\\n\\\n31          toolCallId: '12345', // needs to match the tool call id\\\n\\\n32          toolName: 'get-nutrition-data',\\\n\\\n33          output: {\\\n\\\n34            type: 'json',\\\n\\\n35            value: {\\\n\\\n36              name: 'Cheese, roquefort',\\\n\\\n37              calories: 369,\\\n\\\n38              fat: 31,\\\n\\\n39              protein: 22,\\\n\\\n40            },\\\n\\\n41          },\\\n\\\n42        },\\\n\\\n43        // there could be more tool results here (parallel calling)\\\n\\\n44      ],\\\n\\\n45    },\\\n\\\n46  ],\n\n47});\n```\n\n#### [Multi-modal Tool Results](https://ai-sdk.dev/docs/foundations/prompts\\#multi-modal-tool-results)\n\nMulti-part tool results are experimental and only supported by Anthropic.\n\nTool results can be multi-part and multi-modal, e.g. a text and an image.\nYou can use the `experimental_content` property on tool parts to specify multi-part tool results.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    // ...\\\n\\\n5    {\\\n\\\n6      role: 'tool',\\\n\\\n7      content: [\\\n\\\n8        {\\\n\\\n9          type: 'tool-result',\\\n\\\n10          toolCallId: '12345', // needs to match the tool call id\\\n\\\n11          toolName: 'get-nutrition-data',\\\n\\\n12          // for models that do not support multi-part tool results,\\\n\\\n13          // you can include a regular output part:\\\n\\\n14          output: {\\\n\\\n15            type: 'json',\\\n\\\n16            value: {\\\n\\\n17              name: 'Cheese, roquefort',\\\n\\\n18              calories: 369,\\\n\\\n19              fat: 31,\\\n\\\n20              protein: 22,\\\n\\\n21            },\\\n\\\n22          },\\\n\\\n23        },\\\n\\\n24        {\\\n\\\n25          type: 'tool-result',\\\n\\\n26          toolCallId: '12345', // needs to match the tool call id\\\n\\\n27          toolName: 'get-nutrition-data',\\\n\\\n28          // for models that support multi-part tool results,\\\n\\\n29          // you can include a multi-part content part:\\\n\\\n30          output: {\\\n\\\n31            type: 'content',\\\n\\\n32            value: [\\\n\\\n33              {\\\n\\\n34                type: 'text',\\\n\\\n35                text: 'Here is an image of the nutrition data for the cheese:',\\\n\\\n36              },\\\n\\\n37              {\\\n\\\n38                type: 'media',\\\n\\\n39                data: fs\\\n\\\n40                  .readFileSync('./data/roquefort-nutrition-data.png')\\\n\\\n41                  .toString('base64'),\\\n\\\n42                mediaType: 'image/png',\\\n\\\n43              },\\\n\\\n44            ],\\\n\\\n45          },\\\n\\\n46        },\\\n\\\n47      ],\\\n\\\n48    },\\\n\\\n49  ],\n\n50});\n```\n\n### [System Messages](https://ai-sdk.dev/docs/foundations/prompts\\#system-messages)\n\nSystem messages are messages that are sent to the model before the user messages to guide the assistant's behavior.\nYou can alternatively use the `system` property.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  messages: [\\\n\\\n4    { role: 'system', content: 'You help planning travel itineraries.' },\\\n\\\n5    {\\\n\\\n6      role: 'user',\\\n\\\n7      content:\\\n\\\n8        'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',\\\n\\\n9    },\\\n\\\n10  ],\n\n11});\n```\n\nOn this page\n\n[Prompts](https://ai-sdk.dev/docs/foundations/prompts#prompts)\n\n[Text Prompts](https://ai-sdk.dev/docs/foundations/prompts#text-prompts)\n\n[System Prompts](https://ai-sdk.dev/docs/foundations/prompts#system-prompts)\n\n[Message Prompts](https://ai-sdk.dev/docs/foundations/prompts#message-prompts)\n\n[Provider Options](https://ai-sdk.dev/docs/foundations/prompts#provider-options)\n\n[Function Call Level](https://ai-sdk.dev/docs/foundations/prompts#function-call-level)\n\n[Message Level](https://ai-sdk.dev/docs/foundations/prompts#message-level)\n\n[Message Part Level](https://ai-sdk.dev/docs/foundations/prompts#message-part-level)\n\n[User Messages](https://ai-sdk.dev/docs/foundations/prompts#user-messages)\n\n[Text Parts](https://ai-sdk.dev/docs/foundations/prompts#text-parts)\n\n[Image Parts](https://ai-sdk.dev/docs/foundations/prompts#image-parts)\n\n[Example: Binary image (Buffer)](https://ai-sdk.dev/docs/foundations/prompts#example-binary-image-buffer)\n\n[Example: Base-64 encoded image (string)](https://ai-sdk.dev/docs/foundations/prompts#example-base-64-encoded-image-string)\n\n[Example: Image URL (string)](https://ai-sdk.dev/docs/foundations/prompts#example-image-url-string)\n\n[File Parts](https://ai-sdk.dev/docs/foundations/prompts#file-parts)\n\n[Example: PDF file from Buffer](https://ai-sdk.dev/docs/foundations/prompts#example-pdf-file-from-buffer)\n\n[Example: mp3 audio file from Buffer](https://ai-sdk.dev/docs/foundations/prompts#example-mp3-audio-file-from-buffer)\n\n[Custom Download Function (Experimental)](https://ai-sdk.dev/docs/foundations/prompts#custom-download-function-experimental)\n\n[Assistant Messages](https://ai-sdk.dev/docs/foundations/prompts#assistant-messages)\n\n[Example: Assistant message with text content](https://ai-sdk.dev/docs/foundations/prompts#example-assistant-message-with-text-content)\n\n[Example: Assistant message with text content in array](https://ai-sdk.dev/docs/foundations/prompts#example-assistant-message-with-text-content-in-array)\n\n[Example: Assistant message with tool call content](https://ai-sdk.dev/docs/foundations/prompts#example-assistant-message-with-tool-call-content)\n\n[Example: Assistant message with file content](https://ai-sdk.dev/docs/foundations/prompts#example-assistant-message-with-file-content)\n\n[Tool messages](https://ai-sdk.dev/docs/foundations/prompts#tool-messages)\n\n[Multi-modal Tool Results](https://ai-sdk.dev/docs/foundations/prompts#multi-modal-tool-results)\n\n[System Messages](https://ai-sdk.dev/docs/foundations/prompts#system-messages)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/foundations/prompts",
        "url": "https://ai-sdk.dev/docs/foundations/prompts",
        "url_citation": "https://ai-sdk.dev/docs/foundations/prompts"
      },
      {
        "title": "Tools",
        "title_citation": "https://ai-sdk.dev/docs/foundations/tools",
        "content": "# [Tools](https://ai-sdk.dev/docs/foundations/tools\\#tools)\n\nWhile [large language models (LLMs)](https://ai-sdk.dev/docs/foundations/overview#large-language-models) have incredible generation capabilities,\nthey struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).\n\nTools are actions that an LLM can invoke.\nThe results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, when you ask an LLM for the \"weather in London\", and there is a weather tool available, it could call a tool\nwith London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this\ninformation in its response.\n\n## [What is a tool?](https://ai-sdk.dev/docs/foundations/tools\\#what-is-a-tool)\n\nA tool is an object that can be called by the model to perform a specific task.\nYou can use tools with [`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text)\nand [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) by passing one or more tools to the `tools` parameter.\n\nA tool consists of three properties:\n\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\n- **`inputSchema`**: A [Zod schema](https://ai-sdk.dev/docs/foundations/tools#schema-specification-and-validation-with-zod) or a [JSON schema](https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema) that defines the input required for the tool to run. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\n- **`execute`**: An optional async function that is called with the arguments from the tool call.\n\n`streamUI` uses UI generator tools with a `generate` function that can return\nReact components.\n\nIf the LLM decides to use a tool, it will generate a tool call.\nTools with an `execute` function are run automatically when these calls are generated.\nThe output of the tool calls are returned using tool result objects.\n\nYou can automatically pass tool results back to the LLM\nusing [multi-step calls](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) with `streamText` and `generateText`.\n\n## [Schemas](https://ai-sdk.dev/docs/foundations/tools\\#schemas)\n\nSchemas are used to define and validate the [tool input](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling), tools outputs, and structured output generation.\n\nThe AI SDK supports the following schemas:\n\n- [Zod](https://zod.dev/) v3 and v4 directly or via [`zodSchema()`](https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema)\n- [Valibot](https://valibot.dev/) via [`valibotSchema()`](https://ai-sdk.dev/docs/reference/ai-sdk-core/valibot-schema) from `@ai-sdk/valibot`\n- [Standard JSON Schema](https://standardschema.dev/json-schema) compatible schemas\n- Raw JSON schemas via [`jsonSchema()`](https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema)\n\nYou can also use schemas for structured output generation with\n[`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text) and\n[`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) using the `output`\nsetting.\n\n## [Tool Packages](https://ai-sdk.dev/docs/foundations/tools\\#tool-packages)\n\nGiven tools are JavaScript objects, they can be packaged and distributed through npm like any other library. This makes it easy to share reusable tools across projects and with the community.\n\n### [Using Ready-Made Tool Packages](https://ai-sdk.dev/docs/foundations/tools\\#using-ready-made-tool-packages)\n\nInstall a tool package and import the tools you need:\n\n```bash\n1pnpm add some-tool-package\n```\n\nThen pass them directly to `generateText`, `streamText`, or your agent definition:\n\n```ts\n1import { generateText, stepCountIs } from 'ai';\n\n2import { searchTool } from 'some-tool-package';\n\n3\n\n4const { text } = await generateText({\n\n5  model: 'anthropic/claude-haiku-4.5',\n\n6  prompt: 'When was Vercel Ship AI?',\n\n7  tools: {\n\n8    webSearch: searchTool,\n\n9  },\n\n10  stopWhen: stepCountIs(10),\n\n11});\n```\n\n### [Publishing Your Own Tools](https://ai-sdk.dev/docs/foundations/tools\\#publishing-your-own-tools)\n\nYou can publish your own tool packages to npm for others to use. Simply export your tool objects from your package:\n\n```ts\n1// my-tools/index.ts\n\n2export const myTool = {\n\n3  description: 'A helpful tool',\n\n4  inputSchema: z.object({\n\n5    query: z.string(),\n\n6  }),\n\n7  execute: async ({ query }) => {\n\n8    // your tool logic\n\n9    return result;\n\n10  },\n\n11};\n```\n\nAnyone can then install and use your tools by importing them.\n\nTo get started, you can use the [AI SDK Tool Package Template](https://github.com/vercel-labs/ai-sdk-tool-as-package-template) which provides a ready-to-use starting point for publishing your own tools.\n\n## [Toolsets](https://ai-sdk.dev/docs/foundations/tools\\#toolsets)\n\nWhen you work with tools, you typically need a mix of application-specific tools and general-purpose tools. The community has created various toolsets and resources to help you build and use tools.\n\n### [Ready-to-Use Tool Packages](https://ai-sdk.dev/docs/foundations/tools\\#ready-to-use-tool-packages)\n\nThese packages provide pre-built tools you can install and use immediately:\n\n- **[@exalabs/ai-sdk](https://www.npmjs.com/package/@exalabs/ai-sdk)** \\- Web search tool that lets AI search the web and get real-time information.\n- **[@parallel-web/ai-sdk-tools](https://www.npmjs.com/package/@parallel-web/ai-sdk-tools)** \\- Web search and extract tools powered by Parallel Web API for real-time information and content extraction.\n- **[@perplexity-ai/ai-sdk](https://www.npmjs.com/package/@perplexity-ai/ai-sdk)** \\- Search the web with real-time results and advanced filtering powered by Perplexity's Search API.\n- **[@tavily/ai-sdk](https://www.npmjs.com/package/@tavily/ai-sdk)** \\- Search, extract, crawl, and map tools for enterprise-grade agents to explore the web in real-time.\n- **[Stripe agent tools](https://docs.stripe.com/agents?framework=vercel)** \\- Tools for interacting with Stripe.\n- **[StackOne ToolSet](https://docs.stackone.com/agents/typescript/frameworks/vercel-ai-sdk)** \\- Agentic integrations for hundreds of [enterprise SaaS](https://www.stackone.com/integrations) platforms.\n- **[agentic](https://docs.agentic.so/marketplace/ts-sdks/ai-sdk)** \\- A collection of 20+ tools that connect to external APIs such as [Exa](https://exa.ai/) or [E2B](https://e2b.dev/).\n- **[Amazon Bedrock AgentCore](https://github.com/aws/bedrock-agentcore-sdk-typescript)** \\- Fully managed AI agent services including [**Browser**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (a fast and secure cloud-based browser runtime to enable agents to interact with web applications, fill forms, navigate websites, and extract information) and [**Code Interpreter**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (an isolated sandbox environment for agents to execute code in Python, JavaScript, and TypeScript, enhancing accuracy and expanding ability to solve complex end-to-end tasks).\n- **[@airweave/vercel-ai-sdk](https://www.npmjs.com/package/@airweave/vercel-ai-sdk)** \\- Unified semantic search across 35+ data sources (Notion, Slack, Google Drive, databases, and more) for AI agents.\n- **[Composio](https://docs.composio.dev/providers/vercel)** \\- 250+ tools like GitHub, Gmail, Salesforce and [more](https://composio.dev/tools).\n- **[JigsawStack](http://www.jigsawstack.com/docs/integration/vercel)** \\- Over 30+ small custom fine-tuned models available for specific uses.\n- **[AI Tools Registry](https://ai-tools-registry.vercel.app/)** \\- A Shadcn-compatible tool definitions and components registry for the AI SDK.\n- **[Toolhouse](https://docs.toolhouse.ai/toolhouse/toolhouse-sdk/using-vercel-ai)** \\- AI function-calling in 3 lines of code for over 25 different actions.\n\n### [MCP Tools](https://ai-sdk.dev/docs/foundations/tools\\#mcp-tools)\n\nThese are pre-built tools available as MCP servers:\n\n- **[Smithery](https://smithery.ai/docs/integrations/vercel_ai_sdk)** \\- An open marketplace of 6,000+ MCPs, including [Browserbase](https://browserbase.com/) and [Exa](https://exa.ai/).\n- **[Pipedream](https://pipedream.com/docs/connect/mcp/ai-frameworks/vercel-ai-sdk)** \\- Developer toolkit that lets you easily add 3,000+ integrations to your app or AI agent.\n- **[Apify](https://docs.apify.com/platform/integrations/vercel-ai-sdk)** \\- Apify provides a [marketplace](https://apify.com/store) of thousands of tools for web scraping, data extraction, and browser automation.\n\n### [Tool Building Tutorials](https://ai-sdk.dev/docs/foundations/tools\\#tool-building-tutorials)\n\nThese tutorials and guides help you build your own tools that integrate with specific services:\n\n- **[browserbase](https://docs.browserbase.com/integrations/vercel/introduction#vercel-ai-integration)** \\- Tutorial for building browser tools that run a headless browser.\n- **[browserless](https://docs.browserless.io/ai-integrations/vercel-ai-sdk)** \\- Guide for integrating browser automation (self-hosted or cloud-based).\n- **[AI Tool Maker](https://github.com/nihaocami/ai-tool-maker)** \\- A CLI utility to generate AI SDK tools from OpenAPI specs.\n- **[Interlify](https://www.interlify.com/docs/integrate-with-vercel-ai)** \\- Guide for converting APIs into tools.\n- **[DeepAgent](https://deepagent.amardeep.space/docs/vercel-ai-sdk)** \\- A suite of 50+ AI tools and integrations, seamlessly connecting with APIs like Tavily, E2B, Airtable and [more](https://deepagent.amardeep.space/docs).\n\nDo you have open source tools or tool libraries that are compatible with the\nAI SDK? Please [file a pull request](https://github.com/vercel/ai/pulls) to\nadd them to this list.\n\n## [Learn more](https://ai-sdk.dev/docs/foundations/tools\\#learn-more)\n\nThe AI SDK Core [Tool Calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling)\nand [Agents](https://ai-sdk.dev/docs/foundations/agents) documentation has more information about tools and tool calling.\n\nOn this page\n\n[Tools](https://ai-sdk.dev/docs/foundations/tools#tools)\n\n[What is a tool?](https://ai-sdk.dev/docs/foundations/tools#what-is-a-tool)\n\n[Schemas](https://ai-sdk.dev/docs/foundations/tools#schemas)\n\n[Tool Packages](https://ai-sdk.dev/docs/foundations/tools#tool-packages)\n\n[Using Ready-Made Tool Packages](https://ai-sdk.dev/docs/foundations/tools#using-ready-made-tool-packages)\n\n[Publishing Your Own Tools](https://ai-sdk.dev/docs/foundations/tools#publishing-your-own-tools)\n\n[Toolsets](https://ai-sdk.dev/docs/foundations/tools#toolsets)\n\n[Ready-to-Use Tool Packages](https://ai-sdk.dev/docs/foundations/tools#ready-to-use-tool-packages)\n\n[MCP Tools](https://ai-sdk.dev/docs/foundations/tools#mcp-tools)\n\n[Tool Building Tutorials](https://ai-sdk.dev/docs/foundations/tools#tool-building-tutorials)\n\n[Learn more](https://ai-sdk.dev/docs/foundations/tools#learn-more)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/foundations/tools",
        "url": "https://ai-sdk.dev/docs/foundations/tools",
        "url_citation": "https://ai-sdk.dev/docs/foundations/tools"
      },
      {
        "title": "Streaming",
        "title_citation": "https://ai-sdk.dev/docs/foundations/streaming",
        "content": "# [Streaming](https://ai-sdk.dev/docs/foundations/streaming\\#streaming)\n\nStreaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.\n\n[Large language models (LLMs)](https://ai-sdk.dev/docs/foundations/overview#large-language-models) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you're likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by **displaying parts of the response as they become available**.\n\nBlocking UI\n\nBlocking responses wait until the full response is available before displaying it.\n\nStreaming UI\n\nStreaming responses can transmit parts of the response as they become available.\n\n## [Real-world Examples](https://ai-sdk.dev/docs/foundations/streaming\\#real-world-examples)\n\nHere are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting – the first uses a blocking UI, while the second uses a streaming UI.\n\n### [Blocking UI](https://ai-sdk.dev/docs/foundations/streaming\\#blocking-ui)\n\nCome up with the first 200 characters of the first book in the Harry Potter series.\n\nGenerate\n\n...\n\n### [Streaming UI](https://ai-sdk.dev/docs/foundations/streaming\\#streaming-ui)\n\nCome up with the first 200 characters of the first book in the Harry Potter series.\n\nGenerate\n\n...\n\nAs you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.\n\nWhile streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren't always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.\n\nHowever, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI's `gpt-4.1` in under 10 lines of code using the SDK's [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) function:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const { textStream } = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Write a poem about embedding models.',\n\n6});\n\n7\n\n8for await (const textPart of textStream) {\n\n9  console.log(textPart);\n\n10}\n```\n\nFor an introduction to streaming UIs and the AI SDK, check out our [Getting Started guides](https://ai-sdk.dev/docs/getting-started).\n\nOn this page\n\n[Streaming](https://ai-sdk.dev/docs/foundations/streaming#streaming)\n\n[Real-world Examples](https://ai-sdk.dev/docs/foundations/streaming#real-world-examples)\n\n[Blocking UI](https://ai-sdk.dev/docs/foundations/streaming#blocking-ui)\n\n[Streaming UI](https://ai-sdk.dev/docs/foundations/streaming#streaming-ui)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_7CeEo9ATV2QEHhySPY5jbbg9gcPn)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_7CeEo9ATV2QEHhySPY5jbbg9gcPn)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_7CeEo9ATV2QEHhySPY5jbbg9gcPn)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_7CeEo9ATV2QEHhySPY5jbbg9gcPn)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/foundations/streaming",
        "url": "https://ai-sdk.dev/docs/foundations/streaming",
        "url_citation": "https://ai-sdk.dev/docs/foundations/streaming"
      }
    ],
    "getting_started": [
      {
        "title": "Choosing a Provider",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/choosing-a-provider",
        "content": "# [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/choosing-a-provider\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const { text } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'What is love?',\n\n6});\n```\n\n## [AI Gateway](https://ai-sdk.dev/docs/getting-started/choosing-a-provider\\#ai-gateway)\n\nThe [Vercel AI Gateway](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) is the fastest way to get started with the AI SDK. Access models from OpenAI, Anthropic, Google, and other providers. Authenticate with [OIDC](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway#oidc-authentication-vercel-deployments) or an AI Gateway API key\n\n[Get an API Key](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai%2Fapi-keys%3Futm_source%3Dgateway-models-page%26showCreateKeyModal%3Dtrue&title=Get+Started+with+Vercel+AI+Gateway)\n\nAdd your API key to your environment:\n\n.env.local\n\n```env\n1AI_GATEWAY_API_KEY=your_api_key_here\n```\n\nThe AI Gateway is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration), so you can access models using a simple string:\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const { text } = await generateText({\n\n4  model: 'anthropic/claude-sonnet-4.5',\n\n5  prompt: 'What is love?',\n\n6});\n```\n\nYou can also explicitly import and use the gateway provider:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n## [Using Dedicated Providers](https://ai-sdk.dev/docs/getting-started/choosing-a-provider\\#using-dedicated-providers)\n\nYou can also use [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) provider packages directly. Install the package and create a provider instance. For example, to use Anthropic:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/anthropic\n```\n\n```ts\n1import { anthropic } from '@ai-sdk/anthropic';\n\n2\n\n3model: anthropic('claude-sonnet-4-5');\n```\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nSee [available providers](https://ai-sdk.dev/providers/ai-sdk-providers) for setup instructions for each provider.\n\n## [Custom Providers](https://ai-sdk.dev/docs/getting-started/choosing-a-provider\\#custom-providers)\n\nYou can build your own provider to integrate any service with the AI SDK. The AI SDK provides a [Language Model Specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v3) that ensures compatibility across providers.\n\n```ts\n1import { generateText } from 'ai';\n\n2import { yourProvider } from 'your-custom-provider';\n\n3\n\n4const { text } = await generateText({\n\n5  model: yourProvider('your-model-id'),\n\n6  prompt: 'What is love?',\n\n7});\n```\n\nSee [Writing a Custom Provider](https://ai-sdk.dev/providers/community-providers/custom-providers) for a complete guide.\n\nOn this page\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/choosing-a-provider#choosing-a-provider)\n\n[AI Gateway](https://ai-sdk.dev/docs/getting-started/choosing-a-provider#ai-gateway)\n\n[Using Dedicated Providers](https://ai-sdk.dev/docs/getting-started/choosing-a-provider#using-dedicated-providers)\n\n[Custom Providers](https://ai-sdk.dev/docs/getting-started/choosing-a-provider#custom-providers)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/choosing-a-provider",
        "url": "https://ai-sdk.dev/docs/getting-started/choosing-a-provider",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/choosing-a-provider"
      },
      {
        "title": "Navigating the Library",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
        "content": "# [Navigating the Library](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#navigating-the-library)\n\nThe AI SDK is a powerful toolkit for building AI applications. This page will help you pick the right tools for your requirements.\n\nLet’s start with a quick overview of the AI SDK, which is comprised of three parts:\n\n- **[AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core/overview):** A unified, provider agnostic API for generating text, structured objects, and tool calls with LLMs.\n- **[AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview):** A set of framework-agnostic hooks for building chat and generative user interfaces.\n- [AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc/overview): Stream generative user interfaces with React Server Components (RSC). Development is currently experimental and we recommend using [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview).\n\n## [Choosing the Right Tool for Your Environment](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#choosing-the-right-tool-for-your-environment)\n\nWhen deciding which part of the AI SDK to use, your first consideration should be the environment and existing stack you are working with. Different components of the SDK are tailored to specific frameworks and environments.\n\n| Library | Purpose | Environment Compatibility |\n| --- | --- | --- |\n| [AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core/overview) | Call any LLM with unified API (e.g. [generateText](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text) and [generateObject](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object)) | Any JS environment (e.g. Node.js, Deno, Browser) |\n| [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) | Build streaming chat and generative UIs (e.g. [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat)) | React & Next.js, Vue & Nuxt, Svelte & SvelteKit |\n| [AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc/overview) | Stream generative UIs from Server to Client (e.g. [streamUI](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/stream-ui)). Development is currently experimental and we recommend using [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview). | Any framework that supports React Server Components (e.g. Next.js) |\n\n## [Environment Compatibility](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#environment-compatibility)\n\nThese tools have been designed to work seamlessly with each other and it's likely that you will be using them together. Let's look at how you could decide which libraries to use based on your application environment, existing stack, and requirements.\n\nThe following table outlines AI SDK compatibility based on environment:\n\n| Environment | [AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core/overview) | [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) | [AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc/overview) |\n| --- | --- | --- | --- |\n| None / Node.js / Deno |  |  |  |\n| Vue / Nuxt |  |  |  |\n| Svelte / SvelteKit |  |  |  |\n| Next.js Pages Router |  |  |  |\n| Next.js App Router |  |  |  |\n\n## [When to use AI SDK UI](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#when-to-use-ai-sdk-ui)\n\nAI SDK UI provides a set of framework-agnostic hooks for quickly building **production-ready AI-native applications**. It offers:\n\n- Full support for streaming chat and client-side generative UI\n- Utilities for handling common AI interaction patterns (i.e. chat, completion, assistant)\n- Production-tested reliability and performance\n- Compatibility across popular frameworks\n\n## [AI SDK UI Framework Compatibility](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#ai-sdk-ui-framework-compatibility)\n\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), and [Vue.js](https://vuejs.org/). Here is a comparison of the supported functions across these frameworks:\n\n| Function | React | Svelte | Vue.js |\n| --- | --- | --- | --- |\n| [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) |  |  |  |\n| [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) tool calling |  |  |  |\n| [useCompletion](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion) |  |  |  |\n| [useObject](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) |  |  |  |\n\n[Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\nwelcome to implement missing features for non-React frameworks.\n\n## [When to use AI SDK RSC](https://ai-sdk.dev/docs/getting-started/navigating-the-library\\#when-to-use-ai-sdk-rsc)\n\nAI SDK RSC is currently experimental. We recommend using [AI SDK\\\\\nUI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\nRSC to UI, see our [migration guide](https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui).\n\n[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components)\n(RSCs) provide a new approach to building React applications that allow components\nto render on the server, fetch data directly, and stream the results to the client,\nreducing bundle size and improving performance. They also introduce a new way to\ncall server-side functions from anywhere in your application called [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations).\n\nAI SDK RSC provides a number of utilities that allow you to stream values and UI directly from the server to the client. However, **it's important to be aware of current limitations**:\n\n- **Cancellation**: currently, it is not possible to abort a stream using Server Actions. This will be improved in future releases of React and Next.js.\n- **Increased Data Transfer**: using [`createStreamableUI`](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-ui) can lead to quadratic data transfer (quadratic to the length of generated text). You can avoid this using [`createStreamableValue`](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-value) instead, and rendering the component client-side.\n- **Re-mounting Issue During Streaming**: when using `createStreamableUI`, components re-mount on `.done()`, causing [flickering](https://github.com/vercel/ai/issues/2232).\n\nGiven these limitations, **we recommend using [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) for production applications**.\n\nOn this page\n\n[Navigating the Library](https://ai-sdk.dev/docs/getting-started/navigating-the-library#navigating-the-library)\n\n[Choosing the Right Tool for Your Environment](https://ai-sdk.dev/docs/getting-started/navigating-the-library#choosing-the-right-tool-for-your-environment)\n\n[Environment Compatibility](https://ai-sdk.dev/docs/getting-started/navigating-the-library#environment-compatibility)\n\n[When to use AI SDK UI](https://ai-sdk.dev/docs/getting-started/navigating-the-library#when-to-use-ai-sdk-ui)\n\n[AI SDK UI Framework Compatibility](https://ai-sdk.dev/docs/getting-started/navigating-the-library#ai-sdk-ui-framework-compatibility)\n\n[When to use AI SDK RSC](https://ai-sdk.dev/docs/getting-started/navigating-the-library#when-to-use-ai-sdk-rsc)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
        "url": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/navigating-the-library"
      },
      {
        "title": "Next.js App Router Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
        "content": "# [Next.js App Router Quickstart](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#nextjs-app-router-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Create Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#create-your-application)\n\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\n\nBe sure to select yes when prompted to use the App Router and Tailwind CSS.\nIf you are looking for the Next.js Pages Router quickstart guide, you can\nfind it [here](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router).\n\n```\npnpm create next-app@latest my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\n### [Install dependencies](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK's React hooks. The AI SDK's [Vercel AI Gateway provider](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You'll also install `zod`, a schema validation library used for defining tool inputs.\n\nThis guide uses the Vercel AI Gateway provider so you can access hundreds of\nmodels from different providers with one API key, but you can switch to any\nprovider or model by installing its package. Check out available [AI SDK\\\\\nproviders](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\npnpmnpmyarnbun\n\n```\npnpm add ai @ai-sdk/react zod\n```\n\n### [Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#configure-your-ai-gateway-api-key)\n\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with Vercel AI Gateway.\n\n```\ntouch .env.local\n```\n\nEdit the `.env.local` file:\n\n.env.local\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK's Vercel AI Gateway Provider will default to using the\n`AI_GATEWAY_API_KEY` environment variable.\n\n## [Create a Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#create-a-route-handler)\n\nCreate a route handler, `app/api/chat/route.ts` and add the following code:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse();\n\n12}\n```\n\nLet's take a look at what is happening in this code:\n\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\n2. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\n3. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\n4. Finally, return the result to the client to stream the response.\n\nThis Route Handler creates a POST request endpoint at `/api/chat`.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n#### [Updating the global provider](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#updating-the-global-provider)\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nPick the approach that best matches how you want to manage providers across your application.\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#wire-up-the-ui)\n\nNow that you have a Route Handler that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage } = useChat();\n\n9  return (\n\n10    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n11      {messages.map(message => (\n\n12        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n13          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n14          {message.parts.map((part, i) => {\n\n15            switch (part.type) {\n\n16              case 'text':\n\n17                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n18            }\n\n19          })}\n\n20        </div>\n\n21      ))}\n\n22\n\n23      <form\n\n24        onSubmit={e => {\n\n25          e.preventDefault();\n\n26          sendMessage({ text: input });\n\n27          setInput('');\n\n28        }}\n\n29      >\n\n30        <input\n\n31          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n32          value={input}\n\n33          placeholder=\"Say something...\"\n\n34          onChange={e => setInput(e.currentTarget.value)}\n\n35        />\n\n36      </form>\n\n37    </div>\n\n38  );\n\n39}\n```\n\nMake sure you add the `\"use client\"` directive to the top of your file. This\nallows you to add interactivity with Javascript.\n\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm run dev\n```\n\nHead to your browser and open [http://localhost:3000](http://localhost:3000/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#update-your-route-handler)\n\nModify your `app/api/chat/route.ts` file to include the new weather tool:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages }: { messages: UIMessage[] } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages: await convertToModelMessages(messages),\n\n10    tools: {\n\n11      weather: tool({\n\n12        description: 'Get the weather in a location (fahrenheit)',\n\n13        inputSchema: z.object({\n\n14          location: z.string().describe('The location to get the weather for'),\n\n15        }),\n\n16        execute: async ({ location }) => {\n\n17          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n18          return {\n\n19            location,\n\n20            temperature,\n\n21          };\n\n22        },\n\n23      }),\n\n24    },\n\n25  });\n\n26\n\n27  return result.toUIMessageStreamResponse();\n\n28}\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#update-the-ui)\n\nTo display the tool invocation in your UI, update your `app/page.tsx` file:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage } = useChat();\n\n9  return (\n\n10    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n11      {messages.map(message => (\n\n12        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n13          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n14          {message.parts.map((part, i) => {\n\n15            switch (part.type) {\n\n16              case 'text':\n\n17                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n18              case 'tool-weather':\n\n19                return (\n\n20                  <pre key={`${message.id}-${i}`}>\n\n21                    {JSON.stringify(part, null, 2)}\n\n22                  </pre>\n\n23                );\n\n24            }\n\n25          })}\n\n26        </div>\n\n27      ))}\n\n28\n\n29      <form\n\n30        onSubmit={e => {\n\n31          e.preventDefault();\n\n32          sendMessage({ text: input });\n\n33          setInput('');\n\n34        }}\n\n35      >\n\n36        <input\n\n37          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n38          value={input}\n\n39          placeholder=\"Say something...\"\n\n40          onChange={e => setInput(e.currentTarget.value)}\n\n41        />\n\n42      </form>\n\n43    </div>\n\n44  );\n\n45}\n```\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#update-your-route-handler-1)\n\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31    },\n\n32  });\n\n33\n\n34  return result.toUIMessageStreamResponse();\n\n35}\n```\n\nIn this updated code:\n\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the model to use up to 5 \"steps\" for any given generation.\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the model's tool usage. This means we can also delete the `toolCall` and `toolResult``console.log` statements from the previous example.\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\n### [Add another tool](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#add-another-tool)\n\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31      convertFahrenheitToCelsius: tool({\n\n32        description: 'Convert a temperature in fahrenheit to celsius',\n\n33        inputSchema: z.object({\n\n34          temperature: z\n\n35            .number()\n\n36            .describe('The temperature in fahrenheit to convert'),\n\n37        }),\n\n38        execute: async ({ temperature }) => {\n\n39          const celsius = Math.round((temperature - 32) * (5 / 9));\n\n40          return {\n\n41            celsius,\n\n42          };\n\n43        },\n\n44      }),\n\n45    },\n\n46  });\n\n47\n\n48  return result.toUIMessageStreamResponse();\n\n49}\n```\n\n### [Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#update-your-frontend)\n\nupdate your `app/page.tsx` file to render the new temperature conversion tool:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage } = useChat();\n\n9  return (\n\n10    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n11      {messages.map(message => (\n\n12        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n13          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n14          {message.parts.map((part, i) => {\n\n15            switch (part.type) {\n\n16              case 'text':\n\n17                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n18              case 'tool-weather':\n\n19              case 'tool-convertFahrenheitToCelsius':\n\n20                return (\n\n21                  <pre key={`${message.id}-${i}`}>\n\n22                    {JSON.stringify(part, null, 2)}\n\n23                  </pre>\n\n24                );\n\n25            }\n\n26          })}\n\n27        </div>\n\n28      ))}\n\n29\n\n30      <form\n\n31        onSubmit={e => {\n\n32          e.preventDefault();\n\n33          sendMessage({ text: input });\n\n34          setInput('');\n\n35        }}\n\n36      >\n\n37        <input\n\n38          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n39          value={input}\n\n40          placeholder=\"Say something...\"\n\n41          onChange={e => setInput(e.currentTarget.value)}\n\n42        />\n\n43      </form>\n\n44    </div>\n\n45  );\n\n46}\n```\n\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool output displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/nextjs-app-router\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[Next.js App Router Quickstart](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#nextjs-app-router-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#prerequisites)\n\n[Create Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#create-your-application)\n\n[Install dependencies](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#install-dependencies)\n\n[Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#configure-your-ai-gateway-api-key)\n\n[Create a Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#create-a-route-handler)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#using-other-providers)\n\n[Updating the global provider](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#updating-the-global-provider)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#wire-up-the-ui)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#enhance-your-chatbot-with-tools)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#update-your-route-handler)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#enabling-multi-step-tool-calls)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#update-your-route-handler-1)\n\n[Add another tool](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#add-another-tool)\n\n[Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#update-your-frontend)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/nextjs-app-router#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
        "url": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router"
      },
      {
        "title": "Next.js Pages Router Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
        "content": "# [Next.js Pages Router Quickstart](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#nextjs-pages-router-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Setup Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#setup-your-application)\n\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\n\nBe sure to select no when prompted to use the App Router. If you are looking\nfor the Next.js App Router quickstart guide, you can find it\n[here](https://ai-sdk.dev/docs/getting-started/nextjs-app-router).\n\n```\npnpm create next-app@latest my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\n### [Install dependencies](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK's React hooks. The AI SDK's [Vercel AI Gateway provider](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You'll also install `zod`, a schema validation library used for defining tool inputs.\n\nThis guide uses the Vercel AI Gateway provider so you can access hundreds of\nmodels from different providers with one API key, but you can switch to any\nprovider or model by installing its package. Check out available [AI SDK\\\\\nproviders](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\npnpmnpmyarnbun\n\n```\npnpm add ai @ai-sdk/react zod\n```\n\n### [Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#configure-your-ai-gateway-api-key)\n\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\n\n```\ntouch .env.local\n```\n\nEdit the `.env.local` file:\n\n.env.local\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK's Vercel AI Gateway Provider will default to using the\n`AI_GATEWAY_API_KEY` environment variable.\n\n## [Create a Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#create-a-route-handler)\n\nAs long as you are on Next.js 13+, you can use Route Handlers (using the App\nRouter) alongside the Pages Router. This is recommended to enable you to use\nthe Web APIs interface/signature and to better support streaming.\n\nCreate a Route Handler (`app/api/chat/route.ts`) and add the following code:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse();\n\n12}\n```\n\nLet's take a look at what is happening in this code:\n\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\n2. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\n3. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\n4. Finally, return the result to the client to stream the response.\n\nThis Route Handler creates a POST request endpoint at `/api/chat`.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n#### [Updating the global provider](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#updating-the-global-provider)\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nPick the approach that best matches how you want to manage providers across your application.\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#wire-up-the-ui)\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui) package abstract the complexity of a chat interface into one hook, [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your root page (`pages/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\n\npages/index.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { useState } from 'react';\n\n3\n\n4export default function Chat() {\n\n5  const [input, setInput] = useState('');\n\n6  const { messages, sendMessage } = useChat();\n\n7  return (\n\n8    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n9      {messages.map(message => (\n\n10        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n11          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n12          {message.parts.map((part, i) => {\n\n13            switch (part.type) {\n\n14              case 'text':\n\n15                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n16            }\n\n17          })}\n\n18        </div>\n\n19      ))}\n\n20\n\n21      <form\n\n22        onSubmit={e => {\n\n23          e.preventDefault();\n\n24          sendMessage({ text: input });\n\n25          setInput('');\n\n26        }}\n\n27      >\n\n28        <input\n\n29          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n30          value={input}\n\n31          placeholder=\"Say something...\"\n\n32          onChange={e => setInput(e.currentTarget.value)}\n\n33        />\n\n34      </form>\n\n35    </div>\n\n36  );\n\n37}\n```\n\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm run dev\n```\n\nHead to your browser and open [http://localhost:3000](http://localhost:3000/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#update-your-route-handler)\n\nLet's start by giving your chatbot a weather tool. Update your Route Handler (`app/api/chat/route.ts`):\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages }: { messages: UIMessage[] } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages: await convertToModelMessages(messages),\n\n10    tools: {\n\n11      weather: tool({\n\n12        description: 'Get the weather in a location (fahrenheit)',\n\n13        inputSchema: z.object({\n\n14          location: z.string().describe('The location to get the weather for'),\n\n15        }),\n\n16        execute: async ({ location }) => {\n\n17          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n18          return {\n\n19            location,\n\n20            temperature,\n\n21          };\n\n22        },\n\n23      }),\n\n24    },\n\n25  });\n\n26\n\n27  return result.toUIMessageStreamResponse();\n\n28}\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#update-the-ui)\n\nTo display the tool invocations in your UI, update your `pages/index.tsx` file:\n\npages/index.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { useState } from 'react';\n\n3\n\n4export default function Chat() {\n\n5  const [input, setInput] = useState('');\n\n6  const { messages, sendMessage } = useChat();\n\n7  return (\n\n8    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n9      {messages.map(message => (\n\n10        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n11          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n12          {message.parts.map((part, i) => {\n\n13            switch (part.type) {\n\n14              case 'text':\n\n15                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n16              case 'tool-weather':\n\n17                return (\n\n18                  <pre key={`${message.id}-${i}`}>\n\n19                    {JSON.stringify(part, null, 2)}\n\n20                  </pre>\n\n21                );\n\n22            }\n\n23          })}\n\n24        </div>\n\n25      ))}\n\n26\n\n27      <form\n\n28        onSubmit={e => {\n\n29          e.preventDefault();\n\n30          sendMessage({ text: input });\n\n31          setInput('');\n\n32        }}\n\n33      >\n\n34        <input\n\n35          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n36          value={input}\n\n37          placeholder=\"Say something...\"\n\n38          onChange={e => setInput(e.currentTarget.value)}\n\n39        />\n\n40      </form>\n\n41    </div>\n\n42  );\n\n43}\n```\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#update-your-route-handler-1)\n\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31    },\n\n32  });\n\n33\n\n34  return result.toUIMessageStreamResponse();\n\n35}\n```\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\n### [Add another tool](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#add-another-tool)\n\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31      convertFahrenheitToCelsius: tool({\n\n32        description: 'Convert a temperature in fahrenheit to celsius',\n\n33        inputSchema: z.object({\n\n34          temperature: z\n\n35            .number()\n\n36            .describe('The temperature in fahrenheit to convert'),\n\n37        }),\n\n38        execute: async ({ temperature }) => {\n\n39          const celsius = Math.round((temperature - 32) * (5 / 9));\n\n40          return {\n\n41            celsius,\n\n42          };\n\n43        },\n\n44      }),\n\n45    },\n\n46  });\n\n47\n\n48  return result.toUIMessageStreamResponse();\n\n49}\n```\n\n### [Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#update-your-frontend)\n\nUpdate your `pages/index.tsx` file to render the new temperature conversion tool:\n\npages/index.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { useState } from 'react';\n\n3\n\n4export default function Chat() {\n\n5  const [input, setInput] = useState('');\n\n6  const { messages, sendMessage } = useChat();\n\n7  return (\n\n8    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n9      {messages.map(message => (\n\n10        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n11          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n12          {message.parts.map((part, i) => {\n\n13            switch (part.type) {\n\n14              case 'text':\n\n15                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n16              case 'tool-weather':\n\n17              case 'tool-convertFahrenheitToCelsius':\n\n18                return (\n\n19                  <pre key={`${message.id}-${i}`}>\n\n20                    {JSON.stringify(part, null, 2)}\n\n21                  </pre>\n\n22                );\n\n23            }\n\n24          })}\n\n25        </div>\n\n26      ))}\n\n27\n\n28      <form\n\n29        onSubmit={e => {\n\n30          e.preventDefault();\n\n31          sendMessage({ text: input });\n\n32          setInput('');\n\n33        }}\n\n34      >\n\n35        <input\n\n36          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n37          value={input}\n\n38          placeholder=\"Say something...\"\n\n39          onChange={e => setInput(e.currentTarget.value)}\n\n40        />\n\n41      </form>\n\n42    </div>\n\n43  );\n\n44}\n```\n\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool output displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[Next.js Pages Router Quickstart](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#nextjs-pages-router-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#prerequisites)\n\n[Setup Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#setup-your-application)\n\n[Install dependencies](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#install-dependencies)\n\n[Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#configure-your-ai-gateway-api-key)\n\n[Create a Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#create-a-route-handler)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#using-other-providers)\n\n[Updating the global provider](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#updating-the-global-provider)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#wire-up-the-ui)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#enhance-your-chatbot-with-tools)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#update-your-route-handler)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#enabling-multi-step-tool-calls)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#update-your-route-handler-1)\n\n[Add another tool](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#add-another-tool)\n\n[Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#update-your-frontend)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
        "url": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router"
      },
      {
        "title": "Svelte Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/svelte",
        "content": "# [Svelte Quickstart](https://ai-sdk.dev/docs/getting-started/svelte\\#svelte-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/svelte\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Set Up Your Application](https://ai-sdk.dev/docs/getting-started/svelte\\#set-up-your-application)\n\nStart by creating a new SvelteKit application. This command will create a new directory named `my-ai-app` and set up a basic SvelteKit application inside it.\n\n```\nnpx sv create my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\n### [Install Dependencies](https://ai-sdk.dev/docs/getting-started/svelte\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/svelte`, the AI package and AI SDK's Svelte bindings. The AI SDK's [Vercel AI Gateway provider](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You'll also install `zod`, a schema validation library used for defining tool inputs.\n\nThis guide uses the Vercel AI Gateway provider so you can access hundreds of\nmodels from different providers with one API key, but you can switch to any\nprovider or model by installing its package. Check out available [AI SDK\\\\\nproviders](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\npnpmnpmyarnbun\n\n```\npnpm add -D ai @ai-sdk/svelte zod\n```\n\n### [Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/svelte\\#configure-your-ai-gateway-api-key)\n\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\n\n```\ntouch .env.local\n```\n\nEdit the `.env.local` file:\n\n.env.local\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK's Vercel AI Gateway Provider will default to using the\n`AI_GATEWAY_API_KEY` environment variable. Vite does not automatically load\nenvironment variables onto `process.env`, so you'll need to import\n`AI_GATEWAY_API_KEY` from `$env/static/private` in your code (see below).\n\n## [Create an API route](https://ai-sdk.dev/docs/getting-started/svelte\\#create-an-api-route)\n\nCreate a SvelteKit Endpoint, `src/routes/api/chat/+server.ts` and add the following code:\n\nsrc/routes/api/chat/+server.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  type UIMessage,\n\n4  convertToModelMessages,\n\n5  createGateway,\n\n6} from 'ai';\n\n7\n\n8import { AI_GATEWAY_API_KEY } from '$env/static/private';\n\n9\n\n10const gateway = createGateway({\n\n11  apiKey: AI_GATEWAY_API_KEY,\n\n12});\n\n13\n\n14export async function POST({ request }) {\n\n15  const { messages }: { messages: UIMessage[] } = await request.json();\n\n16\n\n17  const result = streamText({\n\n18    model: gateway('anthropic/claude-sonnet-4.5'),\n\n19    messages: await convertToModelMessages(messages),\n\n20  });\n\n21\n\n22  return result.toUIMessageStreamResponse();\n\n23}\n```\n\nIf you see type errors with `AI_GATEWAY_API_KEY` or your `POST` function, run\nthe dev server.\n\nLet's take a look at what is happening in this code:\n\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\n2. Define a `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\n3. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\n4. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\n5. Return the result to the client to stream the response.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/svelte\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/svelte\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n#### [Updating the global provider](https://ai-sdk.dev/docs/getting-started/svelte\\#updating-the-global-provider)\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nPick the approach that best matches how you want to manage providers across your application.\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/svelte\\#wire-up-the-ui)\n\nNow that you have an API route that can query an LLM, it's time to set up your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one class, `Chat`.\nIts properties and API are largely the same as React's [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your root page (`src/routes/+page.svelte`) with the following code to show a list of chat messages and provide a user message input:\n\nsrc/routes/+page.svelte\n\n```jsx\n1<script lang=\"ts\">\n\n2  import { Chat } from '@ai-sdk/svelte';\n\n3\n\n4  let input = '';\n\n5  const chat = new Chat({});\n\n6\n\n7  function handleSubmit(event: SubmitEvent) {\n\n8    event.preventDefault();\n\n9    chat.sendMessage({ text: input });\n\n10    input = '';\n\n11  }\n\n12</script>\n\n13\n\n14<main>\n\n15  <ul>\n\n16    {#each chat.messages as message, messageIndex (messageIndex)}\n\n17      <li>\n\n18        <div>{message.role}</div>\n\n19        <div>\n\n20          {#each message.parts as part, partIndex (partIndex)}\n\n21            {#if part.type === 'text'}\n\n22              <div>{part.text}</div>\n\n23            {/if}\n\n24          {/each}\n\n25        </div>\n\n26      </li>\n\n27    {/each}\n\n28  </ul>\n\n29  <form onsubmit={handleSubmit}>\n\n30    <input bind:value={input} />\n\n31    <button type=\"submit\">Send</button>\n\n32  </form>\n\n33</main>\n```\n\nThis page utilizes the `Chat` class, which will, by default, use the `POST` route handler you created earlier. The class provides functions and state for handling user input and form submission. The `Chat` class provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/svelte\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm run dev\n```\n\nHead to your browser and open [http://localhost:5173](http://localhost:5173/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Svelte.\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/svelte\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\n### [Update Your API Route](https://ai-sdk.dev/docs/getting-started/svelte\\#update-your-api-route)\n\nModify your `src/routes/api/chat/+server.ts` file to include the new weather tool:\n\nsrc/routes/api/chat/+server.ts\n\n```tsx\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  type UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7  stepCountIs,\n\n8} from 'ai';\n\n9import { z } from 'zod';\n\n10\n\n11import { AI_GATEWAY_API_KEY } from '$env/static/private';\n\n12\n\n13const gateway = createGateway({\n\n14  apiKey: AI_GATEWAY_API_KEY,\n\n15});\n\n16\n\n17export async function POST({ request }) {\n\n18  const { messages }: { messages: UIMessage[] } = await request.json();\n\n19\n\n20  const result = streamText({\n\n21    model: gateway('anthropic/claude-sonnet-4.5'),\n\n22    messages: await convertToModelMessages(messages),\n\n23    tools: {\n\n24      weather: tool({\n\n25        description: 'Get the weather in a location (fahrenheit)',\n\n26        inputSchema: z.object({\n\n27          location: z.string().describe('The location to get the weather for'),\n\n28        }),\n\n29        execute: async ({ location }) => {\n\n30          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n31          return {\n\n32            location,\n\n33            temperature,\n\n34          };\n\n35        },\n\n36      }),\n\n37    },\n\n38  });\n\n39\n\n40  return result.toUIMessageStreamResponse();\n\n41}\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/svelte\\#update-the-ui)\n\nTo display the tool invocation in your UI, update your `src/routes/+page.svelte` file:\n\nsrc/routes/+page.svelte\n\n```jsx\n1<script lang=\"ts\">\n\n2  import { Chat } from '@ai-sdk/svelte';\n\n3\n\n4  let input = '';\n\n5  const chat = new Chat({});\n\n6\n\n7  function handleSubmit(event: SubmitEvent) {\n\n8    event.preventDefault();\n\n9    chat.sendMessage({ text: input });\n\n10    input = '';\n\n11  }\n\n12</script>\n\n13\n\n14<main>\n\n15  <ul>\n\n16    {#each chat.messages as message, messageIndex (messageIndex)}\n\n17      <li>\n\n18        <div>{message.role}</div>\n\n19        <div>\n\n20          {#each message.parts as part, partIndex (partIndex)}\n\n21            {#if part.type === 'text'}\n\n22              <div>{part.text}</div>\n\n23            {:else if part.type === 'tool-weather'}\n\n24              <pre>{JSON.stringify(part, null, 2)}</pre>\n\n25            {/if}\n\n26          {/each}\n\n27        </div>\n\n28      </li>\n\n29    {/each}\n\n30  </ul>\n\n31  <form onsubmit={handleSubmit}>\n\n32    <input bind:value={input} />\n\n33    <button type=\"submit\">Send</button>\n\n34  </form>\n\n35</main>\n```\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/svelte\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your API Route](https://ai-sdk.dev/docs/getting-started/svelte\\#update-your-api-route-1)\n\nModify your `src/routes/api/chat/+server.ts` file to include the `stopWhen` condition:\n\nsrc/routes/api/chat/+server.ts\n\n```ts\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  type UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7  stepCountIs,\n\n8} from 'ai';\n\n9import { z } from 'zod';\n\n10\n\n11import { AI_GATEWAY_API_KEY } from '$env/static/private';\n\n12\n\n13const gateway = createGateway({\n\n14  apiKey: AI_GATEWAY_API_KEY,\n\n15});\n\n16\n\n17export async function POST({ request }) {\n\n18  const { messages }: { messages: UIMessage[] } = await request.json();\n\n19\n\n20  const result = streamText({\n\n21    model: gateway('anthropic/claude-sonnet-4.5'),\n\n22    messages: await convertToModelMessages(messages),\n\n23    stopWhen: stepCountIs(5),\n\n24    tools: {\n\n25      weather: tool({\n\n26        description: 'Get the weather in a location (fahrenheit)',\n\n27        inputSchema: z.object({\n\n28          location: z.string().describe('The location to get the weather for'),\n\n29        }),\n\n30        execute: async ({ location }) => {\n\n31          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32          return {\n\n33            location,\n\n34            temperature,\n\n35          };\n\n36        },\n\n37      }),\n\n38    },\n\n39  });\n\n40\n\n41  return result.toUIMessageStreamResponse();\n\n42}\n```\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\n### [Add another tool](https://ai-sdk.dev/docs/getting-started/svelte\\#add-another-tool)\n\nUpdate your `src/routes/api/chat/+server.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nsrc/routes/api/chat/+server.ts\n\n```tsx\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  type UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7  stepCountIs,\n\n8} from 'ai';\n\n9import { z } from 'zod';\n\n10\n\n11import { AI_GATEWAY_API_KEY } from '$env/static/private';\n\n12\n\n13const gateway = createGateway({\n\n14  apiKey: AI_GATEWAY_API_KEY,\n\n15});\n\n16\n\n17export async function POST({ request }) {\n\n18  const { messages }: { messages: UIMessage[] } = await request.json();\n\n19\n\n20  const result = streamText({\n\n21    model: gateway('anthropic/claude-sonnet-4.5'),\n\n22    messages: await convertToModelMessages(messages),\n\n23    stopWhen: stepCountIs(5),\n\n24    tools: {\n\n25      weather: tool({\n\n26        description: 'Get the weather in a location (fahrenheit)',\n\n27        inputSchema: z.object({\n\n28          location: z.string().describe('The location to get the weather for'),\n\n29        }),\n\n30        execute: async ({ location }) => {\n\n31          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32          return {\n\n33            location,\n\n34            temperature,\n\n35          };\n\n36        },\n\n37      }),\n\n38      convertFahrenheitToCelsius: tool({\n\n39        description: 'Convert a temperature in fahrenheit to celsius',\n\n40        inputSchema: z.object({\n\n41          temperature: z\n\n42            .number()\n\n43            .describe('The temperature in fahrenheit to convert'),\n\n44        }),\n\n45        execute: async ({ temperature }) => {\n\n46          const celsius = Math.round((temperature - 32) * (5 / 9));\n\n47          return {\n\n48            celsius,\n\n49          };\n\n50        },\n\n51      }),\n\n52    },\n\n53  });\n\n54\n\n55  return result.toUIMessageStreamResponse();\n\n56}\n```\n\n### [Update Your Frontend](https://ai-sdk.dev/docs/getting-started/svelte\\#update-your-frontend)\n\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\n\nsrc/routes/+page.svelte\n\n```jsx\n1<script lang=\"ts\">\n\n2  import { Chat } from '@ai-sdk/svelte';\n\n3\n\n4  let input = '';\n\n5  const chat = new Chat({});\n\n6\n\n7  function handleSubmit(event: SubmitEvent) {\n\n8    event.preventDefault();\n\n9    chat.sendMessage({ text: input });\n\n10    input = '';\n\n11  }\n\n12</script>\n\n13\n\n14<main>\n\n15  <ul>\n\n16    {#each chat.messages as message, messageIndex (messageIndex)}\n\n17      <li>\n\n18        <div>{message.role}</div>\n\n19        <div>\n\n20          {#each message.parts as part, partIndex (partIndex)}\n\n21            {#if part.type === 'text'}\n\n22              <div>{part.text}</div>\n\n23            {:else if part.type === 'tool-weather' || part.type === 'tool-convertFahrenheitToCelsius'}\n\n24              <pre>{JSON.stringify(part, null, 2)}</pre>\n\n25            {/if}\n\n26          {/each}\n\n27        </div>\n\n28      </li>\n\n29    {/each}\n\n30  </ul>\n\n31  <form onsubmit={handleSubmit}>\n\n32    <input bind:value={input} />\n\n33    <button type=\"submit\">Send</button>\n\n34  </form>\n\n35</main>\n```\n\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool output displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [How does `@ai-sdk/svelte` differ from `@ai-sdk/react`?](https://ai-sdk.dev/docs/getting-started/svelte\\#how-does-ai-sdksvelte-differ-from-ai-sdkreact)\n\nThe surface-level difference is that Svelte uses classes to manage state, whereas React uses hooks, so `useChat` in React is `Chat` in Svelte. Other than that, there are a few things to keep in mind:\n\n### [1\\. Arguments to classes aren't reactive by default](https://ai-sdk.dev/docs/getting-started/svelte\\#1-arguments-to-classes-arent-reactive-by-default)\n\nUnlike in React, where hooks are rerun any time their containing component is invalidated, code in the `script` block of a Svelte component is only run once when the component is created.\nThis means that, if you want arguments to your class to be reactive, you need to make sure you pass a _reference_ into the class, rather than a value:\n\n```jsx\n1<script>\n\n2  import { Chat } from '@ai-sdk/svelte';\n\n3\n\n4  let { id } = $props();\n\n5\n\n6  // won't work; the class instance will be created once, `id` will be copied by value, and won't update when $props.id changes\n\n7  let chat = new Chat({ id });\n\n8\n\n9  // will work; passes `id` by reference, so `Chat` always has the latest value\n\n10  let chat = new Chat({\n\n11    get id() {\n\n12      return id;\n\n13    },\n\n14  });\n\n15</script>\n```\n\nKeep in mind that this normally doesn't matter; most parameters you'll pass into the Chat class are static (for example, you typically wouldn't expect your `onError` handler to change).\n\n### [2\\. You can't destructure class properties](https://ai-sdk.dev/docs/getting-started/svelte\\#2-you-cant-destructure-class-properties)\n\nIn vanilla JavaScript, destructuring class properties copies them by value and \"disconnects\" them from their class instance:\n\n```js\n1const classInstance = new Whatever();\n\n2classInstance.foo = 'bar';\n\n3const { foo } = classInstance;\n\n4classInstance.foo = 'baz';\n\n5\n\n6console.log(foo); // 'bar'\n```\n\nThe same is true of classes in Svelte:\n\n```jsx\n1<script>\n\n2  import { Chat } from '@ai-sdk/svelte';\n\n3\n\n4  const chat = new Chat({});\n\n5  let { messages } = chat;\n\n6\n\n7  chat.append({ content: 'Hello, world!', role: 'user' }).then(() => {\n\n8    console.log(messages); // []\n\n9    console.log(chat.messages); // [{ content: 'Hello, world!', role: 'user' }] (plus some other stuff)\n\n10  });\n\n11</script>\n```\n\n### [3\\. Instance synchronization requires context](https://ai-sdk.dev/docs/getting-started/svelte\\#3-instance-synchronization-requires-context)\n\nIn React, hook instances with the same `id` are synchronized -- so two instances of `useChat` will have the same `messages`, `status`, etc. if they have the same `id`.\nFor most use cases, you probably don't need this behavior -- but if you do, you can create a context in your root layout file using `createAIContext`:\n\n```jsx\n1<script>\n\n2  import { createAIContext } from '@ai-sdk/svelte';\n\n3\n\n4  let { children } = $props();\n\n5\n\n6  createAIContext();\n\n7  // all hooks created after this or in components that are children of this component\n\n8  // will have synchronized state\n\n9</script>\n\n10\n\n11{@render children()}\n```\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/svelte\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n- To learn more about Svelte, check out the [official documentation](https://svelte.dev/docs/svelte).\n\nOn this page\n\n[Svelte Quickstart](https://ai-sdk.dev/docs/getting-started/svelte#svelte-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/svelte#prerequisites)\n\n[Set Up Your Application](https://ai-sdk.dev/docs/getting-started/svelte#set-up-your-application)\n\n[Install Dependencies](https://ai-sdk.dev/docs/getting-started/svelte#install-dependencies)\n\n[Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/svelte#configure-your-ai-gateway-api-key)\n\n[Create an API route](https://ai-sdk.dev/docs/getting-started/svelte#create-an-api-route)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/svelte#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/svelte#using-other-providers)\n\n[Updating the global provider](https://ai-sdk.dev/docs/getting-started/svelte#updating-the-global-provider)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/svelte#wire-up-the-ui)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/svelte#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/svelte#enhance-your-chatbot-with-tools)\n\n[Update Your API Route](https://ai-sdk.dev/docs/getting-started/svelte#update-your-api-route)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/svelte#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/svelte#enabling-multi-step-tool-calls)\n\n[Update Your API Route](https://ai-sdk.dev/docs/getting-started/svelte#update-your-api-route-1)\n\n[Add another tool](https://ai-sdk.dev/docs/getting-started/svelte#add-another-tool)\n\n[Update Your Frontend](https://ai-sdk.dev/docs/getting-started/svelte#update-your-frontend)\n\n[How does @ai-sdk/svelte differ from @ai-sdk/react?](https://ai-sdk.dev/docs/getting-started/svelte#how-does-ai-sdksvelte-differ-from-ai-sdkreact)\n\n[1\\. Arguments to classes aren't reactive by default](https://ai-sdk.dev/docs/getting-started/svelte#1-arguments-to-classes-arent-reactive-by-default)\n\n[2\\. You can't destructure class properties](https://ai-sdk.dev/docs/getting-started/svelte#2-you-cant-destructure-class-properties)\n\n[3\\. Instance synchronization requires context](https://ai-sdk.dev/docs/getting-started/svelte#3-instance-synchronization-requires-context)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/svelte#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/svelte",
        "url": "https://ai-sdk.dev/docs/getting-started/svelte",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/svelte"
      },
      {
        "title": "Vue.js (Nuxt) Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/nuxt",
        "content": "# [Vue.js (Nuxt) Quickstart](https://ai-sdk.dev/docs/getting-started/nuxt\\#vuejs-nuxt-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/nuxt\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Setup Your Application](https://ai-sdk.dev/docs/getting-started/nuxt\\#setup-your-application)\n\nStart by creating a new Nuxt application. This command will create a new directory named `my-ai-app` and set up a basic Nuxt application inside it.\n\n```\npnpm create nuxt my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\n### [Install dependencies](https://ai-sdk.dev/docs/getting-started/nuxt\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/vue`. The Vercel AI Gateway provider ships with the `ai` package.\n\nThe AI SDK is designed to be a unified interface to interact with any large\nlanguage model. This means that you can change model and providers with just\none line of code! Learn more about [available providers](https://ai-sdk.dev/providers) and\n[building custom providers](https://ai-sdk.dev/providers/community-providers/custom-providers)\nin the [providers](https://ai-sdk.dev/providers) section.\n\npnpmnpmyarnbun\n\n```\npnpm add ai @ai-sdk/vue zod\n```\n\n### [Configure Vercel AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nuxt\\#configure-vercel-ai-gateway-api-key)\n\nCreate a `.env` file in your project root and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\n\n```\ntouch .env\n```\n\nEdit the `.env` file:\n\n.env\n\n```env\n1NUXT_AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key and configure the environment variable in `nuxt.config.ts`:\n\nnuxt.config.ts\n\n```ts\n1export default defineNuxtConfig({\n\n2  // rest of your nuxt config\n\n3  runtimeConfig: {\n\n4    aiGatewayApiKey: '',\n\n5  },\n\n6});\n```\n\nThis guide uses Nuxt's runtime config to manage the API key. The `NUXT_`\nprefix in the environment variable allows Nuxt to automatically load it into\nthe runtime config. While the AI Gateway Provider also supports a default\n`AI_GATEWAY_API_KEY` environment variable, this approach provides better\nintegration with Nuxt's configuration system.\n\n## [Create an API route](https://ai-sdk.dev/docs/getting-started/nuxt\\#create-an-api-route)\n\nCreate an API route, `server/api/chat.ts` and add the following code:\n\nserver/api/chat.ts\n\n```typescript\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  createGateway,\n\n6} from 'ai';\n\n7\n\n8export default defineLazyEventHandler(async () => {\n\n9  const apiKey = useRuntimeConfig().aiGatewayApiKey;\n\n10  if (!apiKey) throw new Error('Missing AI Gateway API key');\n\n11  const gateway = createGateway({\n\n12    apiKey: apiKey,\n\n13  });\n\n14\n\n15  return defineEventHandler(async (event: any) => {\n\n16    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n17\n\n18    const result = streamText({\n\n19      model: gateway('anthropic/claude-sonnet-4.5'),\n\n20      messages: await convertToModelMessages(messages),\n\n21    });\n\n22\n\n23    return result.toUIMessageStreamResponse();\n\n24  });\n\n25});\n```\n\nLet's take a look at what is happening in this code:\n\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\n2. Define an Event Handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\n3. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\n4. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\n5. Return the result to the client to stream the response.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nuxt\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/nuxt\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/nuxt\\#wire-up-the-ui)\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) package abstract the complexity of a chat interface into one hook, [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your root page (`pages/index.vue`) with the following code to show a list of chat messages and provide a user message input:\n\npages/index.vue\n\n```typescript\n1<script setup lang=\"ts\">\n\n2import { Chat } from \"@ai-sdk/vue\";\n\n3import { ref } from \"vue\";\n\n4\n\n5const input = ref(\"\");\n\n6const chat = new Chat({});\n\n7\n\n8const handleSubmit = (e: Event) => {\n\n9    e.preventDefault();\n\n10    chat.sendMessage({ text: input.value });\n\n11    input.value = \"\";\n\n12};\n\n13</script>\n\n14\n\n15<template>\n\n16    <div>\n\n17        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n\n18            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n\n19            <div\n\n20                v-for=\"(part, index) in m.parts\"\n\n21                :key=\"`${m.id}-${part.type}-${index}`\"\n\n22            >\n\n23                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n\n24            </div>\n\n25        </div>\n\n26\n\n27        <form @submit=\"handleSubmit\">\n\n28            <input v-model=\"input\" placeholder=\"Say something...\" />\n\n29        </form>\n\n30    </div>\n\n31</template>\n```\n\nIf your project has `app.vue` instead of `pages/index.vue`, delete the\n`app.vue` file and create a new `pages/index.vue` file with the code above.\n\nThis page utilizes the `useChat` hook, which will, by default, use the API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state (`ref`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/nuxt\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm run dev\n```\n\nHead to your browser and open [http://localhost:3000](http://localhost:3000/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Nuxt.\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nuxt\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\n### [Update Your API Route](https://ai-sdk.dev/docs/getting-started/nuxt\\#update-your-api-route)\n\nModify your `server/api/chat.ts` file to include the new weather tool:\n\nserver/api/chat.ts\n\n```typescript\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export default defineLazyEventHandler(async () => {\n\n11  const apiKey = useRuntimeConfig().aiGatewayApiKey;\n\n12  if (!apiKey) throw new Error('Missing AI Gateway API key');\n\n13  const gateway = createGateway({\n\n14    apiKey: apiKey,\n\n15  });\n\n16\n\n17  return defineEventHandler(async (event: any) => {\n\n18    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n19\n\n20    const result = streamText({\n\n21      model: gateway('anthropic/claude-sonnet-4.5'),\n\n22      messages: await convertToModelMessages(messages),\n\n23      tools: {\n\n24        weather: tool({\n\n25          description: 'Get the weather in a location (fahrenheit)',\n\n26          inputSchema: z.object({\n\n27            location: z\n\n28              .string()\n\n29              .describe('The location to get the weather for'),\n\n30          }),\n\n31          execute: async ({ location }) => {\n\n32            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n33            return {\n\n34              location,\n\n35              temperature,\n\n36            };\n\n37          },\n\n38        }),\n\n39      },\n\n40    });\n\n41\n\n42    return result.toUIMessageStreamResponse();\n\n43  });\n\n44});\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/nuxt\\#update-the-ui)\n\nTo display the tool invocation in your UI, update your `pages/index.vue` file:\n\npages/index.vue\n\n```typescript\n1<script setup lang=\"ts\">\n\n2import { Chat } from \"@ai-sdk/vue\";\n\n3import { ref } from \"vue\";\n\n4\n\n5const input = ref(\"\");\n\n6const chat = new Chat({});\n\n7\n\n8const handleSubmit = (e: Event) => {\n\n9    e.preventDefault();\n\n10    chat.sendMessage({ text: input.value });\n\n11    input.value = \"\";\n\n12};\n\n13</script>\n\n14\n\n15<template>\n\n16    <div>\n\n17        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n\n18            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n\n19            <div\n\n20                v-for=\"(part, index) in m.parts\"\n\n21                :key=\"`${m.id}-${part.type}-${index}`\"\n\n22            >\n\n23                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n\n24                <pre v-if=\"part.type === 'tool-weather'\">{{ JSON.stringify(part, null, 2) }}</pre>\n\n25            </div>\n\n26        </div>\n\n27\n\n28        <form @submit=\"handleSubmit\">\n\n29            <input v-model=\"input\" placeholder=\"Say something...\" />\n\n30        </form>\n\n31    </div>\n\n32</template>\n```\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nuxt\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your API Route](https://ai-sdk.dev/docs/getting-started/nuxt\\#update-your-api-route-1)\n\nModify your `server/api/chat.ts` file to include the `stopWhen` condition:\n\nserver/api/chat.ts\n\n```typescript\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7  stepCountIs,\n\n8} from 'ai';\n\n9import { z } from 'zod';\n\n10\n\n11export default defineLazyEventHandler(async () => {\n\n12  const apiKey = useRuntimeConfig().aiGatewayApiKey;\n\n13  if (!apiKey) throw new Error('Missing AI Gateway API key');\n\n14  const gateway = createGateway({\n\n15    apiKey: apiKey,\n\n16  });\n\n17\n\n18  return defineEventHandler(async (event: any) => {\n\n19    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n20\n\n21    const result = streamText({\n\n22      model: gateway('anthropic/claude-sonnet-4.5'),\n\n23      messages: await convertToModelMessages(messages),\n\n24      stopWhen: stepCountIs(5),\n\n25      tools: {\n\n26        weather: tool({\n\n27          description: 'Get the weather in a location (fahrenheit)',\n\n28          inputSchema: z.object({\n\n29            location: z\n\n30              .string()\n\n31              .describe('The location to get the weather for'),\n\n32          }),\n\n33          execute: async ({ location }) => {\n\n34            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n35            return {\n\n36              location,\n\n37              temperature,\n\n38            };\n\n39          },\n\n40        }),\n\n41      },\n\n42    });\n\n43\n\n44    return result.toUIMessageStreamResponse();\n\n45  });\n\n46});\n```\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\n### [Add another tool](https://ai-sdk.dev/docs/getting-started/nuxt\\#add-another-tool)\n\nUpdate your `server/api/chat.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nserver/api/chat.ts\n\n```typescript\n1import {\n\n2  createGateway,\n\n3  streamText,\n\n4  UIMessage,\n\n5  convertToModelMessages,\n\n6  tool,\n\n7  stepCountIs,\n\n8} from 'ai';\n\n9import { z } from 'zod';\n\n10\n\n11export default defineLazyEventHandler(async () => {\n\n12  const apiKey = useRuntimeConfig().aiGatewayApiKey;\n\n13  if (!apiKey) throw new Error('Missing AI Gateway API key');\n\n14  const gateway = createGateway({\n\n15    apiKey: apiKey,\n\n16  });\n\n17\n\n18  return defineEventHandler(async (event: any) => {\n\n19    const { messages }: { messages: UIMessage[] } = await readBody(event);\n\n20\n\n21    const result = streamText({\n\n22      model: gateway('anthropic/claude-sonnet-4.5'),\n\n23      messages: await convertToModelMessages(messages),\n\n24      stopWhen: stepCountIs(5),\n\n25      tools: {\n\n26        weather: tool({\n\n27          description: 'Get the weather in a location (fahrenheit)',\n\n28          inputSchema: z.object({\n\n29            location: z\n\n30              .string()\n\n31              .describe('The location to get the weather for'),\n\n32          }),\n\n33          execute: async ({ location }) => {\n\n34            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n35            return {\n\n36              location,\n\n37              temperature,\n\n38            };\n\n39          },\n\n40        }),\n\n41        convertFahrenheitToCelsius: tool({\n\n42          description: 'Convert a temperature in fahrenheit to celsius',\n\n43          inputSchema: z.object({\n\n44            temperature: z\n\n45              .number()\n\n46              .describe('The temperature in fahrenheit to convert'),\n\n47          }),\n\n48          execute: async ({ temperature }) => {\n\n49            const celsius = Math.round((temperature - 32) * (5 / 9));\n\n50            return {\n\n51              celsius,\n\n52            };\n\n53          },\n\n54        }),\n\n55      },\n\n56    });\n\n57\n\n58    return result.toUIMessageStreamResponse();\n\n59  });\n\n60});\n```\n\n### [Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nuxt\\#update-your-frontend)\n\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\n\npages/index.vue\n\n```typescript\n1<script setup lang=\"ts\">\n\n2import { Chat } from \"@ai-sdk/vue\";\n\n3import { ref } from \"vue\";\n\n4\n\n5const input = ref(\"\");\n\n6const chat = new Chat({});\n\n7\n\n8const handleSubmit = (e: Event) => {\n\n9    e.preventDefault();\n\n10    chat.sendMessage({ text: input.value });\n\n11    input.value = \"\";\n\n12};\n\n13</script>\n\n14\n\n15<template>\n\n16    <div>\n\n17        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\n\n18            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\n\n19            <div\n\n20                v-for=\"(part, index) in m.parts\"\n\n21                :key=\"`${m.id}-${part.type}-${index}`\"\n\n22            >\n\n23                <div v-if=\"part.type === 'text'\">{{ part.text }}</div>\n\n24                <pre\n\n25                    v-if=\"\n\n26                        part.type === 'tool-weather' ||\n\n27                        part.type === 'tool-convertFahrenheitToCelsius'\n\n28                    \"\n\n29                    >{{ JSON.stringify(part, null, 2) }}</pre\n\n30                >\n\n31            </div>\n\n32        </div>\n\n33\n\n34        <form @submit=\"handleSubmit\">\n\n35            <input v-model=\"input\" placeholder=\"Say something...\" />\n\n36        </form>\n\n37    </div>\n\n38</template>\n```\n\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool output displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/nuxt\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[Vue.js (Nuxt) Quickstart](https://ai-sdk.dev/docs/getting-started/nuxt#vuejs-nuxt-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/nuxt#prerequisites)\n\n[Setup Your Application](https://ai-sdk.dev/docs/getting-started/nuxt#setup-your-application)\n\n[Install dependencies](https://ai-sdk.dev/docs/getting-started/nuxt#install-dependencies)\n\n[Configure Vercel AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nuxt#configure-vercel-ai-gateway-api-key)\n\n[Create an API route](https://ai-sdk.dev/docs/getting-started/nuxt#create-an-api-route)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nuxt#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/nuxt#using-other-providers)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/nuxt#wire-up-the-ui)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/nuxt#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/nuxt#enhance-your-chatbot-with-tools)\n\n[Update Your API Route](https://ai-sdk.dev/docs/getting-started/nuxt#update-your-api-route)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/nuxt#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nuxt#enabling-multi-step-tool-calls)\n\n[Update Your API Route](https://ai-sdk.dev/docs/getting-started/nuxt#update-your-api-route-1)\n\n[Add another tool](https://ai-sdk.dev/docs/getting-started/nuxt#add-another-tool)\n\n[Update Your Frontend](https://ai-sdk.dev/docs/getting-started/nuxt#update-your-frontend)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/nuxt#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/nuxt",
        "url": "https://ai-sdk.dev/docs/getting-started/nuxt",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/nuxt"
      },
      {
        "title": "Node.js Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/nodejs",
        "content": "# [Node.js Quickstart](https://ai-sdk.dev/docs/getting-started/nodejs\\#nodejs-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/nodejs\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Setup Your Application](https://ai-sdk.dev/docs/getting-started/nodejs\\#setup-your-application)\n\nStart by creating a new directory using the `mkdir` command. Change into your new directory and then run the `pnpm init` command. This will create a `package.json` in your new directory.\n\n```bash\n1mkdir my-ai-app\n\n2cd my-ai-app\n\n3pnpm init\n```\n\n### [Install Dependencies](https://ai-sdk.dev/docs/getting-started/nodejs\\#install-dependencies)\n\nInstall `ai`, the AI SDK, along with other necessary dependencies.\n\nThe AI SDK is designed to be a unified interface to interact with any large\nlanguage model. This means that you can change model and providers with just\none line of code! Learn more about [available providers](https://ai-sdk.dev/providers) and\n[building custom providers](https://ai-sdk.dev/providers/community-providers/custom-providers)\nin the [providers](https://ai-sdk.dev/providers) section.\n\n```bash\n1pnpm add ai zod dotenv\n\n2pnpm add -D @types/node tsx typescript\n```\n\nThe `ai` package contains the AI SDK. You will use `zod` to define type-safe schemas that you will pass to the large language model (LLM). You will use `dotenv` to access environment variables (your Vercel AI Gateway key) within your application. There are also three development dependencies, installed with the `-D` flag, that are necessary to run your Typescript code.\n\n### [Configure Vercel AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nodejs\\#configure-vercel-ai-gateway-api-key)\n\nCreate a `.env` file in your project's root directory and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\n\n```\ntouch .env\n```\n\nEdit the `.env` file:\n\n.env\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK will use the `AI_GATEWAY_API_KEY` environment variable to\nauthenticate with Vercel AI Gateway.\n\n## [Create Your Application](https://ai-sdk.dev/docs/getting-started/nodejs\\#create-your-application)\n\nCreate an `index.ts` file in the root of your project and add the following code:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nindex.ts\n\n```ts\n1import { ModelMessage, streamText } from 'ai';\n\n2import 'dotenv/config';\n\n3import * as readline from 'node:readline/promises';\n\n4\n\n5const terminal = readline.createInterface({\n\n6  input: process.stdin,\n\n7  output: process.stdout,\n\n8});\n\n9\n\n10const messages: ModelMessage[] = [];\n\n11\n\n12async function main() {\n\n13  while (true) {\n\n14    const userInput = await terminal.question('You: ');\n\n15\n\n16    messages.push({ role: 'user', content: userInput });\n\n17\n\n18    const result = streamText({\n\n19      model: \"anthropic/claude-sonnet-4.5\",\n\n20      messages,\n\n21    });\n\n22\n\n23    let fullResponse = '';\n\n24    process.stdout.write('\\nAssistant: ');\n\n25    for await (const delta of result.textStream) {\n\n26      fullResponse += delta;\n\n27      process.stdout.write(delta);\n\n28    }\n\n29    process.stdout.write('\\n\\n');\n\n30\n\n31    messages.push({ role: 'assistant', content: fullResponse });\n\n32  }\n\n33}\n\n34\n\n35main().catch(console.error);\n```\n\nLet's take a look at what is happening in this code:\n\n1. Set up a readline interface to take input from the terminal, enabling interactive sessions directly from the command line.\n2. Initialize an array called `messages` to store the history of your conversation. This history allows the agent to maintain context in ongoing dialogues.\n3. In the `main` function:\n\n- Prompt for and capture user input, storing it in `userInput`.\n- Add user input to the `messages` array as a user message.\n- Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages`.\n- Iterate over the text stream returned by the `streamText` function (`result.textStream`) and print the contents of the stream to the terminal.\n- Add the assistant's response to the `messages` array.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/nodejs\\#running-your-application)\n\nWith that, you have built everything you need for your agent! To start your application, use the command:\n\n```\npnpm tsx index.ts\n```\n\nYou should see a prompt in your terminal. Test it out by entering a message and see the AI agent respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Node.js.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nodejs\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/nodejs\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n## [Enhance Your Agent with Tools](https://ai-sdk.dev/docs/getting-started/nodejs\\#enhance-your-agent-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the agent would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your agent by adding a simple weather tool.\n\n### [Update Your Application](https://ai-sdk.dev/docs/getting-started/nodejs\\#update-your-application)\n\nModify your `index.ts` file to include the new weather tool:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nindex.ts\n\n```ts\n1import { ModelMessage, streamText, tool } from 'ai';\n\n2import 'dotenv/config';\n\n3import { z } from 'zod';\n\n4import * as readline from 'node:readline/promises';\n\n5\n\n6const terminal = readline.createInterface({\n\n7  input: process.stdin,\n\n8  output: process.stdout,\n\n9});\n\n10\n\n11const messages: ModelMessage[] = [];\n\n12\n\n13async function main() {\n\n14  while (true) {\n\n15    const userInput = await terminal.question('You: ');\n\n16\n\n17    messages.push({ role: 'user', content: userInput });\n\n18\n\n19    const result = streamText({\n\n20      model: \"anthropic/claude-sonnet-4.5\",\n\n21      messages,\n\n22      tools: {\n\n23        weather: tool({\n\n24          description: 'Get the weather in a location (fahrenheit)',\n\n25          inputSchema: z.object({\n\n26            location: z\n\n27              .string()\n\n28              .describe('The location to get the weather for'),\n\n29          }),\n\n30          execute: async ({ location }) => {\n\n31            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32            return {\n\n33              location,\n\n34              temperature,\n\n35            };\n\n36          },\n\n37        }),\n\n38      },\n\n39    });\n\n40\n\n41    let fullResponse = '';\n\n42    process.stdout.write('\\nAssistant: ');\n\n43    for await (const delta of result.textStream) {\n\n44      fullResponse += delta;\n\n45      process.stdout.write(delta);\n\n46    }\n\n47    process.stdout.write('\\n\\n');\n\n48\n\n49    messages.push({ role: 'assistant', content: fullResponse });\n\n50  }\n\n51}\n\n52\n\n53main().catch(console.error);\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the agent understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The agent will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your agent can \"fetch\" weather information for any location the user asks about. When the agent determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and the results will be used by the agent to generate its response.\n\nTry asking something like \"What's the weather in New York?\" and see how the agent uses the new tool.\n\nNotice the blank \"assistant\" response? This is because instead of generating a text response, the agent generated a tool call. You can access the tool call and subsequent tool result in the `toolCall` and `toolResult` keys of the result object.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```typescript\n1import { ModelMessage, streamText, tool } from 'ai';\n\n2import 'dotenv/config';\n\n3import { z } from 'zod';\n\n4import * as readline from 'node:readline/promises';\n\n5\n\n6const terminal = readline.createInterface({\n\n7  input: process.stdin,\n\n8  output: process.stdout,\n\n9});\n\n10\n\n11const messages: ModelMessage[] = [];\n\n12\n\n13async function main() {\n\n14  while (true) {\n\n15    const userInput = await terminal.question('You: ');\n\n16\n\n17    messages.push({ role: 'user', content: userInput });\n\n18\n\n19    const result = streamText({\n\n20      model: \"anthropic/claude-sonnet-4.5\",\n\n21      messages,\n\n22      tools: {\n\n23        weather: tool({\n\n24          description: 'Get the weather in a location (fahrenheit)',\n\n25          inputSchema: z.object({\n\n26            location: z\n\n27              .string()\n\n28              .describe('The location to get the weather for'),\n\n29          }),\n\n30          execute: async ({ location }) => {\n\n31            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32            return {\n\n33              location,\n\n34              temperature,\n\n35            };\n\n36          },\n\n37        }),\n\n38      },\n\n39    });\n\n40\n\n41    let fullResponse = '';\n\n42    process.stdout.write('\\nAssistant: ');\n\n43    for await (const delta of result.textStream) {\n\n44      fullResponse += delta;\n\n45      process.stdout.write(delta);\n\n46    }\n\n47    process.stdout.write('\\n\\n');\n\n48\n\n49    console.log(await result.toolCalls);\n\n50    console.log(await result.toolResults);\n\n51    messages.push({ role: 'assistant', content: fullResponse });\n\n52  }\n\n53}\n\n54\n\n55main().catch(console.error);\n```\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nodejs\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool results are visible in the chat interface, the agent isn't using this information to answer your original query. This is because once the agent generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. This feature will automatically send tool results back to the agent to trigger an additional generation until the stopping condition you define is met. In this case, you want the agent to answer your question using the results from the weather tool.\n\n### [Update Your Application](https://ai-sdk.dev/docs/getting-started/nodejs\\#update-your-application-1)\n\nModify your `index.ts` file to configure stopping conditions with `stopWhen`:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nindex.ts\n\n```ts\n1import { ModelMessage, streamText, tool, stepCountIs } from 'ai';\n\n2import 'dotenv/config';\n\n3import { z } from 'zod';\n\n4import * as readline from 'node:readline/promises';\n\n5\n\n6const terminal = readline.createInterface({\n\n7  input: process.stdin,\n\n8  output: process.stdout,\n\n9});\n\n10\n\n11const messages: ModelMessage[] = [];\n\n12\n\n13async function main() {\n\n14  while (true) {\n\n15    const userInput = await terminal.question('You: ');\n\n16\n\n17    messages.push({ role: 'user', content: userInput });\n\n18\n\n19    const result = streamText({\n\n20      model: \"anthropic/claude-sonnet-4.5\",\n\n21      messages,\n\n22      tools: {\n\n23        weather: tool({\n\n24          description: 'Get the weather in a location (fahrenheit)',\n\n25          inputSchema: z.object({\n\n26            location: z\n\n27              .string()\n\n28              .describe('The location to get the weather for'),\n\n29          }),\n\n30          execute: async ({ location }) => {\n\n31            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32            return {\n\n33              location,\n\n34              temperature,\n\n35            };\n\n36          },\n\n37        }),\n\n38      },\n\n39      stopWhen: stepCountIs(5),\n\n40      onStepFinish: async ({ toolResults }) => {\n\n41        if (toolResults.length) {\n\n42          console.log(JSON.stringify(toolResults, null, 2));\n\n43        }\n\n44      },\n\n45    });\n\n46\n\n47    let fullResponse = '';\n\n48    process.stdout.write('\\nAssistant: ');\n\n49    for await (const delta of result.textStream) {\n\n50      fullResponse += delta;\n\n51      process.stdout.write(delta);\n\n52    }\n\n53    process.stdout.write('\\n\\n');\n\n54\n\n55    messages.push({ role: 'assistant', content: fullResponse });\n\n56  }\n\n57}\n\n58\n\n59main().catch(console.error);\n```\n\nIn this updated code:\n\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the agent to use up to 5 \"steps\" for any given generation.\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the agent's tool usage. This means we can also delete the `toolCall` and `toolResult``console.log` statements from the previous example.\n\nNow, when you ask about the weather in a location, you should see the agent using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the agent to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the agent to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\n### [Adding a second tool](https://ai-sdk.dev/docs/getting-started/nodejs\\#adding-a-second-tool)\n\nUpdate your `index.ts` file to add a new tool to convert the temperature from Celsius to Fahrenheit:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nindex.ts\n\n```ts\n1import { ModelMessage, streamText, tool, stepCountIs } from 'ai';\n\n2import 'dotenv/config';\n\n3import { z } from 'zod';\n\n4import * as readline from 'node:readline/promises';\n\n5\n\n6const terminal = readline.createInterface({\n\n7  input: process.stdin,\n\n8  output: process.stdout,\n\n9});\n\n10\n\n11const messages: ModelMessage[] = [];\n\n12\n\n13async function main() {\n\n14  while (true) {\n\n15    const userInput = await terminal.question('You: ');\n\n16\n\n17    messages.push({ role: 'user', content: userInput });\n\n18\n\n19    const result = streamText({\n\n20      model: \"anthropic/claude-sonnet-4.5\",\n\n21      messages,\n\n22      tools: {\n\n23        weather: tool({\n\n24          description: 'Get the weather in a location (fahrenheit)',\n\n25          inputSchema: z.object({\n\n26            location: z\n\n27              .string()\n\n28              .describe('The location to get the weather for'),\n\n29          }),\n\n30          execute: async ({ location }) => {\n\n31            const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n32            return {\n\n33              location,\n\n34              temperature,\n\n35            };\n\n36          },\n\n37        }),\n\n38        convertFahrenheitToCelsius: tool({\n\n39          description: 'Convert a temperature in fahrenheit to celsius',\n\n40          inputSchema: z.object({\n\n41            temperature: z\n\n42              .number()\n\n43              .describe('The temperature in fahrenheit to convert'),\n\n44          }),\n\n45          execute: async ({ temperature }) => {\n\n46            const celsius = Math.round((temperature - 32) * (5 / 9));\n\n47            return {\n\n48              celsius,\n\n49            };\n\n50          },\n\n51        }),\n\n52      },\n\n53      stopWhen: stepCountIs(5),\n\n54      onStepFinish: async ({ toolResults }) => {\n\n55        if (toolResults.length) {\n\n56          console.log(JSON.stringify(toolResults, null, 2));\n\n57        }\n\n58      },\n\n59    });\n\n60\n\n61    let fullResponse = '';\n\n62    process.stdout.write('\\nAssistant: ');\n\n63    for await (const delta of result.textStream) {\n\n64      fullResponse += delta;\n\n65      process.stdout.write(delta);\n\n66    }\n\n67    process.stdout.write('\\n\\n');\n\n68\n\n69    messages.push({ role: 'assistant', content: fullResponse });\n\n70  }\n\n71}\n\n72\n\n73main().catch(console.error);\n```\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The agent will call the weather tool for New York.\n2. You'll see the tool result logged.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The agent will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the agent to gather information and use it to provide more accurate and contextual responses, making your agent considerably more useful.\n\nThis example demonstrates how tools can expand your agent's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the agent to access and process real-world data in real-time and perform actions that interact with the outside world. Tools bridge the gap between the agent's knowledge cutoff and current information, while also enabling it to take meaningful actions beyond just generating text responses.\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/nodejs\\#where-to-next)\n\nYou've built an AI agent using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[Node.js Quickstart](https://ai-sdk.dev/docs/getting-started/nodejs#nodejs-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/nodejs#prerequisites)\n\n[Setup Your Application](https://ai-sdk.dev/docs/getting-started/nodejs#setup-your-application)\n\n[Install Dependencies](https://ai-sdk.dev/docs/getting-started/nodejs#install-dependencies)\n\n[Configure Vercel AI Gateway API key](https://ai-sdk.dev/docs/getting-started/nodejs#configure-vercel-ai-gateway-api-key)\n\n[Create Your Application](https://ai-sdk.dev/docs/getting-started/nodejs#create-your-application)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/nodejs#running-your-application)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/nodejs#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/nodejs#using-other-providers)\n\n[Enhance Your Agent with Tools](https://ai-sdk.dev/docs/getting-started/nodejs#enhance-your-agent-with-tools)\n\n[Update Your Application](https://ai-sdk.dev/docs/getting-started/nodejs#update-your-application)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/nodejs#enabling-multi-step-tool-calls)\n\n[Update Your Application](https://ai-sdk.dev/docs/getting-started/nodejs#update-your-application-1)\n\n[Adding a second tool](https://ai-sdk.dev/docs/getting-started/nodejs#adding-a-second-tool)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/nodejs#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/nodejs",
        "url": "https://ai-sdk.dev/docs/getting-started/nodejs",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/nodejs"
      },
      {
        "title": "Expo Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/expo",
        "content": "# [Expo Quickstart](https://ai-sdk.dev/docs/getting-started/expo\\#expo-quickstart)\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface with [Expo](https://expo.dev/). Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/expo\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Create Your Application](https://ai-sdk.dev/docs/getting-started/expo\\#create-your-application)\n\nStart by creating a new Expo application. This command will create a new directory named `my-ai-app` and set up a basic Expo application inside it.\n\n```\npnpm create expo-app@latest my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\nThis guide requires Expo 52 or higher.\n\n### [Install dependencies](https://ai-sdk.dev/docs/getting-started/expo\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK's React hooks. The AI SDK's [Vercel AI Gateway provider](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You'll also install `zod`, a schema validation library used for defining tool inputs.\n\nThis guide uses the Vercel AI Gateway provider so you can access hundreds of\nmodels from different providers with one API key, but you can switch to any\nprovider or model by installing its package. Check out available [AI SDK\\\\\nproviders](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\npnpmnpmyarnbun\n\n```\npnpm add ai @ai-sdk/react zod\n```\n\n### [Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/expo\\#configure-your-ai-gateway-api-key)\n\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\n\n```\ntouch .env.local\n```\n\nEdit the `.env.local` file:\n\n.env.local\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK's Vercel AI Gateway Provider will default to using the\n`AI_GATEWAY_API_KEY` environment variable.\n\n## [Create an API Route](https://ai-sdk.dev/docs/getting-started/expo\\#create-an-api-route)\n\nCreate a route handler, `app/api/chat+api.ts` and add the following code:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat+api.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse({\n\n12    headers: {\n\n13      'Content-Type': 'application/octet-stream',\n\n14      'Content-Encoding': 'none',\n\n15    },\n\n16  });\n\n17}\n```\n\nLet's take a look at what is happening in this code:\n\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.\n2. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `ai`) and `messages` (defined in step 1). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour.\n3. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\n4. Finally, return the result to the client to stream the response.\n\nThis API route creates a POST request endpoint at `/api/chat`.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/expo\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/expo\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n#### [Updating the global provider](https://ai-sdk.dev/docs/getting-started/expo\\#updating-the-global-provider)\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nPick the approach that best matches how you want to manage providers across your application.\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/expo\\#wire-up-the-ui)\n\nNow that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your root page (`app/(tabs)/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\n\napp/(tabs)/index.tsx\n\n```tsx\n1import { generateAPIUrl } from '@/utils';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { DefaultChatTransport } from 'ai';\n\n4import { fetch as expoFetch } from 'expo/fetch';\n\n5import { useState } from 'react';\n\n6import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n7\n\n8export default function App() {\n\n9  const [input, setInput] = useState('');\n\n10  const { messages, error, sendMessage } = useChat({\n\n11    transport: new DefaultChatTransport({\n\n12      fetch: expoFetch as unknown as typeof globalThis.fetch,\n\n13      api: generateAPIUrl('/api/chat'),\n\n14    }),\n\n15    onError: error => console.error(error, 'ERROR'),\n\n16  });\n\n17\n\n18  if (error) return <Text>{error.message}</Text>;\n\n19\n\n20  return (\n\n21    <SafeAreaView style={{ height: '100%' }}>\n\n22      <View\n\n23        style={{\n\n24          height: '95%',\n\n25          display: 'flex',\n\n26          flexDirection: 'column',\n\n27          paddingHorizontal: 8,\n\n28        }}\n\n29      >\n\n30        <ScrollView style={{ flex: 1 }}>\n\n31          {messages.map(m => (\n\n32            <View key={m.id} style={{ marginVertical: 8 }}>\n\n33              <View>\n\n34                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n\n35                {m.parts.map((part, i) => {\n\n36                  switch (part.type) {\n\n37                    case 'text':\n\n38                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n\n39                  }\n\n40                })}\n\n41              </View>\n\n42            </View>\n\n43          ))}\n\n44        </ScrollView>\n\n45\n\n46        <View style={{ marginTop: 8 }}>\n\n47          <TextInput\n\n48            style={{ backgroundColor: 'white', padding: 8 }}\n\n49            placeholder=\"Say something...\"\n\n50            value={input}\n\n51            onChange={e => setInput(e.nativeEvent.text)}\n\n52            onSubmitEditing={e => {\n\n53              e.preventDefault();\n\n54              sendMessage({ text: input });\n\n55              setInput('');\n\n56            }}\n\n57            autoFocus={true}\n\n58          />\n\n59        </View>\n\n60      </View>\n\n61    </SafeAreaView>\n\n62  );\n\n63}\n```\n\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\nYou use the expo/fetch function instead of the native node fetch to enable\nstreaming of chat responses. This requires Expo 52 or higher.\n\n### [Create the API URL Generator](https://ai-sdk.dev/docs/getting-started/expo\\#create-the-api-url-generator)\n\nBecause you're using expo/fetch for streaming responses instead of the native fetch function, you'll need an API URL generator to ensure you are using the correct base url and format depending on the client environment (e.g. web or mobile). Create a new file called `utils.ts` in the root of your project and add the following code:\n\nutils.ts\n\n```ts\n1import Constants from 'expo-constants';\n\n2\n\n3export const generateAPIUrl = (relativePath: string) => {\n\n4  const origin = Constants.experienceUrl.replace('exp://', 'http://');\n\n5\n\n6  const path = relativePath.startsWith('/') ? relativePath : `/${relativePath}`;\n\n7\n\n8  if (process.env.NODE_ENV === 'development') {\n\n9    return origin.concat(path);\n\n10  }\n\n11\n\n12  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {\n\n13    throw new Error(\n\n14      'EXPO_PUBLIC_API_BASE_URL environment variable is not defined',\n\n15    );\n\n16  }\n\n17\n\n18  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);\n\n19};\n```\n\nThis utility function handles URL generation for both development and production environments, ensuring your API calls work correctly across different devices and configurations.\n\nBefore deploying to production, you must set the `EXPO_PUBLIC_API_BASE_URL`\nenvironment variable in your production environment. This variable should\npoint to the base URL of your API server.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/expo\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm expo\n```\n\nHead to your browser and open [http://localhost:8081](http://localhost:8081/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Expo.\n\nIf you experience \"Property `structuredClone` doesn't exist\" errors on mobile,\nadd the [polyfills described below](https://ai-sdk.dev/docs/getting-started/expo#polyfills).\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/expo\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\n### [Update Your API route](https://ai-sdk.dev/docs/getting-started/expo\\#update-your-api-route)\n\nModify your `app/api/chat+api.ts` file to include the new weather tool:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat+api.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages }: { messages: UIMessage[] } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages: await convertToModelMessages(messages),\n\n10    tools: {\n\n11      weather: tool({\n\n12        description: 'Get the weather in a location (fahrenheit)',\n\n13        inputSchema: z.object({\n\n14          location: z.string().describe('The location to get the weather for'),\n\n15        }),\n\n16        execute: async ({ location }) => {\n\n17          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n18          return {\n\n19            location,\n\n20            temperature,\n\n21          };\n\n22        },\n\n23      }),\n\n24    },\n\n25  });\n\n26\n\n27  return result.toUIMessageStreamResponse({\n\n28    headers: {\n\n29      'Content-Type': 'application/octet-stream',\n\n30      'Content-Encoding': 'none',\n\n31    },\n\n32  });\n\n33}\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nYou may need to restart your development server for the changes to take\neffect.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/expo\\#update-the-ui)\n\nTo display the weather tool invocation in your UI, update your `app/(tabs)/index.tsx` file:\n\napp/(tabs)/index.tsx\n\n```tsx\n1import { generateAPIUrl } from '@/utils';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { DefaultChatTransport } from 'ai';\n\n4import { fetch as expoFetch } from 'expo/fetch';\n\n5import { useState } from 'react';\n\n6import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n7\n\n8export default function App() {\n\n9  const [input, setInput] = useState('');\n\n10  const { messages, error, sendMessage } = useChat({\n\n11    transport: new DefaultChatTransport({\n\n12      fetch: expoFetch as unknown as typeof globalThis.fetch,\n\n13      api: generateAPIUrl('/api/chat'),\n\n14    }),\n\n15    onError: error => console.error(error, 'ERROR'),\n\n16  });\n\n17\n\n18  if (error) return <Text>{error.message}</Text>;\n\n19\n\n20  return (\n\n21    <SafeAreaView style={{ height: '100%' }}>\n\n22      <View\n\n23        style={{\n\n24          height: '95%',\n\n25          display: 'flex',\n\n26          flexDirection: 'column',\n\n27          paddingHorizontal: 8,\n\n28        }}\n\n29      >\n\n30        <ScrollView style={{ flex: 1 }}>\n\n31          {messages.map(m => (\n\n32            <View key={m.id} style={{ marginVertical: 8 }}>\n\n33              <View>\n\n34                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n\n35                {m.parts.map((part, i) => {\n\n36                  switch (part.type) {\n\n37                    case 'text':\n\n38                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n\n39                    case 'tool-weather':\n\n40                      return (\n\n41                        <Text key={`${m.id}-${i}`}>\n\n42                          {JSON.stringify(part, null, 2)}\n\n43                        </Text>\n\n44                      );\n\n45                  }\n\n46                })}\n\n47              </View>\n\n48            </View>\n\n49          ))}\n\n50        </ScrollView>\n\n51\n\n52        <View style={{ marginTop: 8 }}>\n\n53          <TextInput\n\n54            style={{ backgroundColor: 'white', padding: 8 }}\n\n55            placeholder=\"Say something...\"\n\n56            value={input}\n\n57            onChange={e => setInput(e.nativeEvent.text)}\n\n58            onSubmitEditing={e => {\n\n59              e.preventDefault();\n\n60              sendMessage({ text: input });\n\n61              setInput('');\n\n62            }}\n\n63            autoFocus={true}\n\n64          />\n\n65        </View>\n\n66      </View>\n\n67    </SafeAreaView>\n\n68  );\n\n69}\n```\n\nYou may need to restart your development server for the changes to take\neffect.\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/expo\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your API Route](https://ai-sdk.dev/docs/getting-started/expo\\#update-your-api-route-1)\n\nModify your `app/api/chat+api.ts` file to include the `stopWhen` condition:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat+api.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31    },\n\n32  });\n\n33\n\n34  return result.toUIMessageStreamResponse({\n\n35    headers: {\n\n36      'Content-Type': 'application/octet-stream',\n\n37      'Content-Encoding': 'none',\n\n38    },\n\n39  });\n\n40}\n```\n\nYou may need to restart your development server for the changes to take\neffect.\n\nHead back to the Expo app and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\n\n### [Add More Tools](https://ai-sdk.dev/docs/getting-started/expo\\#add-more-tools)\n\nUpdate your `app/api/chat+api.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat+api.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages }: { messages: UIMessage[] } = await req.json();\n\n12\n\n13  const result = streamText({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    messages: await convertToModelMessages(messages),\n\n16    stopWhen: stepCountIs(5),\n\n17    tools: {\n\n18      weather: tool({\n\n19        description: 'Get the weather in a location (fahrenheit)',\n\n20        inputSchema: z.object({\n\n21          location: z.string().describe('The location to get the weather for'),\n\n22        }),\n\n23        execute: async ({ location }) => {\n\n24          const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n25          return {\n\n26            location,\n\n27            temperature,\n\n28          };\n\n29        },\n\n30      }),\n\n31      convertFahrenheitToCelsius: tool({\n\n32        description: 'Convert a temperature in fahrenheit to celsius',\n\n33        inputSchema: z.object({\n\n34          temperature: z\n\n35            .number()\n\n36            .describe('The temperature in fahrenheit to convert'),\n\n37        }),\n\n38        execute: async ({ temperature }) => {\n\n39          const celsius = Math.round((temperature - 32) * (5 / 9));\n\n40          return {\n\n41            celsius,\n\n42          };\n\n43        },\n\n44      }),\n\n45    },\n\n46  });\n\n47\n\n48  return result.toUIMessageStreamResponse({\n\n49    headers: {\n\n50      'Content-Type': 'application/octet-stream',\n\n51      'Content-Encoding': 'none',\n\n52    },\n\n53  });\n\n54}\n```\n\nYou may need to restart your development server for the changes to take\neffect.\n\n### [Update the UI for the new tool](https://ai-sdk.dev/docs/getting-started/expo\\#update-the-ui-for-the-new-tool)\n\nTo display the temperature conversion tool invocation in your UI, update your `app/(tabs)/index.tsx` file to handle the new tool part:\n\napp/(tabs)/index.tsx\n\n```tsx\n1import { generateAPIUrl } from '@/utils';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { DefaultChatTransport } from 'ai';\n\n4import { fetch as expoFetch } from 'expo/fetch';\n\n5import { useState } from 'react';\n\n6import { View, TextInput, ScrollView, Text, SafeAreaView } from 'react-native';\n\n7\n\n8export default function App() {\n\n9  const [input, setInput] = useState('');\n\n10  const { messages, error, sendMessage } = useChat({\n\n11    transport: new DefaultChatTransport({\n\n12      fetch: expoFetch as unknown as typeof globalThis.fetch,\n\n13      api: generateAPIUrl('/api/chat'),\n\n14    }),\n\n15    onError: error => console.error(error, 'ERROR'),\n\n16  });\n\n17\n\n18  if (error) return <Text>{error.message}</Text>;\n\n19\n\n20  return (\n\n21    <SafeAreaView style={{ height: '100%' }}>\n\n22      <View\n\n23        style={{\n\n24          height: '95%',\n\n25          display: 'flex',\n\n26          flexDirection: 'column',\n\n27          paddingHorizontal: 8,\n\n28        }}\n\n29      >\n\n30        <ScrollView style={{ flex: 1 }}>\n\n31          {messages.map(m => (\n\n32            <View key={m.id} style={{ marginVertical: 8 }}>\n\n33              <View>\n\n34                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\n\n35                {m.parts.map((part, i) => {\n\n36                  switch (part.type) {\n\n37                    case 'text':\n\n38                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\n\n39                    case 'tool-weather':\n\n40                    case 'tool-convertFahrenheitToCelsius':\n\n41                      return (\n\n42                        <Text key={`${m.id}-${i}`}>\n\n43                          {JSON.stringify(part, null, 2)}\n\n44                        </Text>\n\n45                      );\n\n46                  }\n\n47                })}\n\n48              </View>\n\n49            </View>\n\n50          ))}\n\n51        </ScrollView>\n\n52\n\n53        <View style={{ marginTop: 8 }}>\n\n54          <TextInput\n\n55            style={{ backgroundColor: 'white', padding: 8 }}\n\n56            placeholder=\"Say something...\"\n\n57            value={input}\n\n58            onChange={e => setInput(e.nativeEvent.text)}\n\n59            onSubmitEditing={e => {\n\n60              e.preventDefault();\n\n61              sendMessage({ text: input });\n\n62              setInput('');\n\n63            }}\n\n64            autoFocus={true}\n\n65          />\n\n66        </View>\n\n67      </View>\n\n68    </SafeAreaView>\n\n69  );\n\n70}\n```\n\nYou may need to restart your development server for the changes to take\neffect.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool result displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [Polyfills](https://ai-sdk.dev/docs/getting-started/expo\\#polyfills)\n\nSeveral functions that are internally used by the AI SDK might not available in the Expo runtime depending on your configuration and the target platform.\n\nFirst, install the following packages:\n\npnpmnpmyarnbun\n\n```\npnpm add @ungap/structured-clone @stardazed/streams-text-encoding\n```\n\nThen create a new file in the root of your project with the following polyfills:\n\npolyfills.js\n\n```ts\n1import { Platform } from 'react-native';\n\n2import structuredClone from '@ungap/structured-clone';\n\n3\n\n4if (Platform.OS !== 'web') {\n\n5  const setupPolyfills = async () => {\n\n6    const { polyfillGlobal } = await import(\n\n7      'react-native/Libraries/Utilities/PolyfillFunctions'\n\n8    );\n\n9\n\n10    const { TextEncoderStream, TextDecoderStream } = await import(\n\n11      '@stardazed/streams-text-encoding'\n\n12    );\n\n13\n\n14    if (!('structuredClone' in global)) {\n\n15      polyfillGlobal('structuredClone', () => structuredClone);\n\n16    }\n\n17\n\n18    polyfillGlobal('TextEncoderStream', () => TextEncoderStream);\n\n19    polyfillGlobal('TextDecoderStream', () => TextDecoderStream);\n\n20  };\n\n21\n\n22  setupPolyfills();\n\n23}\n\n24\n\n25export {};\n```\n\nFinally, import the polyfills in your root `_layout.tsx`:\n\n\\_layout.tsx\n\n```ts\n1import '@/polyfills';\n```\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/expo\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[Expo Quickstart](https://ai-sdk.dev/docs/getting-started/expo#expo-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/expo#prerequisites)\n\n[Create Your Application](https://ai-sdk.dev/docs/getting-started/expo#create-your-application)\n\n[Install dependencies](https://ai-sdk.dev/docs/getting-started/expo#install-dependencies)\n\n[Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/expo#configure-your-ai-gateway-api-key)\n\n[Create an API Route](https://ai-sdk.dev/docs/getting-started/expo#create-an-api-route)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/expo#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/expo#using-other-providers)\n\n[Updating the global provider](https://ai-sdk.dev/docs/getting-started/expo#updating-the-global-provider)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/expo#wire-up-the-ui)\n\n[Create the API URL Generator](https://ai-sdk.dev/docs/getting-started/expo#create-the-api-url-generator)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/expo#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/expo#enhance-your-chatbot-with-tools)\n\n[Update Your API route](https://ai-sdk.dev/docs/getting-started/expo#update-your-api-route)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/expo#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/expo#enabling-multi-step-tool-calls)\n\n[Update Your API Route](https://ai-sdk.dev/docs/getting-started/expo#update-your-api-route-1)\n\n[Add More Tools](https://ai-sdk.dev/docs/getting-started/expo#add-more-tools)\n\n[Update the UI for the new tool](https://ai-sdk.dev/docs/getting-started/expo#update-the-ui-for-the-new-tool)\n\n[Polyfills](https://ai-sdk.dev/docs/getting-started/expo#polyfills)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/expo#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/expo",
        "url": "https://ai-sdk.dev/docs/getting-started/expo",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/expo"
      },
      {
        "title": "TanStack Start Quickstart",
        "title_citation": "https://ai-sdk.dev/docs/getting-started/tanstack-start",
        "content": "# [TanStack Start Quickstart](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#tanstack-start-quickstart)\n\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\n\nIn this quickstart tutorial, you'll build a simple agent with a streaming chat user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\n\nIf you are unfamiliar with the concepts of [Prompt Engineering](https://ai-sdk.dev/docs/advanced/prompt-engineering) and [HTTP Streaming](https://ai-sdk.dev/docs/advanced/why-streaming), you can optionally read these documents first.\n\n## [Prerequisites](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#prerequisites)\n\nTo follow this quickstart, you'll need:\n\n- Node.js 18+ and pnpm installed on your local development machine.\n- A [Vercel AI Gateway](https://vercel.com/ai-gateway) API key.\n\nIf you haven't obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\n\n## [Create Your Application](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#create-your-application)\n\nStart by creating a new TanStack Start application. This command will create a new directory named `my-ai-app` and set up a basic TanStack Start application inside it.\n\n```\npnpm create @tanstack/start@latest my-ai-app\n```\n\nNavigate to the newly created directory:\n\n```\ncd my-ai-app\n```\n\n### [Install dependencies](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#install-dependencies)\n\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK's React hooks. The AI SDK's [Vercel AI Gateway provider](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You'll also install `zod`, a schema validation library used for defining tool inputs.\n\nThis guide uses the Vercel AI Gateway provider so you can access hundreds of\nmodels from different providers with one API key, but you can switch to any\nprovider or model by installing its package. Check out available [AI SDK\\\\\nproviders](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\npnpmnpmyarnbun\n\n```\npnpm add ai @ai-sdk/react zod\n```\n\n### [Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#configure-your-ai-gateway-api-key)\n\nCreate a `.env` file in your project root and add your AI Gateway API key. This key authenticates your application with Vercel AI Gateway.\n\n```\ntouch .env\n```\n\nEdit the `.env` file:\n\n.env\n\n```env\n1AI_GATEWAY_API_KEY=xxxxxxxxx\n```\n\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\n\nThe AI SDK's Vercel AI Gateway Provider will default to using the\n`AI_GATEWAY_API_KEY` environment variable.\n\n## [Create a Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#create-a-route-handler)\n\nCreate a route handler, `src/routes/api/chat.ts` and add the following code:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nsrc/routes/api/chat.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2import { createFileRoute } from '@tanstack/react-router';\n\n3\n\n4export const Route = createFileRoute('/api/chat')({\n\n5  server: {\n\n6    handlers: {\n\n7      POST: async ({ request }) => {\n\n8        const { messages }: { messages: UIMessage[] } = await request.json();\n\n9\n\n10        const result = streamText({\n\n11          model: \"anthropic/claude-sonnet-4.5\",\n\n12          messages: await convertToModelMessages(messages),\n\n13        });\n\n14\n\n15        return result.toUIMessageStreamResponse();\n\n16      },\n\n17    },\n\n18  },\n\n19});\n```\n\nLet's take a look at what is happening in this code:\n\n1. Define an asynchronous `POST` request handler using TanStack Start's server routes and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\n2. Call [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings) to further customise the model's behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\n3. The `streamText` function returns a [`StreamTextResult`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toUIMessageStreamResponse`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\n4. Finally, return the result to the client to stream the response.\n\nThis Route Handler creates a POST request endpoint at `/api/chat`.\n\n## [Choosing a Provider](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#choosing-a-provider)\n\nThe AI SDK supports dozens of model providers through [first-party](https://ai-sdk.dev/providers/ai-sdk-providers), [OpenAI-compatible](https://ai-sdk.dev/providers/openai-compatible-providers), and [community](https://ai-sdk.dev/providers/community-providers) packages.\n\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1model: \"anthropic/claude-sonnet-4.5\";\n```\n\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\n\n```ts\n1// Option 1: Import from 'ai' package (included by default)\n\n2import { gateway } from 'ai';\n\n3model: gateway('anthropic/claude-sonnet-4.5');\n\n4\n\n5// Option 2: Install and import from '@ai-sdk/gateway' package\n\n6import { gateway } from '@ai-sdk/gateway';\n\n7model: gateway('anthropic/claude-sonnet-4.5');\n```\n\n### [Using other providers](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#using-other-providers)\n\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\n\npnpmnpmyarnbun\n\n```\npnpm add @ai-sdk/openai\n```\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3model: openai('gpt-5.1');\n```\n\n#### [Updating the global provider](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#updating-the-global-provider)\n\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration).\n\nPick the approach that best matches how you want to manage providers across your application.\n\n## [Wire up the UI](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#wire-up-the-ui)\n\nNow that you have a Route Handler that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](https://ai-sdk.dev/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat).\n\nUpdate your index route (`src/routes/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\n\nsrc/routes/index.tsx\n\n```tsx\n1import { createFileRoute } from '@tanstack/react-router';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { useState } from 'react';\n\n4\n\n5export const Route = createFileRoute('/')({\n\n6  component: Chat,\n\n7});\n\n8\n\n9function Chat() {\n\n10  const [input, setInput] = useState('');\n\n11  const { messages, sendMessage } = useChat();\n\n12  return (\n\n13    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n14      {messages.map(message => (\n\n15        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n16          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n17          {message.parts.map((part, i) => {\n\n18            switch (part.type) {\n\n19              case 'text':\n\n20                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n21            }\n\n22          })}\n\n23        </div>\n\n24      ))}\n\n25\n\n26      <form\n\n27        onSubmit={e => {\n\n28          e.preventDefault();\n\n29          sendMessage({ text: input });\n\n30          setInput('');\n\n31        }}\n\n32      >\n\n33        <input\n\n34          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n35          value={input}\n\n36          placeholder=\"Say something...\"\n\n37          onChange={e => setInput(e.currentTarget.value)}\n\n38        />\n\n39      </form>\n\n40    </div>\n\n41  );\n\n42}\n```\n\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\n\n- `messages` \\- the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\n- `sendMessage` \\- a function to send a message to the chat API.\n\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\n\nThe LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.\n\n## [Running Your Application](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#running-your-application)\n\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\n\n```\npnpm run dev\n```\n\nHead to your browser and open [http://localhost:3000](http://localhost:3000/). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with TanStack Start.\n\n## [Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#enhance-your-chatbot-with-tools)\n\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) come in.\n\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\n\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\n\nLet's enhance your chatbot by adding a simple weather tool.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#update-your-route-handler)\n\nModify your `src/routes/api/chat.ts` file to include the new weather tool:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nsrc/routes/api/chat.ts\n\n```tsx\n1import { streamText, UIMessage, convertToModelMessages, tool } from 'ai';\n\n2import { createFileRoute } from '@tanstack/react-router';\n\n3import { z } from 'zod';\n\n4\n\n5export const Route = createFileRoute('/api/chat')({\n\n6  server: {\n\n7    handlers: {\n\n8      POST: async ({ request }) => {\n\n9        const { messages }: { messages: UIMessage[] } = await request.json();\n\n10\n\n11        const result = streamText({\n\n12          model: \"anthropic/claude-sonnet-4.5\",\n\n13          messages: await convertToModelMessages(messages),\n\n14          tools: {\n\n15            weather: tool({\n\n16              description: 'Get the weather in a location (fahrenheit)',\n\n17              inputSchema: z.object({\n\n18                location: z\n\n19                  .string()\n\n20                  .describe('The location to get the weather for'),\n\n21              }),\n\n22              execute: async ({ location }) => {\n\n23                const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n24                return {\n\n25                  location,\n\n26                  temperature,\n\n27                };\n\n28              },\n\n29            }),\n\n30          },\n\n31        });\n\n32\n\n33        return result.toUIMessageStreamResponse();\n\n34      },\n\n35    },\n\n36  },\n\n37});\n```\n\nIn this updated code:\n\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\n\n2. You define a `tools` object with a `weather` tool. This tool:\n   - Has a description that helps the model understand when to use it.\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can't, it will ask the user for the missing information.\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\n\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\n\nTry asking something like \"What's the weather in New York?\" and see how the model uses the new tool.\n\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\n\nTool parts are always named `tool-{toolName}`, where `{toolName}` is the key\nyou used when defining the tool. In this case, since we defined the tool as\n`weather`, the part type is `tool-weather`.\n\n### [Update the UI](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#update-the-ui)\n\nTo display the tool invocation in your UI, update your `src/routes/index.tsx` file:\n\nsrc/routes/index.tsx\n\n```tsx\n1import { createFileRoute } from '@tanstack/react-router';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { useState } from 'react';\n\n4\n\n5export const Route = createFileRoute('/')({\n\n6  component: Chat,\n\n7});\n\n8\n\n9function Chat() {\n\n10  const [input, setInput] = useState('');\n\n11  const { messages, sendMessage } = useChat();\n\n12  return (\n\n13    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n14      {messages.map(message => (\n\n15        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n16          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n17          {message.parts.map((part, i) => {\n\n18            switch (part.type) {\n\n19              case 'text':\n\n20                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n21              case 'tool-weather':\n\n22                return (\n\n23                  <pre key={`${message.id}-${i}`}>\n\n24                    {JSON.stringify(part, null, 2)}\n\n25                  </pre>\n\n26                );\n\n27            }\n\n28          })}\n\n29        </div>\n\n30      ))}\n\n31\n\n32      <form\n\n33        onSubmit={e => {\n\n34          e.preventDefault();\n\n35          sendMessage({ text: input });\n\n36          setInput('');\n\n37        }}\n\n38      >\n\n39        <input\n\n40          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n41          value={input}\n\n42          placeholder=\"Say something...\"\n\n43          onChange={e => setInput(e.currentTarget.value)}\n\n44        />\n\n45      </form>\n\n46    </div>\n\n47  );\n\n48}\n```\n\nWith this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\n\nNow, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.\n\n## [Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#enabling-multi-step-tool-calls)\n\nYou may have noticed that while the tool is now visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\n\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\n\n### [Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#update-your-route-handler-1)\n\nModify your `src/routes/api/chat.ts` file to include the `stopWhen` condition:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nsrc/routes/api/chat.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { createFileRoute } from '@tanstack/react-router';\n\n9import { z } from 'zod';\n\n10\n\n11export const Route = createFileRoute('/api/chat')({\n\n12  server: {\n\n13    handlers: {\n\n14      POST: async ({ request }) => {\n\n15        const { messages }: { messages: UIMessage[] } = await request.json();\n\n16\n\n17        const result = streamText({\n\n18          model: \"anthropic/claude-sonnet-4.5\",\n\n19          messages: await convertToModelMessages(messages),\n\n20          stopWhen: stepCountIs(5),\n\n21          tools: {\n\n22            weather: tool({\n\n23              description: 'Get the weather in a location (fahrenheit)',\n\n24              inputSchema: z.object({\n\n25                location: z\n\n26                  .string()\n\n27                  .describe('The location to get the weather for'),\n\n28              }),\n\n29              execute: async ({ location }) => {\n\n30                const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n31                return {\n\n32                  location,\n\n33                  temperature,\n\n34                };\n\n35              },\n\n36            }),\n\n37          },\n\n38        });\n\n39\n\n40        return result.toUIMessageStreamResponse();\n\n41      },\n\n42    },\n\n43  },\n\n44});\n```\n\nIn this updated code, you set `stopWhen` to be when `stepCountIs(5)`, allowing the model to use up to 5 \"steps\" for any given generation.\n\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\n\nBy setting `stopWhen: stepCountIs(5)`, you're allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\n\n### [Add another tool](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#add-another-tool)\n\nUpdate your `src/routes/api/chat.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nsrc/routes/api/chat.ts\n\n```tsx\n1import {\n\n2  streamText,\n\n3  UIMessage,\n\n4  convertToModelMessages,\n\n5  tool,\n\n6  stepCountIs,\n\n7} from 'ai';\n\n8import { createFileRoute } from '@tanstack/react-router';\n\n9import { z } from 'zod';\n\n10\n\n11export const Route = createFileRoute('/api/chat')({\n\n12  server: {\n\n13    handlers: {\n\n14      POST: async ({ request }) => {\n\n15        const { messages }: { messages: UIMessage[] } = await request.json();\n\n16\n\n17        const result = streamText({\n\n18          model: \"anthropic/claude-sonnet-4.5\",\n\n19          messages: await convertToModelMessages(messages),\n\n20          stopWhen: stepCountIs(5),\n\n21          tools: {\n\n22            weather: tool({\n\n23              description: 'Get the weather in a location (fahrenheit)',\n\n24              inputSchema: z.object({\n\n25                location: z\n\n26                  .string()\n\n27                  .describe('The location to get the weather for'),\n\n28              }),\n\n29              execute: async ({ location }) => {\n\n30                const temperature = Math.round(Math.random() * (90 - 32) + 32);\n\n31                return {\n\n32                  location,\n\n33                  temperature,\n\n34                };\n\n35              },\n\n36            }),\n\n37            convertFahrenheitToCelsius: tool({\n\n38              description: 'Convert a temperature in fahrenheit to celsius',\n\n39              inputSchema: z.object({\n\n40                temperature: z\n\n41                  .number()\n\n42                  .describe('The temperature in fahrenheit to convert'),\n\n43              }),\n\n44              execute: async ({ temperature }) => {\n\n45                const celsius = Math.round((temperature - 32) * (5 / 9));\n\n46                return {\n\n47                  celsius,\n\n48                };\n\n49              },\n\n50            }),\n\n51          },\n\n52        });\n\n53\n\n54        return result.toUIMessageStreamResponse();\n\n55      },\n\n56    },\n\n57  },\n\n58});\n```\n\n### [Update Your Frontend](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#update-your-frontend)\n\nupdate your `src/routes/index.tsx` file to render the new temperature conversion tool:\n\nsrc/routes/index.tsx\n\n```tsx\n1import { createFileRoute } from '@tanstack/react-router';\n\n2import { useChat } from '@ai-sdk/react';\n\n3import { useState } from 'react';\n\n4\n\n5export const Route = createFileRoute('/')({\n\n6  component: Chat,\n\n7});\n\n8\n\n9function Chat() {\n\n10  const [input, setInput] = useState('');\n\n11  const { messages, sendMessage } = useChat();\n\n12  return (\n\n13    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n14      {messages.map(message => (\n\n15        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n16          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n17          {message.parts.map((part, i) => {\n\n18            switch (part.type) {\n\n19              case 'text':\n\n20                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n21              case 'tool-weather':\n\n22              case 'tool-convertFahrenheitToCelsius':\n\n23                return (\n\n24                  <pre key={`${message.id}-${i}`}>\n\n25                    {JSON.stringify(part, null, 2)}\n\n26                  </pre>\n\n27                );\n\n28            }\n\n29          })}\n\n30        </div>\n\n31      ))}\n\n32\n\n33      <form\n\n34        onSubmit={e => {\n\n35          e.preventDefault();\n\n36          sendMessage({ text: input });\n\n37          setInput('');\n\n38        }}\n\n39      >\n\n40        <input\n\n41          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n42          value={input}\n\n43          placeholder=\"Say something...\"\n\n44          onChange={e => setInput(e.currentTarget.value)}\n\n45        />\n\n46      </form>\n\n47    </div>\n\n48  );\n\n49}\n```\n\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\n\nNow, when you ask \"What's the weather in New York in celsius?\", you should see a more complete interaction:\n\n1. The model will call the weather tool for New York.\n2. You'll see the tool output displayed.\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\n4. The model will then use that information to provide a natural language response about the weather in New York.\n\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\n\nThis simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.\n\n## [Where to Next?](https://ai-sdk.dev/docs/getting-started/tanstack-start\\#where-to-next)\n\nYou've built an AI chatbot using the AI SDK! From here, you have several paths to explore:\n\n- To learn more about the AI SDK, read through the [documentation](https://ai-sdk.dev/docs).\n- If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](https://ai-sdk.dev/docs/guides/rag-chatbot) and [multi-modal chatbot](https://ai-sdk.dev/docs/guides/multi-modal-chatbot) guides.\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\n\nOn this page\n\n[TanStack Start Quickstart](https://ai-sdk.dev/docs/getting-started/tanstack-start#tanstack-start-quickstart)\n\n[Prerequisites](https://ai-sdk.dev/docs/getting-started/tanstack-start#prerequisites)\n\n[Create Your Application](https://ai-sdk.dev/docs/getting-started/tanstack-start#create-your-application)\n\n[Install dependencies](https://ai-sdk.dev/docs/getting-started/tanstack-start#install-dependencies)\n\n[Configure your AI Gateway API key](https://ai-sdk.dev/docs/getting-started/tanstack-start#configure-your-ai-gateway-api-key)\n\n[Create a Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start#create-a-route-handler)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/tanstack-start#choosing-a-provider)\n\n[Using other providers](https://ai-sdk.dev/docs/getting-started/tanstack-start#using-other-providers)\n\n[Updating the global provider](https://ai-sdk.dev/docs/getting-started/tanstack-start#updating-the-global-provider)\n\n[Wire up the UI](https://ai-sdk.dev/docs/getting-started/tanstack-start#wire-up-the-ui)\n\n[Running Your Application](https://ai-sdk.dev/docs/getting-started/tanstack-start#running-your-application)\n\n[Enhance Your Chatbot with Tools](https://ai-sdk.dev/docs/getting-started/tanstack-start#enhance-your-chatbot-with-tools)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start#update-your-route-handler)\n\n[Update the UI](https://ai-sdk.dev/docs/getting-started/tanstack-start#update-the-ui)\n\n[Enabling Multi-Step Tool Calls](https://ai-sdk.dev/docs/getting-started/tanstack-start#enabling-multi-step-tool-calls)\n\n[Update Your Route Handler](https://ai-sdk.dev/docs/getting-started/tanstack-start#update-your-route-handler-1)\n\n[Add another tool](https://ai-sdk.dev/docs/getting-started/tanstack-start#add-another-tool)\n\n[Update Your Frontend](https://ai-sdk.dev/docs/getting-started/tanstack-start#update-your-frontend)\n\n[Where to Next?](https://ai-sdk.dev/docs/getting-started/tanstack-start#where-to-next)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/getting-started/tanstack-start",
        "url": "https://ai-sdk.dev/docs/getting-started/tanstack-start",
        "url_citation": "https://ai-sdk.dev/docs/getting-started/tanstack-start"
      }
    ],
    "agents": [
      {
        "title": "Agents",
        "title_citation": "https://ai-sdk.dev/docs/agents/overview",
        "content": "# [Agents](https://ai-sdk.dev/docs/agents/overview\\#agents)\n\nAgents are **large language models (LLMs)** that use **tools** in a **loop** to accomplish tasks.\n\nThese components work together:\n\n- **LLMs** process input and decide the next action\n- **Tools** extend capabilities beyond text generation (reading files, calling APIs, writing to databases)\n- **Loop**orchestrates execution through:\n  - **Context management** \\- Maintaining conversation history and deciding what the model sees (input) at each step\n  - **Stopping conditions** \\- Determining when the loop (task) is complete\n\n## [ToolLoopAgent Class](https://ai-sdk.dev/docs/agents/overview\\#toolloopagent-class)\n\nThe ToolLoopAgent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, stepCountIs, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const weatherAgent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather in a location (in Fahrenheit)',\n\n9      inputSchema: z.object({\n\n10        location: z.string().describe('The location to get the weather for'),\n\n11      }),\n\n12      execute: async ({ location }) => ({\n\n13        location,\n\n14        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n15      }),\n\n16    }),\n\n17    convertFahrenheitToCelsius: tool({\n\n18      description: 'Convert temperature from Fahrenheit to Celsius',\n\n19      inputSchema: z.object({\n\n20        temperature: z.number().describe('Temperature in Fahrenheit'),\n\n21      }),\n\n22      execute: async ({ temperature }) => {\n\n23        const celsius = Math.round((temperature - 32) * (5 / 9));\n\n24        return { celsius };\n\n25      },\n\n26    }),\n\n27  },\n\n28  // Agent's default behavior is to stop after a maximum of 20 steps\n\n29  // stopWhen: stepCountIs(20),\n\n30});\n\n31\n\n32const result = await weatherAgent.generate({\n\n33  prompt: 'What is the weather in San Francisco in celsius?',\n\n34});\n\n35\n\n36console.log(result.text); // agent's final answer\n\n37console.log(result.steps); // steps taken by the agent\n```\n\nThe agent automatically:\n\n1. Calls the `weather` tool to get the temperature in Fahrenheit\n2. Calls `convertFahrenheitToCelsius` to convert it\n3. Generates a final text response with the result\n\nThe Agent class handles the loop, context management, and stopping conditions.\n\n## [Why Use the Agent Class?](https://ai-sdk.dev/docs/agents/overview\\#why-use-the-agent-class)\n\nThe Agent class is the recommended approach for building agents with the AI SDK because it:\n\n- **Reduces boilerplate** \\- Manages loops and message arrays\n- **Improves reusability** \\- Define once, use throughout your application\n- **Simplifies maintenance** \\- Single place to update agent configuration\n\nFor most use cases, start with the Agent class. Use core functions (`generateText`, `streamText`) when you need explicit control over each step for complex structured workflows.\n\n## [Structured Workflows](https://ai-sdk.dev/docs/agents/overview\\#structured-workflows)\n\nAgents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:\n\n- Conditional statements for explicit branching\n- Standard functions for reusable logic\n- Error handling for robustness\n- Explicit control flow for predictability\n\n[Explore workflow patterns](https://ai-sdk.dev/docs/agents/workflows) to learn more about building structured, reliable systems.\n\n## [Next Steps](https://ai-sdk.dev/docs/agents/overview\\#next-steps)\n\n- **[Building Agents](https://ai-sdk.dev/docs/agents/building-agents)** \\- Guide to creating agents with the Agent class\n- **[Workflow Patterns](https://ai-sdk.dev/docs/agents/workflows)** \\- Structured patterns using core functions for complex workflows\n- **[Loop Control](https://ai-sdk.dev/docs/agents/loop-control)** \\- Execution control with stopWhen and prepareStep\n\nOn this page\n\n[Agents](https://ai-sdk.dev/docs/agents/overview#agents)\n\n[ToolLoopAgent Class](https://ai-sdk.dev/docs/agents/overview#toolloopagent-class)\n\n[Why Use the Agent Class?](https://ai-sdk.dev/docs/agents/overview#why-use-the-agent-class)\n\n[Structured Workflows](https://ai-sdk.dev/docs/agents/overview#structured-workflows)\n\n[Next Steps](https://ai-sdk.dev/docs/agents/overview#next-steps)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/agents/overview",
        "url": "https://ai-sdk.dev/docs/agents/overview",
        "url_citation": "https://ai-sdk.dev/docs/agents/overview"
      },
      {
        "title": "Building Agents",
        "title_citation": "https://ai-sdk.dev/docs/agents/building-agents",
        "content": "# [Building Agents](https://ai-sdk.dev/docs/agents/building-agents\\#building-agents)\n\nThe Agent class provides a structured way to encapsulate LLM configuration, tools, and behavior into reusable components. It handles the agent loop for you, allowing the LLM to call tools multiple times in sequence to accomplish complex tasks. Define agents once and use them across your application.\n\n## [Why Use the ToolLoopAgent Class?](https://ai-sdk.dev/docs/agents/building-agents\\#why-use-the-toolloopagent-class)\n\nWhen building AI applications, you often need to:\n\n- **Reuse configurations** \\- Same model settings, tools, and prompts across different parts of your application\n- **Maintain consistency** \\- Ensure the same behavior and capabilities throughout your codebase\n- **Simplify API routes** \\- Reduce boilerplate in your endpoints\n- **Type safety** \\- Get full TypeScript support for your agent's tools and outputs\n\nThe ToolLoopAgent class provides a single place to define your agent's behavior.\n\n## [Creating an Agent](https://ai-sdk.dev/docs/agents/building-agents\\#creating-an-agent)\n\nDefine an agent by instantiating the ToolLoopAgent class with your desired configuration:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const myAgent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  instructions: 'You are a helpful assistant.',\n\n6  tools: {\n\n7    // Your tools here\n\n8  },\n\n9});\n```\n\n## [Configuration Options](https://ai-sdk.dev/docs/agents/building-agents\\#configuration-options)\n\nThe Agent class accepts all the same settings as `generateText` and `streamText`. Configure:\n\n### [Model and System Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#model-and-system-instructions)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  instructions: 'You are an expert software engineer.',\n\n6});\n```\n\n### [Tools](https://ai-sdk.dev/docs/agents/building-agents\\#tools)\n\nProvide tools that the agent can use to accomplish tasks:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const codeAgent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    runCode: tool({\n\n8      description: 'Execute Python code',\n\n9      inputSchema: z.object({\n\n10        code: z.string(),\n\n11      }),\n\n12      execute: async ({ code }) => {\n\n13        // Execute code and return result\n\n14        return { output: 'Code executed successfully' };\n\n15      },\n\n16    }),\n\n17  },\n\n18});\n```\n\n### [Loop Control](https://ai-sdk.dev/docs/agents/building-agents\\#loop-control)\n\nBy default, agents run for 20 steps (`stopWhen: stepCountIs(20)`). In each step, the model either generates text or calls a tool. If it generates text, the agent completes. If it calls a tool, the AI SDK executes that tool.\n\nTo let agents call multiple tools in sequence, configure `stopWhen` to allow more steps. After each tool execution, the agent triggers a new generation where the model can call another tool or generate text:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, stepCountIs } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  stopWhen: stepCountIs(20), // Allow up to 20 steps\n\n6});\n```\n\nEach step represents one generation (which results in either text or a tool call). The loop continues until:\n\n- A finish reasoning other than tool-calls is returned, or\n- A tool that is invoked does not have an execute function, or\n- A tool call needs approval, or\n- A stop condition is met\n\nYou can combine multiple conditions:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, stepCountIs } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  stopWhen: [\\\n\\\n6    stepCountIs(20), // Maximum 20 steps\\\n\\\n7    yourCustomCondition(), // Custom logic for when to stop\\\n\\\n8  ],\n\n9});\n```\n\nLearn more about [loop control and stop conditions](https://ai-sdk.dev/docs/agents/loop-control).\n\n### [Tool Choice](https://ai-sdk.dev/docs/agents/building-agents\\#tool-choice)\n\nControl how the agent uses tools:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    // your tools here\n\n7  },\n\n8  toolChoice: 'required', // Force tool use\n\n9  // or toolChoice: 'none' to disable tools\n\n10  // or toolChoice: 'auto' (default) to let the model decide\n\n11});\n```\n\nYou can also force the use of a specific tool:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    weather: weatherTool,\n\n7    cityAttractions: attractionsTool,\n\n8  },\n\n9  toolChoice: {\n\n10    type: 'tool',\n\n11    toolName: 'weather', // Force the weather tool to be used\n\n12  },\n\n13});\n```\n\n### [Structured Output](https://ai-sdk.dev/docs/agents/building-agents\\#structured-output)\n\nDefine structured output schemas:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, Output, stepCountIs } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const analysisAgent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      sentiment: z.enum(['positive', 'neutral', 'negative']),\n\n9      summary: z.string(),\n\n10      keyPoints: z.array(z.string()),\n\n11    }),\n\n12  }),\n\n13  stopWhen: stepCountIs(10),\n\n14});\n\n15\n\n16const { output } = await analysisAgent.generate({\n\n17  prompt: 'Analyze customer feedback from the last quarter',\n\n18});\n```\n\n## [Define Agent Behavior with System Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#define-agent-behavior-with-system-instructions)\n\nSystem instructions define your agent's behavior, personality, and constraints. They set the context for all interactions and guide how the agent responds to user queries and uses tools.\n\n### [Basic System Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#basic-system-instructions)\n\nSet the agent's role and expertise:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const agent = new ToolLoopAgent({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  instructions:\n\n4    'You are an expert data analyst. You provide clear insights from complex data.',\n\n5});\n```\n\n### [Detailed Behavioral Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#detailed-behavioral-instructions)\n\nProvide specific guidelines for agent behavior:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const codeReviewAgent = new ToolLoopAgent({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  instructions: `You are a senior software engineer conducting code reviews.\n\n4\n\n5  Your approach:\n\n6  - Focus on security vulnerabilities first\n\n7  - Identify performance bottlenecks\n\n8  - Suggest improvements for readability and maintainability\n\n9  - Be constructive and educational in your feedback\n\n10  - Always explain why something is an issue and how to fix it`,\n\n11});\n```\n\n### [Constrain Agent Behavior](https://ai-sdk.dev/docs/agents/building-agents\\#constrain-agent-behavior)\n\nSet boundaries and ensure consistent behavior:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const customerSupportAgent = new ToolLoopAgent({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  instructions: `You are a customer support specialist for an e-commerce platform.\n\n4\n\n5  Rules:\n\n6  - Never make promises about refunds without checking the policy\n\n7  - Always be empathetic and professional\n\n8  - If you don't know something, say so and offer to escalate\n\n9  - Keep responses concise and actionable\n\n10  - Never share internal company information`,\n\n11  tools: {\n\n12    checkOrderStatus,\n\n13    lookupPolicy,\n\n14    createTicket,\n\n15  },\n\n16});\n```\n\n### [Tool Usage Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#tool-usage-instructions)\n\nGuide how the agent should use available tools:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const researchAgent = new ToolLoopAgent({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  instructions: `You are a research assistant with access to search and document tools.\n\n4\n\n5  When researching:\n\n6  1. Always start with a broad search to understand the topic\n\n7  2. Use document analysis for detailed information\n\n8  3. Cross-reference multiple sources before drawing conclusions\n\n9  4. Cite your sources when presenting information\n\n10  5. If information conflicts, present both viewpoints`,\n\n11  tools: {\n\n12    webSearch,\n\n13    analyzeDocument,\n\n14    extractQuotes,\n\n15  },\n\n16});\n```\n\n### [Format and Style Instructions](https://ai-sdk.dev/docs/agents/building-agents\\#format-and-style-instructions)\n\nControl the output format and communication style:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const technicalWriterAgent = new ToolLoopAgent({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  instructions: `You are a technical documentation writer.\n\n4\n\n5  Writing style:\n\n6  - Use clear, simple language\n\n7  - Avoid jargon unless necessary\n\n8  - Structure information with headers and bullet points\n\n9  - Include code examples where relevant\n\n10  - Write in second person (\"you\" instead of \"the user\")\n\n11\n\n12  Always format responses in Markdown.`,\n\n13});\n```\n\n## [Using an Agent](https://ai-sdk.dev/docs/agents/building-agents\\#using-an-agent)\n\nOnce defined, you can use your agent in three ways:\n\n### [Generate Text](https://ai-sdk.dev/docs/agents/building-agents\\#generate-text)\n\nUse `generate()` for one-time text generation:\n\n```ts\n1const result = await myAgent.generate({\n\n2  prompt: 'What is the weather like?',\n\n3});\n\n4\n\n5console.log(result.text);\n```\n\n### [Stream Text](https://ai-sdk.dev/docs/agents/building-agents\\#stream-text)\n\nUse `stream()` for streaming responses:\n\n```ts\n1const stream = myAgent.stream({\n\n2  prompt: 'Tell me a story',\n\n3});\n\n4\n\n5for await (const chunk of stream.textStream) {\n\n6  console.log(chunk);\n\n7}\n```\n\n### [Respond to UI Messages](https://ai-sdk.dev/docs/agents/building-agents\\#respond-to-ui-messages)\n\nUse `createAgentUIStreamResponse()` to create API responses for client applications:\n\n```ts\n1// In your API route (e.g., app/api/chat/route.ts)\n\n2import { createAgentUIStreamResponse } from 'ai';\n\n3\n\n4export async function POST(request: Request) {\n\n5  const { messages } = await request.json();\n\n6\n\n7  return createAgentUIStreamResponse({\n\n8    agent: myAgent,\n\n9    messages,\n\n10  });\n\n11}\n```\n\n## [End-to-end Type Safety](https://ai-sdk.dev/docs/agents/building-agents\\#end-to-end-type-safety)\n\nYou can infer types for your agent's `UIMessage`s:\n\n```ts\n1import { ToolLoopAgent, InferAgentUIMessage } from 'ai';\n\n2\n\n3const myAgent = new ToolLoopAgent({\n\n4  // ... configuration\n\n5});\n\n6\n\n7// Infer the UIMessage type for UI components or persistence\n\n8export type MyAgentUIMessage = InferAgentUIMessage<typeof myAgent>;\n```\n\nUse this type in your client components with `useChat`:\n\ncomponents/chat.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import type { MyAgentUIMessage } from '@/agent/my-agent';\n\n5\n\n6export function Chat() {\n\n7  const { messages } = useChat<MyAgentUIMessage>();\n\n8  // Full type safety for your messages and tools\n\n9}\n```\n\n## [Next Steps](https://ai-sdk.dev/docs/agents/building-agents\\#next-steps)\n\nNow that you understand building agents, you can:\n\n- Explore [workflow patterns](https://ai-sdk.dev/docs/agents/workflows) for structured patterns using core functions\n- Learn about [loop control](https://ai-sdk.dev/docs/agents/loop-control) for advanced execution control\n- See [manual loop examples](https://ai-sdk.dev/cookbook/node/manual-agent-loop) for custom workflow implementations\n\nOn this page\n\n[Building Agents](https://ai-sdk.dev/docs/agents/building-agents#building-agents)\n\n[Why Use the ToolLoopAgent Class?](https://ai-sdk.dev/docs/agents/building-agents#why-use-the-toolloopagent-class)\n\n[Creating an Agent](https://ai-sdk.dev/docs/agents/building-agents#creating-an-agent)\n\n[Configuration Options](https://ai-sdk.dev/docs/agents/building-agents#configuration-options)\n\n[Model and System Instructions](https://ai-sdk.dev/docs/agents/building-agents#model-and-system-instructions)\n\n[Tools](https://ai-sdk.dev/docs/agents/building-agents#tools)\n\n[Loop Control](https://ai-sdk.dev/docs/agents/building-agents#loop-control)\n\n[Tool Choice](https://ai-sdk.dev/docs/agents/building-agents#tool-choice)\n\n[Structured Output](https://ai-sdk.dev/docs/agents/building-agents#structured-output)\n\n[Define Agent Behavior with System Instructions](https://ai-sdk.dev/docs/agents/building-agents#define-agent-behavior-with-system-instructions)\n\n[Basic System Instructions](https://ai-sdk.dev/docs/agents/building-agents#basic-system-instructions)\n\n[Detailed Behavioral Instructions](https://ai-sdk.dev/docs/agents/building-agents#detailed-behavioral-instructions)\n\n[Constrain Agent Behavior](https://ai-sdk.dev/docs/agents/building-agents#constrain-agent-behavior)\n\n[Tool Usage Instructions](https://ai-sdk.dev/docs/agents/building-agents#tool-usage-instructions)\n\n[Format and Style Instructions](https://ai-sdk.dev/docs/agents/building-agents#format-and-style-instructions)\n\n[Using an Agent](https://ai-sdk.dev/docs/agents/building-agents#using-an-agent)\n\n[Generate Text](https://ai-sdk.dev/docs/agents/building-agents#generate-text)\n\n[Stream Text](https://ai-sdk.dev/docs/agents/building-agents#stream-text)\n\n[Respond to UI Messages](https://ai-sdk.dev/docs/agents/building-agents#respond-to-ui-messages)\n\n[End-to-end Type Safety](https://ai-sdk.dev/docs/agents/building-agents#end-to-end-type-safety)\n\n[Next Steps](https://ai-sdk.dev/docs/agents/building-agents#next-steps)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/agents/building-agents",
        "url": "https://ai-sdk.dev/docs/agents/building-agents",
        "url_citation": "https://ai-sdk.dev/docs/agents/building-agents"
      },
      {
        "title": "Workflow Patterns",
        "title_citation": "https://ai-sdk.dev/docs/agents/workflows",
        "content": "# [Workflow Patterns](https://ai-sdk.dev/docs/agents/workflows\\#workflow-patterns)\n\nCombine the building blocks from the [overview](https://ai-sdk.dev/docs/agents/overview) with these patterns to add structure and reliability to your agents:\n\n- [Sequential Processing](https://ai-sdk.dev/docs/agents/workflows#sequential-processing-chains) \\- Steps executed in order\n- [Parallel Processing](https://ai-sdk.dev/docs/agents/workflows#parallel-processing) \\- Independent tasks run simultaneously\n- [Evaluation/Feedback Loops](https://ai-sdk.dev/docs/agents/workflows#evaluator-optimizer) \\- Results checked and improved iteratively\n- [Orchestration](https://ai-sdk.dev/docs/agents/workflows#orchestrator-worker) \\- Coordinating multiple components\n- [Routing](https://ai-sdk.dev/docs/agents/workflows#routing) \\- Directing work based on context\n\n## [Choose Your Approach](https://ai-sdk.dev/docs/agents/workflows\\#choose-your-approach)\n\nConsider these key factors:\n\n- **Flexibility vs Control** \\- How much freedom does the LLM need vs how tightly you must constrain its actions?\n- **Error Tolerance** \\- What are the consequences of mistakes in your use case?\n- **Cost Considerations** \\- More complex systems typically mean more LLM calls and higher costs\n- **Maintenance** \\- Simpler architectures are easier to debug and modify\n\n**Start with the simplest approach that meets your needs**. Add complexity only when required by:\n\n1. Breaking down tasks into clear steps\n2. Adding tools for specific capabilities\n3. Implementing feedback loops for quality control\n4. Introducing multiple agents for complex workflows\n\nLet's look at examples of these patterns in action.\n\n## [Patterns with Examples](https://ai-sdk.dev/docs/agents/workflows\\#patterns-with-examples)\n\nThese patterns, adapted from [Anthropic's guide on building effective agents](https://www.anthropic.com/research/building-effective-agents), serve as building blocks you can combine to create comprehensive workflows. Each pattern addresses specific aspects of task execution. Combine them thoughtfully to build reliable solutions for complex problems.\n\n## [Sequential Processing (Chains)](https://ai-sdk.dev/docs/agents/workflows\\#sequential-processing-chains)\n\nThe simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. Use this pattern for tasks with well-defined sequences, like content generation pipelines or data transformation processes.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4async function generateMarketingCopy(input: string) {\n\n5  const model = \"anthropic/claude-sonnet-4.5\";\n\n6\n\n7  // First step: Generate marketing copy\n\n8  const { text: copy } = await generateText({\n\n9    model,\n\n10    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,\n\n11  });\n\n12\n\n13  // Perform quality check on copy\n\n14  const { object: qualityMetrics } = await generateObject({\n\n15    model,\n\n16    schema: z.object({\n\n17      hasCallToAction: z.boolean(),\n\n18      emotionalAppeal: z.number().min(1).max(10),\n\n19      clarity: z.number().min(1).max(10),\n\n20    }),\n\n21    prompt: `Evaluate this marketing copy for:\n\n22    1. Presence of call to action (true/false)\n\n23    2. Emotional appeal (1-10)\n\n24    3. Clarity (1-10)\n\n25\n\n26    Copy to evaluate: ${copy}`,\n\n27  });\n\n28\n\n29  // If quality check fails, regenerate with more specific instructions\n\n30  if (\n\n31    !qualityMetrics.hasCallToAction ||\n\n32    qualityMetrics.emotionalAppeal < 7 ||\n\n33    qualityMetrics.clarity < 7\n\n34  ) {\n\n35    const { text: improvedCopy } = await generateText({\n\n36      model,\n\n37      prompt: `Rewrite this marketing copy with:\n\n38      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}\n\n39      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}\n\n40      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}\n\n41\n\n42      Original copy: ${copy}`,\n\n43    });\n\n44    return { copy: improvedCopy, qualityMetrics };\n\n45  }\n\n46\n\n47  return { copy, qualityMetrics };\n\n48}\n```\n\n## [Routing](https://ai-sdk.dev/docs/agents/workflows\\#routing)\n\nThis pattern lets the model decide which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. Use this when handling varied inputs that require different processing approaches. In the example below, the first LLM call's results determine the second call's model size and system prompt.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject, generateText } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4async function handleCustomerQuery(query: string) {\n\n5  const model = \"anthropic/claude-sonnet-4.5\";\n\n6\n\n7  // First step: Classify the query type\n\n8  const { object: classification } = await generateObject({\n\n9    model,\n\n10    schema: z.object({\n\n11      reasoning: z.string(),\n\n12      type: z.enum(['general', 'refund', 'technical']),\n\n13      complexity: z.enum(['simple', 'complex']),\n\n14    }),\n\n15    prompt: `Classify this customer query:\n\n16    ${query}\n\n17\n\n18    Determine:\n\n19    1. Query type (general, refund, or technical)\n\n20    2. Complexity (simple or complex)\n\n21    3. Brief reasoning for classification`,\n\n22  });\n\n23\n\n24  // Route based on classification\n\n25  // Set model and system prompt based on query type and complexity\n\n26  const { text: response } = await generateText({\n\n27    model:\n\n28      classification.complexity === 'simple'\n\n29        ? 'openai/gpt-4o-mini'\n\n30        : 'openai/o4-mini',\n\n31    system: {\n\n32      general:\n\n33        'You are an expert customer service agent handling general inquiries.',\n\n34      refund:\n\n35        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',\n\n36      technical:\n\n37        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',\n\n38    }[classification.type],\n\n39    prompt: query,\n\n40  });\n\n41\n\n42  return { response, classification };\n\n43}\n```\n\n## [Parallel Processing](https://ai-sdk.dev/docs/agents/workflows\\#parallel-processing)\n\nBreak down tasks into independent subtasks that execute simultaneously. This pattern uses parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyze multiple documents or process different aspects of a single input concurrently (like code review).\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4// Example: Parallel code review with multiple specialized reviewers\n\n5async function parallelCodeReview(code: string) {\n\n6  const model = \"anthropic/claude-sonnet-4.5\";\n\n7\n\n8  // Run parallel reviews\n\n9  const [securityReview, performanceReview, maintainabilityReview] =\n\n10    await Promise.all([\\\n\\\n11      generateObject({\\\n\\\n12        model,\\\n\\\n13        system:\\\n\\\n14          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',\\\n\\\n15        schema: z.object({\\\n\\\n16          vulnerabilities: z.array(z.string()),\\\n\\\n17          riskLevel: z.enum(['low', 'medium', 'high']),\\\n\\\n18          suggestions: z.array(z.string()),\\\n\\\n19        }),\\\n\\\n20        prompt: `Review this code:\\\n\\\n21      ${code}`,\\\n\\\n22      }),\\\n\\\n23\\\n\\\n24      generateObject({\\\n\\\n25        model,\\\n\\\n26        system:\\\n\\\n27          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',\\\n\\\n28        schema: z.object({\\\n\\\n29          issues: z.array(z.string()),\\\n\\\n30          impact: z.enum(['low', 'medium', 'high']),\\\n\\\n31          optimizations: z.array(z.string()),\\\n\\\n32        }),\\\n\\\n33        prompt: `Review this code:\\\n\\\n34      ${code}`,\\\n\\\n35      }),\\\n\\\n36\\\n\\\n37      generateObject({\\\n\\\n38        model,\\\n\\\n39        system:\\\n\\\n40          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',\\\n\\\n41        schema: z.object({\\\n\\\n42          concerns: z.array(z.string()),\\\n\\\n43          qualityScore: z.number().min(1).max(10),\\\n\\\n44          recommendations: z.array(z.string()),\\\n\\\n45        }),\\\n\\\n46        prompt: `Review this code:\\\n\\\n47      ${code}`,\\\n\\\n48      }),\\\n\\\n49    ]);\n\n50\n\n51  const reviews = [\\\n\\\n52    { ...securityReview.object, type: 'security' },\\\n\\\n53    { ...performanceReview.object, type: 'performance' },\\\n\\\n54    { ...maintainabilityReview.object, type: 'maintainability' },\\\n\\\n55  ];\n\n56\n\n57  // Aggregate results using another model instance\n\n58  const { text: summary } = await generateText({\n\n59    model,\n\n60    system: 'You are a technical lead summarizing multiple code reviews.',\n\n61    prompt: `Synthesize these code review results into a concise summary with key actions:\n\n62    ${JSON.stringify(reviews, null, 2)}`,\n\n63  });\n\n64\n\n65  return { reviews, summary };\n\n66}\n```\n\n## [Orchestrator-Worker](https://ai-sdk.dev/docs/agents/workflows\\#orchestrator-worker)\n\nA primary model (orchestrator) coordinates the execution of specialized workers. Each worker optimizes for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4async function implementFeature(featureRequest: string) {\n\n5  // Orchestrator: Plan the implementation\n\n6  const { object: implementationPlan } = await generateObject({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    schema: z.object({\n\n9      files: z.array(\n\n10        z.object({\n\n11          purpose: z.string(),\n\n12          filePath: z.string(),\n\n13          changeType: z.enum(['create', 'modify', 'delete']),\n\n14        }),\n\n15      ),\n\n16      estimatedComplexity: z.enum(['low', 'medium', 'high']),\n\n17    }),\n\n18    system:\n\n19      'You are a senior software architect planning feature implementations.',\n\n20    prompt: `Analyze this feature request and create an implementation plan:\n\n21    ${featureRequest}`,\n\n22  });\n\n23\n\n24  // Workers: Execute the planned changes\n\n25  const fileChanges = await Promise.all(\n\n26    implementationPlan.files.map(async file => {\n\n27      // Each worker is specialized for the type of change\n\n28      const workerSystemPrompt = {\n\n29        create:\n\n30          'You are an expert at implementing new files following best practices and project patterns.',\n\n31        modify:\n\n32          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',\n\n33        delete:\n\n34          'You are an expert at safely removing code while ensuring no breaking changes.',\n\n35      }[file.changeType];\n\n36\n\n37      const { object: change } = await generateObject({\n\n38        model: \"anthropic/claude-sonnet-4.5\",\n\n39        schema: z.object({\n\n40          explanation: z.string(),\n\n41          code: z.string(),\n\n42        }),\n\n43        system: workerSystemPrompt,\n\n44        prompt: `Implement the changes for ${file.filePath} to support:\n\n45        ${file.purpose}\n\n46\n\n47        Consider the overall feature context:\n\n48        ${featureRequest}`,\n\n49      });\n\n50\n\n51      return {\n\n52        file,\n\n53        implementation: change,\n\n54      };\n\n55    }),\n\n56  );\n\n57\n\n58  return {\n\n59    plan: implementationPlan,\n\n60    changes: fileChanges,\n\n61  };\n\n62}\n```\n\n## [Evaluator-Optimizer](https://ai-sdk.dev/docs/agents/workflows\\#evaluator-optimizer)\n\nAdd quality control to workflows with dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow proceeds, retries with adjusted parameters, or takes corrective action. This creates robust workflows capable of self-improvement and error recovery.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4async function translateWithFeedback(text: string, targetLanguage: string) {\n\n5  let currentTranslation = '';\n\n6  let iterations = 0;\n\n7  const MAX_ITERATIONS = 3;\n\n8\n\n9  // Initial translation\n\n10  const { text: translation } = await generateText({\n\n11    model: \"anthropic/claude-sonnet-4.5\",\n\n12    system: 'You are an expert literary translator.',\n\n13    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:\n\n14    ${text}`,\n\n15  });\n\n16\n\n17  currentTranslation = translation;\n\n18\n\n19  // Evaluation-optimization loop\n\n20  while (iterations < MAX_ITERATIONS) {\n\n21    // Evaluate current translation\n\n22    const { object: evaluation } = await generateObject({\n\n23      model: \"anthropic/claude-sonnet-4.5\",\n\n24      schema: z.object({\n\n25        qualityScore: z.number().min(1).max(10),\n\n26        preservesTone: z.boolean(),\n\n27        preservesNuance: z.boolean(),\n\n28        culturallyAccurate: z.boolean(),\n\n29        specificIssues: z.array(z.string()),\n\n30        improvementSuggestions: z.array(z.string()),\n\n31      }),\n\n32      system: 'You are an expert in evaluating literary translations.',\n\n33      prompt: `Evaluate this translation:\n\n34\n\n35      Original: ${text}\n\n36      Translation: ${currentTranslation}\n\n37\n\n38      Consider:\n\n39      1. Overall quality\n\n40      2. Preservation of tone\n\n41      3. Preservation of nuance\n\n42      4. Cultural accuracy`,\n\n43    });\n\n44\n\n45    // Check if quality meets threshold\n\n46    if (\n\n47      evaluation.qualityScore >= 8 &&\n\n48      evaluation.preservesTone &&\n\n49      evaluation.preservesNuance &&\n\n50      evaluation.culturallyAccurate\n\n51    ) {\n\n52      break;\n\n53    }\n\n54\n\n55    // Generate improved translation based on feedback\n\n56    const { text: improvedTranslation } = await generateText({\n\n57      model: \"anthropic/claude-sonnet-4.5\",\n\n58      system: 'You are an expert literary translator.',\n\n59      prompt: `Improve this translation based on the following feedback:\n\n60      ${evaluation.specificIssues.join('\\n')}\n\n61      ${evaluation.improvementSuggestions.join('\\n')}\n\n62\n\n63      Original: ${text}\n\n64      Current Translation: ${currentTranslation}`,\n\n65    });\n\n66\n\n67    currentTranslation = improvedTranslation;\n\n68    iterations++;\n\n69  }\n\n70\n\n71  return {\n\n72    finalTranslation: currentTranslation,\n\n73    iterationsRequired: iterations,\n\n74  };\n\n75}\n```\n\nOn this page\n\n[Workflow Patterns](https://ai-sdk.dev/docs/agents/workflows#workflow-patterns)\n\n[Choose Your Approach](https://ai-sdk.dev/docs/agents/workflows#choose-your-approach)\n\n[Patterns with Examples](https://ai-sdk.dev/docs/agents/workflows#patterns-with-examples)\n\n[Sequential Processing (Chains)](https://ai-sdk.dev/docs/agents/workflows#sequential-processing-chains)\n\n[Routing](https://ai-sdk.dev/docs/agents/workflows#routing)\n\n[Parallel Processing](https://ai-sdk.dev/docs/agents/workflows#parallel-processing)\n\n[Orchestrator-Worker](https://ai-sdk.dev/docs/agents/workflows#orchestrator-worker)\n\n[Evaluator-Optimizer](https://ai-sdk.dev/docs/agents/workflows#evaluator-optimizer)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/agents/workflows",
        "url": "https://ai-sdk.dev/docs/agents/workflows",
        "url_citation": "https://ai-sdk.dev/docs/agents/workflows"
      },
      {
        "title": "Loop Control",
        "title_citation": "https://ai-sdk.dev/docs/agents/loop-control",
        "content": "# [Loop Control](https://ai-sdk.dev/docs/agents/loop-control\\#loop-control)\n\nYou can control both the execution flow and the settings at each step of the agent loop. The loop continues until:\n\n- A finish reasoning other than tool-calls is returned, or\n- A tool that is invoked does not have an execute function, or\n- A tool call needs approval, or\n- A stop condition is met\n\nThe AI SDK provides built-in loop control through two parameters: `stopWhen` for defining stopping conditions and `prepareStep` for modifying settings (model, tools, messages, and more) between steps.\n\n## [Stop Conditions](https://ai-sdk.dev/docs/agents/loop-control\\#stop-conditions)\n\nThe `stopWhen` parameter controls when to stop execution when there are tool results in the last step. By default, agents stop after 20 steps using `stepCountIs(20)`.\n\nWhen you provide `stopWhen`, the agent continues executing after tool calls until a stopping condition is met. When the condition is an array, execution stops when any of the conditions are met.\n\n### [Use Built-in Conditions](https://ai-sdk.dev/docs/agents/loop-control\\#use-built-in-conditions)\n\nThe AI SDK provides several built-in stopping conditions:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, stepCountIs } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    // your tools\n\n7  },\n\n8  stopWhen: stepCountIs(20), // Default state: stop after 20 steps maximum\n\n9});\n\n10\n\n11const result = await agent.generate({\n\n12  prompt: 'Analyze this dataset and create a summary report',\n\n13});\n```\n\n### [Combine Multiple Conditions](https://ai-sdk.dev/docs/agents/loop-control\\#combine-multiple-conditions)\n\nCombine multiple stopping conditions. The loop stops when it meets any condition:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, stepCountIs, hasToolCall } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    // your tools\n\n7  },\n\n8  stopWhen: [\\\n\\\n9    stepCountIs(20), // Maximum 20 steps\\\n\\\n10    hasToolCall('someTool'), // Stop after calling 'someTool'\\\n\\\n11  ],\n\n12});\n\n13\n\n14const result = await agent.generate({\n\n15  prompt: 'Research and analyze the topic',\n\n16});\n```\n\n### [Create Custom Conditions](https://ai-sdk.dev/docs/agents/loop-control\\#create-custom-conditions)\n\nBuild custom stopping conditions for specific requirements:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent, StopCondition, ToolSet } from 'ai';\n\n2\n\n3const tools = {\n\n4  // your tools\n\n5} satisfies ToolSet;\n\n6\n\n7const hasAnswer: StopCondition<typeof tools> = ({ steps }) => {\n\n8  // Stop when the model generates text containing \"ANSWER:\"\n\n9  return steps.some(step => step.text?.includes('ANSWER:')) ?? false;\n\n10};\n\n11\n\n12const agent = new ToolLoopAgent({\n\n13  model: \"anthropic/claude-sonnet-4.5\",\n\n14  tools,\n\n15  stopWhen: hasAnswer,\n\n16});\n\n17\n\n18const result = await agent.generate({\n\n19  prompt: 'Find the answer and respond with \"ANSWER: [your answer]\"',\n\n20});\n```\n\nCustom conditions receive step information across all steps:\n\n```ts\n1const budgetExceeded: StopCondition<typeof tools> = ({ steps }) => {\n\n2  const totalUsage = steps.reduce(\n\n3    (acc, step) => ({\n\n4      inputTokens: acc.inputTokens + (step.usage?.inputTokens ?? 0),\n\n5      outputTokens: acc.outputTokens + (step.usage?.outputTokens ?? 0),\n\n6    }),\n\n7    { inputTokens: 0, outputTokens: 0 },\n\n8  );\n\n9\n\n10  const costEstimate =\n\n11    (totalUsage.inputTokens * 0.01 + totalUsage.outputTokens * 0.03) / 1000;\n\n12  return costEstimate > 0.5; // Stop if cost exceeds $0.50\n\n13};\n```\n\n## [Prepare Step](https://ai-sdk.dev/docs/agents/loop-control\\#prepare-step)\n\nThe `prepareStep` callback runs before each step in the loop and defaults to the initial settings if you don't return any changes. Use it to modify settings, manage context, or implement dynamic behavior based on execution history.\n\n### [Dynamic Model Selection](https://ai-sdk.dev/docs/agents/loop-control\\#dynamic-model-selection)\n\nSwitch models based on step requirements:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: 'openai/gpt-4o-mini', // Default model\n\n5  tools: {\n\n6    // your tools\n\n7  },\n\n8  prepareStep: async ({ stepNumber, messages }) => {\n\n9    // Use a stronger model for complex reasoning after initial steps\n\n10    if (stepNumber > 2 && messages.length > 10) {\n\n11      return {\n\n12        model: \"anthropic/claude-sonnet-4.5\",\n\n13      };\n\n14    }\n\n15    // Continue with default settings\n\n16    return {};\n\n17  },\n\n18});\n\n19\n\n20const result = await agent.generate({\n\n21  prompt: '...',\n\n22});\n```\n\n### [Context Management](https://ai-sdk.dev/docs/agents/loop-control\\#context-management)\n\nManage growing conversation history in long-running loops:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    // your tools\n\n7  },\n\n8  prepareStep: async ({ messages }) => {\n\n9    // Keep only recent messages to stay within context limits\n\n10    if (messages.length > 20) {\n\n11      return {\n\n12        messages: [\\\n\\\n13          messages[0], // Keep system instructions\\\n\\\n14          ...messages.slice(-10), // Keep last 10 messages\\\n\\\n15        ],\n\n16      };\n\n17    }\n\n18    return {};\n\n19  },\n\n20});\n\n21\n\n22const result = await agent.generate({\n\n23  prompt: '...',\n\n24});\n```\n\n### [Tool Selection](https://ai-sdk.dev/docs/agents/loop-control\\#tool-selection)\n\nControl which tools are available at each step:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    search: searchTool,\n\n7    analyze: analyzeTool,\n\n8    summarize: summarizeTool,\n\n9  },\n\n10  prepareStep: async ({ stepNumber, steps }) => {\n\n11    // Search phase (steps 0-2)\n\n12    if (stepNumber <= 2) {\n\n13      return {\n\n14        activeTools: ['search'],\n\n15        toolChoice: 'required',\n\n16      };\n\n17    }\n\n18\n\n19    // Analysis phase (steps 3-5)\n\n20    if (stepNumber <= 5) {\n\n21      return {\n\n22        activeTools: ['analyze'],\n\n23      };\n\n24    }\n\n25\n\n26    // Summary phase (step 6+)\n\n27    return {\n\n28      activeTools: ['summarize'],\n\n29      toolChoice: 'required',\n\n30    };\n\n31  },\n\n32});\n\n33\n\n34const result = await agent.generate({\n\n35  prompt: '...',\n\n36});\n```\n\nYou can also force a specific tool to be used:\n\n```ts\n1prepareStep: async ({ stepNumber }) => {\n\n2  if (stepNumber === 0) {\n\n3    // Force the search tool to be used first\n\n4    return {\n\n5      toolChoice: { type: 'tool', toolName: 'search' },\n\n6    };\n\n7  }\n\n8\n\n9  if (stepNumber === 5) {\n\n10    // Force the summarize tool after analysis\n\n11    return {\n\n12      toolChoice: { type: 'tool', toolName: 'summarize' },\n\n13    };\n\n14  }\n\n15\n\n16  return {};\n\n17};\n```\n\n### [Message Modification](https://ai-sdk.dev/docs/agents/loop-control\\#message-modification)\n\nTransform messages before sending them to the model:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2\n\n3const agent = new ToolLoopAgent({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  tools: {\n\n6    // your tools\n\n7  },\n\n8  prepareStep: async ({ messages, stepNumber }) => {\n\n9    // Summarize tool results to reduce token usage\n\n10    const processedMessages = messages.map(msg => {\n\n11      if (msg.role === 'tool' && msg.content.length > 1000) {\n\n12        return {\n\n13          ...msg,\n\n14          content: summarizeToolResult(msg.content),\n\n15        };\n\n16      }\n\n17      return msg;\n\n18    });\n\n19\n\n20    return { messages: processedMessages };\n\n21  },\n\n22});\n\n23\n\n24const result = await agent.generate({\n\n25  prompt: '...',\n\n26});\n```\n\n## [Access Step Information](https://ai-sdk.dev/docs/agents/loop-control\\#access-step-information)\n\nBoth `stopWhen` and `prepareStep` receive detailed information about the current execution:\n\n```ts\n1prepareStep: async ({\n\n2  model, // Current model configuration\n\n3  stepNumber, // Current step number (0-indexed)\n\n4  steps, // All previous steps with their results\n\n5  messages, // Messages to be sent to the model\n\n6}) => {\n\n7  // Access previous tool calls and results\n\n8  const previousToolCalls = steps.flatMap(step => step.toolCalls);\n\n9  const previousResults = steps.flatMap(step => step.toolResults);\n\n10\n\n11  // Make decisions based on execution history\n\n12  if (previousToolCalls.some(call => call.toolName === 'dataAnalysis')) {\n\n13    return {\n\n14      toolChoice: { type: 'tool', toolName: 'reportGenerator' },\n\n15    };\n\n16  }\n\n17\n\n18  return {};\n\n19},\n```\n\n## [Manual Loop Control](https://ai-sdk.dev/docs/agents/loop-control\\#manual-loop-control)\n\nFor scenarios requiring complete control over the agent loop, you can use AI SDK Core functions (`generateText` and `streamText`) to implement your own loop management instead of using `stopWhen` and `prepareStep`. This approach provides maximum flexibility for complex workflows.\n\n### [Implementing a Manual Loop](https://ai-sdk.dev/docs/agents/loop-control\\#implementing-a-manual-loop)\n\nBuild your own agent loop when you need full control over execution:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, ModelMessage } from 'ai';\n\n2\n\n3const messages: ModelMessage[] = [{ role: 'user', content: '...' }];\n\n4\n\n5let step = 0;\n\n6const maxSteps = 10;\n\n7\n\n8while (step < maxSteps) {\n\n9  const result = await generateText({\n\n10    model: \"anthropic/claude-sonnet-4.5\",\n\n11    messages,\n\n12    tools: {\n\n13      // your tools here\n\n14    },\n\n15  });\n\n16\n\n17  messages.push(...result.response.messages);\n\n18\n\n19  if (result.text) {\n\n20    break; // Stop when model generates text\n\n21  }\n\n22\n\n23  step++;\n\n24}\n```\n\nThis manual approach gives you complete control over:\n\n- Message history management\n- Step-by-step decision making\n- Custom stopping conditions\n- Dynamic tool and model selection\n- Error handling and recovery\n\n[Learn more about manual agent loops in the cookbook](https://ai-sdk.dev/cookbook/node/manual-agent-loop).\n\nOn this page\n\n[Loop Control](https://ai-sdk.dev/docs/agents/loop-control#loop-control)\n\n[Stop Conditions](https://ai-sdk.dev/docs/agents/loop-control#stop-conditions)\n\n[Use Built-in Conditions](https://ai-sdk.dev/docs/agents/loop-control#use-built-in-conditions)\n\n[Combine Multiple Conditions](https://ai-sdk.dev/docs/agents/loop-control#combine-multiple-conditions)\n\n[Create Custom Conditions](https://ai-sdk.dev/docs/agents/loop-control#create-custom-conditions)\n\n[Prepare Step](https://ai-sdk.dev/docs/agents/loop-control#prepare-step)\n\n[Dynamic Model Selection](https://ai-sdk.dev/docs/agents/loop-control#dynamic-model-selection)\n\n[Context Management](https://ai-sdk.dev/docs/agents/loop-control#context-management)\n\n[Tool Selection](https://ai-sdk.dev/docs/agents/loop-control#tool-selection)\n\n[Message Modification](https://ai-sdk.dev/docs/agents/loop-control#message-modification)\n\n[Access Step Information](https://ai-sdk.dev/docs/agents/loop-control#access-step-information)\n\n[Manual Loop Control](https://ai-sdk.dev/docs/agents/loop-control#manual-loop-control)\n\n[Implementing a Manual Loop](https://ai-sdk.dev/docs/agents/loop-control#implementing-a-manual-loop)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/agents/loop-control",
        "url": "https://ai-sdk.dev/docs/agents/loop-control",
        "url_citation": "https://ai-sdk.dev/docs/agents/loop-control"
      },
      {
        "title": "Configuring Call Options",
        "title_citation": "https://ai-sdk.dev/docs/agents/configuring-call-options",
        "content": "# [Configuring Call Options](https://ai-sdk.dev/docs/agents/configuring-call-options\\#configuring-call-options)\n\nCall options allow you to pass type-safe structured inputs to your agent. Use them to dynamically modify any agent setting based on the specific request.\n\n## [Why Use Call Options?](https://ai-sdk.dev/docs/agents/configuring-call-options\\#why-use-call-options)\n\nWhen you need agent behavior to change based on runtime context:\n\n- **Add dynamic context** \\- Inject retrieved documents, user preferences, or session data into prompts\n- **Select models dynamically** \\- Choose faster or more capable models based on request complexity\n- **Configure tools per request** \\- Pass user location to search tools or adjust tool behavior\n- **Customize provider options** \\- Set reasoning effort, temperature, or other provider-specific settings\n\nWithout call options, you'd need to create multiple agents or handle configuration logic outside the agent.\n\n## [How It Works](https://ai-sdk.dev/docs/agents/configuring-call-options\\#how-it-works)\n\nDefine call options in three steps:\n\n1. **Define the schema** \\- Specify what inputs you accept using `callOptionsSchema`\n2. **Configure with `prepareCall`** \\- Use those inputs to modify agent settings\n3. **Pass options at runtime** \\- Provide the options when calling `generate()` or `stream()`\n\n## [Basic Example](https://ai-sdk.dev/docs/agents/configuring-call-options\\#basic-example)\n\nAdd user context to your agent's prompt at runtime:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const supportAgent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  callOptionsSchema: z.object({\n\n7    userId: z.string(),\n\n8    accountType: z.enum(['free', 'pro', 'enterprise']),\n\n9  }),\n\n10  instructions: 'You are a helpful customer support agent.',\n\n11  prepareCall: ({ options, ...settings }) => ({\n\n12    ...settings,\n\n13    instructions:\n\n14      settings.instructions +\n\n15      `\\nUser context:\n\n16- Account type: ${options.accountType}\n\n17- User ID: ${options.userId}\n\n18\n\n19Adjust your response based on the user's account level.`,\n\n20  }),\n\n21});\n\n22\n\n23// Call the agent with specific user context\n\n24const result = await supportAgent.generate({\n\n25  prompt: 'How do I upgrade my account?',\n\n26  options: {\n\n27    userId: 'user_123',\n\n28    accountType: 'free',\n\n29  },\n\n30});\n```\n\nThe `options` parameter is now required and type-checked. If you don't provide it or pass incorrect types, TypeScript will error.\n\n## [Modifying Agent Settings](https://ai-sdk.dev/docs/agents/configuring-call-options\\#modifying-agent-settings)\n\nUse `prepareCall` to modify any agent setting. Return only the settings you want to change.\n\n### [Dynamic Model Selection](https://ai-sdk.dev/docs/agents/configuring-call-options\\#dynamic-model-selection)\n\nChoose models based on request characteristics:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const agent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\", // Default model\n\n6  callOptionsSchema: z.object({\n\n7    complexity: z.enum(['simple', 'complex']),\n\n8  }),\n\n9  prepareCall: ({ options, ...settings }) => ({\n\n10    ...settings,\n\n11    model:\n\n12      options.complexity === 'simple' ? 'openai/gpt-4o-mini' : 'openai/o1-mini',\n\n13  }),\n\n14});\n\n15\n\n16// Use faster model for simple queries\n\n17await agent.generate({\n\n18  prompt: 'What is 2+2?',\n\n19  options: { complexity: 'simple' },\n\n20});\n\n21\n\n22// Use more capable model for complex reasoning\n\n23await agent.generate({\n\n24  prompt: 'Explain quantum entanglement',\n\n25  options: { complexity: 'complex' },\n\n26});\n```\n\n### [Dynamic Tool Configuration](https://ai-sdk.dev/docs/agents/configuring-call-options\\#dynamic-tool-configuration)\n\nConfigure tools based on runtime context:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { ToolLoopAgent } from 'ai';\n\n3import { z } from 'zod';\n\n4\n\n5const newsAgent = new ToolLoopAgent({\n\n6  model: \"anthropic/claude-sonnet-4.5\",\n\n7  callOptionsSchema: z.object({\n\n8    userCity: z.string().optional(),\n\n9    userRegion: z.string().optional(),\n\n10  }),\n\n11  tools: {\n\n12    web_search: openai.tools.webSearch(),\n\n13  },\n\n14  prepareCall: ({ options, ...settings }) => ({\n\n15    ...settings,\n\n16    tools: {\n\n17      web_search: openai.tools.webSearch({\n\n18        searchContextSize: 'low',\n\n19        userLocation: {\n\n20          type: 'approximate',\n\n21          city: options.userCity,\n\n22          region: options.userRegion,\n\n23          country: 'US',\n\n24        },\n\n25      }),\n\n26    },\n\n27  }),\n\n28});\n\n29\n\n30await newsAgent.generate({\n\n31  prompt: 'What are the top local news stories?',\n\n32  options: {\n\n33    userCity: 'San Francisco',\n\n34    userRegion: 'California',\n\n35  },\n\n36});\n```\n\n### [Provider-Specific Options](https://ai-sdk.dev/docs/agents/configuring-call-options\\#provider-specific-options)\n\nConfigure provider settings dynamically:\n\n```ts\n1import { openai, OpenAIProviderOptions } from '@ai-sdk/openai';\n\n2import { ToolLoopAgent } from 'ai';\n\n3import { z } from 'zod';\n\n4\n\n5const agent = new ToolLoopAgent({\n\n6  model: 'openai/o3',\n\n7  callOptionsSchema: z.object({\n\n8    taskDifficulty: z.enum(['low', 'medium', 'high']),\n\n9  }),\n\n10  prepareCall: ({ options, ...settings }) => ({\n\n11    ...settings,\n\n12    providerOptions: {\n\n13      openai: {\n\n14        reasoningEffort: options.taskDifficulty,\n\n15      } satisfies OpenAIProviderOptions,\n\n16    },\n\n17  }),\n\n18});\n\n19\n\n20await agent.generate({\n\n21  prompt: 'Analyze this complex scenario...',\n\n22  options: { taskDifficulty: 'high' },\n\n23});\n```\n\n## [Advanced Patterns](https://ai-sdk.dev/docs/agents/configuring-call-options\\#advanced-patterns)\n\n### [Retrieval Augmented Generation (RAG)](https://ai-sdk.dev/docs/agents/configuring-call-options\\#retrieval-augmented-generation-rag)\n\nFetch relevant context and inject it into your prompt:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const ragAgent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  callOptionsSchema: z.object({\n\n7    query: z.string(),\n\n8  }),\n\n9  prepareCall: async ({ options, ...settings }) => {\n\n10    // Fetch relevant documents (this can be async)\n\n11    const documents = await vectorSearch(options.query);\n\n12\n\n13    return {\n\n14      ...settings,\n\n15      instructions: `Answer questions using the following context:\n\n16\n\n17${documents.map(doc => doc.content).join('\\n\\n')}`,\n\n18    };\n\n19  },\n\n20});\n\n21\n\n22await ragAgent.generate({\n\n23  prompt: 'What is our refund policy?',\n\n24  options: { query: 'refund policy' },\n\n25});\n```\n\nThe `prepareCall` function can be async, enabling you to fetch data before configuring the agent.\n\n### [Combining Multiple Modifications](https://ai-sdk.dev/docs/agents/configuring-call-options\\#combining-multiple-modifications)\n\nModify multiple settings together:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { ToolLoopAgent } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const agent = new ToolLoopAgent({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  callOptionsSchema: z.object({\n\n7    userRole: z.enum(['admin', 'user']),\n\n8    urgency: z.enum(['low', 'high']),\n\n9  }),\n\n10  tools: {\n\n11    readDatabase: readDatabaseTool,\n\n12    writeDatabase: writeDatabaseTool,\n\n13  },\n\n14  prepareCall: ({ options, ...settings }) => ({\n\n15    ...settings,\n\n16    // Upgrade model for urgent requests\n\n17    model: options.urgency === 'high' ? \"anthropic/claude-sonnet-4.5\" : settings.model,\n\n18    // Limit tools based on user role\n\n19    activeTools:\n\n20      options.userRole === 'admin'\n\n21        ? ['readDatabase', 'writeDatabase']\n\n22        : ['readDatabase'],\n\n23    // Adjust instructions\n\n24    instructions: `You are a ${options.userRole} assistant.\n\n25${options.userRole === 'admin' ? 'You have full database access.' : 'You have read-only access.'}`,\n\n26  }),\n\n27});\n\n28\n\n29await agent.generate({\n\n30  prompt: 'Update the user record',\n\n31  options: {\n\n32    userRole: 'admin',\n\n33    urgency: 'high',\n\n34  },\n\n35});\n```\n\n## [Using with createAgentUIStreamResponse](https://ai-sdk.dev/docs/agents/configuring-call-options\\#using-with-createagentuistreamresponse)\n\nPass call options through API routes to your agent:\n\napp/api/chat/route.ts\n\n```ts\n1import { createAgentUIStreamResponse } from 'ai';\n\n2import { myAgent } from '@/ai/agents/my-agent';\n\n3\n\n4export async function POST(request: Request) {\n\n5  const { messages, userId, accountType } = await request.json();\n\n6\n\n7  return createAgentUIStreamResponse({\n\n8    agent: myAgent,\n\n9    messages,\n\n10    options: {\n\n11      userId,\n\n12      accountType,\n\n13    },\n\n14  });\n\n15}\n```\n\n## [Next Steps](https://ai-sdk.dev/docs/agents/configuring-call-options\\#next-steps)\n\n- Learn about [loop control](https://ai-sdk.dev/docs/agents/loop-control) for execution management\n- Explore [workflow patterns](https://ai-sdk.dev/docs/agents/workflows) for complex multi-step processes\n\nOn this page\n\n[Configuring Call Options](https://ai-sdk.dev/docs/agents/configuring-call-options#configuring-call-options)\n\n[Why Use Call Options?](https://ai-sdk.dev/docs/agents/configuring-call-options#why-use-call-options)\n\n[How It Works](https://ai-sdk.dev/docs/agents/configuring-call-options#how-it-works)\n\n[Basic Example](https://ai-sdk.dev/docs/agents/configuring-call-options#basic-example)\n\n[Modifying Agent Settings](https://ai-sdk.dev/docs/agents/configuring-call-options#modifying-agent-settings)\n\n[Dynamic Model Selection](https://ai-sdk.dev/docs/agents/configuring-call-options#dynamic-model-selection)\n\n[Dynamic Tool Configuration](https://ai-sdk.dev/docs/agents/configuring-call-options#dynamic-tool-configuration)\n\n[Provider-Specific Options](https://ai-sdk.dev/docs/agents/configuring-call-options#provider-specific-options)\n\n[Advanced Patterns](https://ai-sdk.dev/docs/agents/configuring-call-options#advanced-patterns)\n\n[Retrieval Augmented Generation (RAG)](https://ai-sdk.dev/docs/agents/configuring-call-options#retrieval-augmented-generation-rag)\n\n[Combining Multiple Modifications](https://ai-sdk.dev/docs/agents/configuring-call-options#combining-multiple-modifications)\n\n[Using with createAgentUIStreamResponse](https://ai-sdk.dev/docs/agents/configuring-call-options#using-with-createagentuistreamresponse)\n\n[Next Steps](https://ai-sdk.dev/docs/agents/configuring-call-options#next-steps)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/agents/configuring-call-options",
        "url": "https://ai-sdk.dev/docs/agents/configuring-call-options",
        "url_citation": "https://ai-sdk.dev/docs/agents/configuring-call-options"
      }
    ],
    "ai_sdk_core": [
      {
        "title": "AI SDK Core",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
        "content": "# [AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core/overview\\#ai-sdk-core)\n\nLarge Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale.\nThey are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.\n\nAI SDK Core **simplifies working with LLMs by offering a standardized way of integrating them into your app** \\- so you can focus on building great AI applications for your users, not waste time on technical details.\n\nFor example, here’s how you can generate text with various models using the AI SDK:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```typescript\n1import { generateText } from \"ai\";\n\n2\n\n3const { text } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: \"What is love?\",\n\n6});\n```\n\nLove is a complex and multifaceted emotion that can be felt and expressed in many different ways. It involves deep affection, care, compassion, and connection towards another person or thing.\n\n## [AI SDK Core Functions](https://ai-sdk.dev/docs/ai-sdk-core/overview\\#ai-sdk-core-functions)\n\nAI SDK Core has various functions designed for [text generation](https://ai-sdk.dev/docs/ai-sdk-core/generating-text), [structured data generation](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data), and [tool usage](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling).\nThese functions take a standardized approach to setting up [prompts](https://ai-sdk.dev/docs/ai-sdk-core/prompts) and [settings](https://ai-sdk.dev/docs/ai-sdk-core/settings), making it easier to work with different models.\n\n- [`generateText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text): Generates text and [tool calls](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling).\nThis function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\n- [`streamText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text): Stream text and tool calls.\nYou can use the `streamText` function for interactive use cases such as [chat bots](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot) and [content streaming](https://ai-sdk.dev/docs/ai-sdk-ui/completion).\n- [`generateObject`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data): Generates a typed, structured object that matches a [Zod](https://zod.dev/) schema.\nYou can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\n- [`streamObject`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data): Stream a structured object that matches a Zod schema.\nYou can use this function to [stream generated UIs](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation).\n\n## [API Reference](https://ai-sdk.dev/docs/ai-sdk-core/overview\\#api-reference)\n\nPlease check out the [AI SDK Core API Reference](https://ai-sdk.dev/docs/reference/ai-sdk-core) for more details on each function.\n\nOn this page\n\n[AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core/overview#ai-sdk-core)\n\n[AI SDK Core Functions](https://ai-sdk.dev/docs/ai-sdk-core/overview#ai-sdk-core-functions)\n\n[API Reference](https://ai-sdk.dev/docs/ai-sdk-core/overview#api-reference)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/overview"
      },
      {
        "title": "Generating and Streaming Text",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
        "content": "# [Generating and Streaming Text](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#generating-and-streaming-text)\n\nLarge language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process.\nFor example, you can ask a model to come up with a recipe, draft an email, or summarize a document.\n\nThe AI SDK Core provides two functions to generate text and stream it from LLMs:\n\n- [`generateText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#generatetext): Generates text for a given prompt and model.\n- [`streamText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#streamtext): Streams text from a given prompt and model.\n\nAdvanced LLM features such as [tool calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling) and [structured data generation](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data) are built on top of text generation.\n\n## [`generateText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#generatetext)\n\nYou can generate text using the [`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text) function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { generateText } from 'ai';\n\n2\n\n3const { text } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n6});\n```\n\nYou can use more [advanced prompts](https://ai-sdk.dev/docs/ai-sdk-core/prompts) to generate text with more complex instructions and content:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { generateText } from 'ai';\n\n2\n\n3const { text } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  system:\n\n6    'You are a professional writer. ' +\n\n7    'You write simple, clear, and concise content.',\n\n8  prompt: `Summarize the following article in 3-5 sentences: ${article}`,\n\n9});\n```\n\nThe result object of `generateText` contains several promises that resolve when all required data is available:\n\n- `result.content`: The content that was generated in the last step.\n- `result.text`: The generated text.\n- `result.reasoning`: The full reasoning that the model has generated in the last step.\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\n- `result.files`: The files that were generated in the last step.\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\n- `result.toolCalls`: The tool calls that were made in the last step.\n- `result.toolResults`: The results of the tool calls from the last step.\n- `result.finishReason`: The reason the model finished generating text.\n- `result.rawFinishReason`: The raw reason why the generation finished (from the provider).\n- `result.usage`: The usage of the model during the final step of text generation.\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\n- `result.request`: Additional request information.\n- `result.response`: Additional response information, including response messages and body.\n- `result.providerMetadata`: Additional provider-specific metadata.\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\n- `result.output`: The generated structured output using the `output` specification.\n\n### [Accessing response headers & body](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#accessing-response-headers--body)\n\nSometimes you need access to the full response from the model provider,\ne.g. to access some provider-specific headers or body content.\n\nYou can access the raw response headers and body using the `response` property:\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  // ...\n\n5});\n\n6\n\n7console.log(JSON.stringify(result.response.headers, null, 2));\n\n8console.log(JSON.stringify(result.response.body, null, 2));\n```\n\n### [`onFinish` callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#onfinish-callback)\n\nWhen using `generateText`, you can provide an `onFinish` callback that is triggered after the last step is finished (\n[API Reference](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text#on-finish)\n).\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\n\n7    // your own logic, e.g. for saving the chat history or recording usage\n\n8\n\n9    const messages = response.messages; // messages that were generated\n\n10  },\n\n11});\n```\n\n## [`streamText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#streamtext)\n\nDepending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.\n\nAI SDK Core provides the [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) function which simplifies streaming text from LLMs:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6});\n\n7\n\n8// example: use textStream as an async iterable\n\n9for await (const textPart of result.textStream) {\n\n10  console.log(textPart);\n\n11}\n```\n\n`result.textStream` is both a `ReadableStream` and an `AsyncIterable`.\n\n`streamText` immediately starts streaming and suppresses errors to prevent\nserver crashes. Use the `onError` callback to log errors.\n\nYou can use `streamText` on its own or in combination with [AI SDK\\\\\nUI](https://ai-sdk.dev/examples/next-pages/basics/streaming-text-generation) and [AI SDK\\\\\nRSC](https://ai-sdk.dev/examples/next-app/basics/streaming-text-generation).\nThe result object contains several helper functions to make the integration into [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui) easier:\n\n- `result.toUIMessageStreamResponse()`: Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.\n- `result.pipeUIMessageStreamToResponse()`: Writes UI Message stream delta output to a Node.js response-like object.\n- `result.toTextStreamResponse()`: Creates a simple text stream HTTP response.\n- `result.pipeTextStreamToResponse()`: Writes text delta output to a Node.js response-like object.\n\n`streamText` is using backpressure and only generates tokens as they are\nrequested. You need to consume the stream in order for it to finish.\n\nIt also provides several promises that resolve when the stream is finished:\n\n- `result.content`: The content that was generated in the last step.\n- `result.text`: The generated text.\n- `result.reasoning`: The full reasoning that the model has generated.\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\n- `result.files`: Files that have been generated by the model in the last step.\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\n- `result.toolCalls`: The tool calls that have been executed in the last step.\n- `result.toolResults`: The tool results that have been generated in the last step.\n- `result.finishReason`: The reason the model finished generating text.\n- `result.rawFinishReason`: The raw reason why the generation finished (from the provider).\n- `result.usage`: The usage of the model during the final step of text generation.\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\n- `result.request`: Additional request information from the last step.\n- `result.response`: Additional response information from the last step.\n- `result.providerMetadata`: Additional provider-specific metadata from the last step.\n\n### [`onError` callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#onerror-callback)\n\n`streamText` immediately starts streaming to enable sending data without waiting for the model.\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\n\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { streamText } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6  onError({ error }) {\n\n7    console.error(error); // your error logging logic here\n\n8  },\n\n9});\n```\n\n### [`onChunk` callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#onchunk-callback)\n\nWhen using `streamText`, you can provide an `onChunk` callback that is triggered for each chunk of the stream.\n\nIt receives the following chunk types:\n\n- `text`\n- `reasoning`\n- `source`\n- `tool-call`\n- `tool-input-start`\n- `tool-input-delta`\n- `tool-result`\n- `raw`\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { streamText } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6  onChunk({ chunk }) {\n\n7    // implement your own logic here, e.g.:\n\n8    if (chunk.type === 'text') {\n\n9      console.log(chunk.text);\n\n10    }\n\n11  },\n\n12});\n```\n\n### [`onFinish` callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#onfinish-callback-1)\n\nWhen using `streamText`, you can provide an `onFinish` callback that is triggered when the stream is finished (\n[API Reference](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#on-finish)\n).\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { streamText } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\n\n7    // your own logic, e.g. for saving the chat history or recording usage\n\n8\n\n9    const messages = response.messages; // messages that were generated\n\n10  },\n\n11});\n```\n\n### [`fullStream` property](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#fullstream-property)\n\nYou can read a stream with all events using the `fullStream` property.\nThis can be useful if you want to implement your own UI or handle the stream in a different way.\nHere is an example of how to use the `fullStream` property:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { streamText } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const result = streamText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    cityAttractions: {\n\n8      inputSchema: z.object({ city: z.string() }),\n\n9      execute: async ({ city }) => ({\n\n10        attractions: ['attraction1', 'attraction2', 'attraction3'],\n\n11      }),\n\n12    },\n\n13  },\n\n14  prompt: 'What are some San Francisco tourist attractions?',\n\n15});\n\n16\n\n17for await (const part of result.fullStream) {\n\n18  switch (part.type) {\n\n19    case 'start': {\n\n20      // handle start of stream\n\n21      break;\n\n22    }\n\n23    case 'start-step': {\n\n24      // handle start of step\n\n25      break;\n\n26    }\n\n27    case 'text-start': {\n\n28      // handle text start\n\n29      break;\n\n30    }\n\n31    case 'text-delta': {\n\n32      // handle text delta here\n\n33      break;\n\n34    }\n\n35    case 'text-end': {\n\n36      // handle text end\n\n37      break;\n\n38    }\n\n39    case 'reasoning-start': {\n\n40      // handle reasoning start\n\n41      break;\n\n42    }\n\n43    case 'reasoning-delta': {\n\n44      // handle reasoning delta here\n\n45      break;\n\n46    }\n\n47    case 'reasoning-end': {\n\n48      // handle reasoning end\n\n49      break;\n\n50    }\n\n51    case 'source': {\n\n52      // handle source here\n\n53      break;\n\n54    }\n\n55    case 'file': {\n\n56      // handle file here\n\n57      break;\n\n58    }\n\n59    case 'tool-call': {\n\n60      switch (part.toolName) {\n\n61        case 'cityAttractions': {\n\n62          // handle tool call here\n\n63          break;\n\n64        }\n\n65      }\n\n66      break;\n\n67    }\n\n68    case 'tool-input-start': {\n\n69      // handle tool input start\n\n70      break;\n\n71    }\n\n72    case 'tool-input-delta': {\n\n73      // handle tool input delta\n\n74      break;\n\n75    }\n\n76    case 'tool-input-end': {\n\n77      // handle tool input end\n\n78      break;\n\n79    }\n\n80    case 'tool-result': {\n\n81      switch (part.toolName) {\n\n82        case 'cityAttractions': {\n\n83          // handle tool result here\n\n84          break;\n\n85        }\n\n86      }\n\n87      break;\n\n88    }\n\n89    case 'tool-error': {\n\n90      // handle tool error\n\n91      break;\n\n92    }\n\n93    case 'finish-step': {\n\n94      // handle finish step\n\n95      break;\n\n96    }\n\n97    case 'finish': {\n\n98      // handle finish here\n\n99      break;\n\n100    }\n\n101    case 'error': {\n\n102      // handle error here\n\n103      break;\n\n104    }\n\n105    case 'raw': {\n\n106      // handle raw value\n\n107      break;\n\n108    }\n\n109  }\n\n110}\n```\n\n### [Stream transformation](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#stream-transformation)\n\nYou can use the `experimental_transform` option to transform the stream.\nThis is useful for e.g. filtering, changing, or smoothing the text stream.\n\nThe transformations are applied before the callbacks are invoked and the promises are resolved.\nIf you e.g. have a transformation that changes all text to uppercase, the `onFinish` callback will receive the transformed text.\n\n#### [Smoothing streams](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#smoothing-streams)\n\nThe AI SDK Core provides a [`smoothStream` function](https://ai-sdk.dev/docs/reference/ai-sdk-core/smooth-stream) that\ncan be used to smooth out text streaming.\n\n```tsx\n1import { smoothStream, streamText } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  model,\n\n5  prompt,\n\n6  experimental_transform: smoothStream(),\n\n7});\n```\n\n#### [Custom transformations](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#custom-transformations)\n\nYou can also implement your own custom transformations.\nThe transformation function receives the tools that are available to the model,\nand returns a function that is used to transform the stream.\nTools can either be generic or limited to the tools that you are using.\n\nHere is an example of how to implement a custom transformation that converts\nall text to uppercase:\n\n```ts\n1const upperCaseTransform =\n\n2  <TOOLS extends ToolSet>() =>\n\n3  (options: { tools: TOOLS; stopStream: () => void }) =>\n\n4    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n\n5      transform(chunk, controller) {\n\n6        controller.enqueue(\n\n7          // for text chunks, convert the text to uppercase:\n\n8          chunk.type === 'text'\n\n9            ? { ...chunk, text: chunk.text.toUpperCase() }\n\n10            : chunk,\n\n11        );\n\n12      },\n\n13    });\n```\n\nYou can also stop the stream using the `stopStream` function.\nThis is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.\n\nWhen you invoke `stopStream`, it is important to simulate the `step-finish` and `finish` events to guarantee that a well-formed stream is returned\nand all callbacks are invoked.\n\n```ts\n1const stopWordTransform =\n\n2  <TOOLS extends ToolSet>() =>\n\n3  ({ stopStream }: { stopStream: () => void }) =>\n\n4    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n\n5      // note: this is a simplified transformation for testing;\n\n6      // in a real-world version more there would need to be\n\n7      // stream buffering and scanning to correctly emit prior text\n\n8      // and to detect all STOP occurrences.\n\n9      transform(chunk, controller) {\n\n10        if (chunk.type !== 'text') {\n\n11          controller.enqueue(chunk);\n\n12          return;\n\n13        }\n\n14\n\n15        if (chunk.text.includes('STOP')) {\n\n16          // stop the stream\n\n17          stopStream();\n\n18\n\n19          // simulate the finish-step event\n\n20          controller.enqueue({\n\n21            type: 'finish-step',\n\n22            finishReason: 'stop',\n\n23            logprobs: undefined,\n\n24            usage: {\n\n25              completionTokens: NaN,\n\n26              promptTokens: NaN,\n\n27              totalTokens: NaN,\n\n28            },\n\n29            request: {},\n\n30            response: {\n\n31              id: 'response-id',\n\n32              modelId: 'mock-model-id',\n\n33              timestamp: new Date(0),\n\n34            },\n\n35            warnings: [],\n\n36            isContinued: false,\n\n37          });\n\n38\n\n39          // simulate the finish event\n\n40          controller.enqueue({\n\n41            type: 'finish',\n\n42            finishReason: 'stop',\n\n43            logprobs: undefined,\n\n44            usage: {\n\n45              completionTokens: NaN,\n\n46              promptTokens: NaN,\n\n47              totalTokens: NaN,\n\n48            },\n\n49            response: {\n\n50              id: 'response-id',\n\n51              modelId: 'mock-model-id',\n\n52              timestamp: new Date(0),\n\n53            },\n\n54          });\n\n55\n\n56          return;\n\n57        }\n\n58\n\n59        controller.enqueue(chunk);\n\n60      },\n\n61    });\n```\n\n#### [Multiple transformations](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#multiple-transformations)\n\nYou can also provide multiple transformations. They are applied in the order they are provided.\n\n```tsx\n1const result = streamText({\n\n2  model,\n\n3  prompt,\n\n4  experimental_transform: [firstTransform, secondTransform],\n\n5});\n```\n\n## [Sources](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#sources)\n\nSome providers such as [Perplexity](https://ai-sdk.dev/providers/ai-sdk-providers/perplexity#sources) and\n[Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\n\nCurrently sources are limited to web pages that ground the response.\nYou can access them using the `sources` property of the result.\n\nEach `url` source contains the following properties:\n\n- `id`: The ID of the source.\n- `url`: The URL of the source.\n- `title`: The optional title of the source.\n- `providerMetadata`: Provider metadata for the source.\n\nWhen you use `generateText`, you can access the sources using the `sources` property:\n\n```ts\n1const result = await generateText({\n\n2  model: 'google/gemini-2.5-flash',\n\n3  tools: {\n\n4    google_search: google.tools.googleSearch({}),\n\n5  },\n\n6  prompt: 'List the top 5 San Francisco news from the past week.',\n\n7});\n\n8\n\n9for (const source of result.sources) {\n\n10  if (source.sourceType === 'url') {\n\n11    console.log('ID:', source.id);\n\n12    console.log('Title:', source.title);\n\n13    console.log('URL:', source.url);\n\n14    console.log('Provider metadata:', source.providerMetadata);\n\n15    console.log();\n\n16  }\n\n17}\n```\n\nWhen you use `streamText`, you can access the sources using the `fullStream` property:\n\n```tsx\n1const result = streamText({\n\n2  model: 'google/gemini-2.5-flash',\n\n3  tools: {\n\n4    google_search: google.tools.googleSearch({}),\n\n5  },\n\n6  prompt: 'List the top 5 San Francisco news from the past week.',\n\n7});\n\n8\n\n9for await (const part of result.fullStream) {\n\n10  if (part.type === 'source' && part.sourceType === 'url') {\n\n11    console.log('ID:', part.id);\n\n12    console.log('Title:', part.title);\n\n13    console.log('URL:', part.url);\n\n14    console.log('Provider metadata:', part.providerMetadata);\n\n15    console.log();\n\n16  }\n\n17}\n```\n\nThe sources are also available in the `result.sources` promise.\n\n## [Examples](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#examples)\n\nYou can see `generateText` and `streamText` in action using various frameworks in the following examples:\n\n### [`generateText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#generatetext-1)\n\n[Learn to generate text in Node.js](https://ai-sdk.dev/examples/node/generating-text/generate-text) [Learn to generate text in Next.js with Route Handlers (AI SDK UI)](https://ai-sdk.dev/examples/next-pages/basics/generating-text) [Learn to generate text in Next.js with Server Actions (AI SDK RSC)](https://ai-sdk.dev/examples/next-app/basics/generating-text)\n\n### [`streamText`](https://ai-sdk.dev/docs/ai-sdk-core/generating-text\\#streamtext-1)\n\n[Learn to stream text in Node.js](https://ai-sdk.dev/examples/node/generating-text/stream-text) [Learn to stream text in Next.js with Route Handlers (AI SDK UI)](https://ai-sdk.dev/examples/next-pages/basics/streaming-text-generation) [Learn to stream text in Next.js with Server Actions (AI SDK RSC)](https://ai-sdk.dev/examples/next-app/basics/streaming-text-generation)\n\nOn this page\n\n[Generating and Streaming Text](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#generating-and-streaming-text)\n\n[generateText](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#generatetext)\n\n[Accessing response headers & body](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#accessing-response-headers--body)\n\n[onFinish callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#onfinish-callback)\n\n[streamText](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#streamtext)\n\n[onError callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#onerror-callback)\n\n[onChunk callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#onchunk-callback)\n\n[onFinish callback](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#onfinish-callback-1)\n\n[fullStream property](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#fullstream-property)\n\n[Stream transformation](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#stream-transformation)\n\n[Smoothing streams](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#smoothing-streams)\n\n[Custom transformations](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#custom-transformations)\n\n[Multiple transformations](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#multiple-transformations)\n\n[Sources](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#sources)\n\n[Examples](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#examples)\n\n[generateText](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#generatetext-1)\n\n[streamText](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#streamtext-1)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text"
      },
      {
        "title": "Generating Structured Data",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
        "content": "# [Generating Structured Data](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generating-structured-data)\n\nWhile text generation can be useful, your use case will likely call for generating structured data.\nFor example, you might want to extract information from text, classify data, or generate synthetic data.\n\nMany language models are capable of generating structured data, often defined as using \"JSON modes\" or \"tools\".\nHowever, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.\n\nThe AI SDK standardises structured object generation across model providers\nusing the `output` property on [`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text)\nand [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text).\nYou can use [Zod schemas](https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema), [Valibot](https://ai-sdk.dev/docs/reference/ai-sdk-core/valibot-schema), or [JSON schemas](https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema) to specify the shape of the data that you want,\nand the AI model will generate data that conforms to that structure.\n\nStructured output generation is part of the `generateText` and `streamText`\nflow. This means you can combine it with tool calling in the same request.\n\n## [Generating Structured Outputs](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generating-structured-outputs)\n\nUse `generateText` with `Output.object()` to generate structured data from a prompt.\nThe schema is also used to validate the generated data, ensuring type safety and correctness.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      recipe: z.object({\n\n9        name: z.string(),\n\n10        ingredients: z.array(\n\n11          z.object({ name: z.string(), amount: z.string() }),\n\n12        ),\n\n13        steps: z.array(z.string()),\n\n14      }),\n\n15    }),\n\n16  }),\n\n17  prompt: 'Generate a lasagna recipe.',\n\n18});\n```\n\nStructured output generation counts as a step in the AI SDK's multi-turn\nexecution model (where each model call or tool execution is one step). When\ncombining with tools, account for this in your `stopWhen` configuration.\n\n### [Accessing response headers & body](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#accessing-response-headers--body)\n\nSometimes you need access to the full response from the model provider,\ne.g. to access some provider-specific headers or body content.\n\nYou can access the raw response headers and body using the `response` property:\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  // ...\n\n5  output: Output.object({ schema }),\n\n6});\n\n7\n\n8console.log(JSON.stringify(result.response.headers, null, 2));\n\n9console.log(JSON.stringify(result.response.body, null, 2));\n```\n\n## [Stream Structured Outputs](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#stream-structured-outputs)\n\nGiven the added complexity of returning structured data, model response time can be unacceptable for your interactive use case.\nWith `streamText` and `output`, you can stream the model's structured response as it is generated.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { partialOutputStream } = streamText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      recipe: z.object({\n\n9        name: z.string(),\n\n10        ingredients: z.array(\n\n11          z.object({ name: z.string(), amount: z.string() }),\n\n12        ),\n\n13        steps: z.array(z.string()),\n\n14      }),\n\n15    }),\n\n16  }),\n\n17  prompt: 'Generate a lasagna recipe.',\n\n18});\n\n19\n\n20// use partialOutputStream as an async iterable\n\n21for await (const partialObject of partialOutputStream) {\n\n22  console.log(partialObject);\n\n23}\n```\n\nYou can consume the structured output on the client with the [`useObject`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) hook.\n\n### [Error Handling in Streams](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#error-handling-in-streams)\n\n`streamText` starts streaming immediately. When errors occur during streaming, they become part of the stream rather than thrown exceptions (to prevent stream crashes).\n\nTo handle errors, provide an `onError` callback:\n\n```tsx\n1import { streamText, Output } from 'ai';\n\n2\n\n3const result = streamText({\n\n4  // ...\n\n5  output: Output.object({ schema }),\n\n6  onError({ error }) {\n\n7    console.error(error); // log to your error tracking service\n\n8  },\n\n9});\n```\n\nFor non-streaming error handling with `generateText`, see the [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#error-handling) section below.\n\n## [Output Types](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#output-types)\n\nThe AI SDK supports multiple ways of specifying the expected structure of generated data via the `Output` object. You can select from various strategies for structured/text generation and validation.\n\n### [`Output.text()`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#outputtext)\n\nUse `Output.text()` to generate plain text from a model. This option doesn't enforce any schema on the result: you simply receive the model's text as a string. This is the default behavior when no `output` is specified.\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2\n\n3const { output } = await generateText({\n\n4  // ...\n\n5  output: Output.text(),\n\n6  prompt: 'Tell me a joke.',\n\n7});\n\n8// output will be a string (the joke)\n```\n\n### [`Output.object()`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#outputobject)\n\nUse `Output.object({ schema })` to generate a structured object based on a schema (for example, a Zod schema). The output is type-validated to ensure the returned result matches the schema.\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  // ...\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      name: z.string(),\n\n9      age: z.number().nullable(),\n\n10      labels: z.array(z.string()),\n\n11    }),\n\n12  }),\n\n13  prompt: 'Generate information for a test user.',\n\n14});\n\n15// output will be an object matching the schema above\n```\n\nPartial outputs streamed via `streamText` cannot be validated against your\nprovided schema, as incomplete data may not yet conform to the expected\nstructure.\n\n### [`Output.array()`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#outputarray)\n\nUse `Output.array({ element })` to specify that you expect an array of typed objects from the model, where each element should conform to a schema (defined in the `element` property).\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  // ...\n\n6  output: Output.array({\n\n7    element: z.object({\n\n8      location: z.string(),\n\n9      temperature: z.number(),\n\n10      condition: z.string(),\n\n11    }),\n\n12  }),\n\n13  prompt: 'List the weather for San Francisco and Paris.',\n\n14});\n\n15// output will be an array of objects like:\n\n16// [\\\n\\\n17//   { location: 'San Francisco', temperature: 70, condition: 'Sunny' },\\\n\\\n18//   { location: 'Paris', temperature: 65, condition: 'Cloudy' },\\\n\\\n19// ]\n```\n\nWith `streamText`, you can also iterate over the generated array elements using `elementStream`:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { elementStream } = streamText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.array({\n\n7    element: z.object({\n\n8      name: z.string(),\n\n9      class: z\n\n10        .string()\n\n11        .describe('Character class, e.g. warrior, mage, or thief.'),\n\n12      description: z.string(),\n\n13    }),\n\n14  }),\n\n15  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n\n16});\n\n17\n\n18for await (const hero of elementStream) {\n\n19  console.log(hero);\n\n20}\n```\n\nEach element emitted by `elementStream` is complete and validated against your\nelement schema, ensuring type safety for each item as it is generated.\n\n### [`Output.choice()`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#outputchoice)\n\nUse `Output.choice({ options })` when you expect the model to choose from a specific set of string options, such as for classification or fixed-enum answers.\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2\n\n3const { output } = await generateText({\n\n4  // ...\n\n5  output: Output.choice({\n\n6    options: ['sunny', 'rainy', 'snowy'],\n\n7  }),\n\n8  prompt: 'Is the weather sunny, rainy, or snowy today?',\n\n9});\n\n10// output will be one of: 'sunny', 'rainy', or 'snowy'\n```\n\nYou can provide any set of string options, and the output will always be a single string value that matches one of the specified options. The AI SDK validates that the result matches one of your options, and will throw if the model returns something invalid.\n\nThis is especially useful for making classification-style generations or forcing valid values for API compatibility.\n\n### [`Output.json()`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#outputjson)\n\nUse `Output.json()` when you want to generate and parse unstructured JSON values from the model, without enforcing a specific schema. This is useful if you want to capture arbitrary objects, flexible structures, or when you want to rely on the model's natural output rather than rigid validation.\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2\n\n3const { output } = await generateText({\n\n4  // ...\n\n5  output: Output.json(),\n\n6  prompt:\n\n7    'For each city, return the current temperature and weather condition as a JSON object.',\n\n8});\n\n9\n\n10// output could be any valid JSON, for example:\n\n11// {\n\n12//   \"San Francisco\": { \"temperature\": 70, \"condition\": \"Sunny\" },\n\n13//   \"Paris\": { \"temperature\": 65, \"condition\": \"Cloudy\" }\n\n14// }\n```\n\nWith `Output.json`, the AI SDK only checks that the response is valid JSON; it doesn't validate the structure or types of the values. If you need schema validation, use the `.object` or `.array` outputs instead.\n\nFor more advanced validation or different structures, see [the Output API reference](https://ai-sdk.dev/docs/reference/ai-sdk-core/output).\n\n## [Generating Structured Outputs with Tools](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generating-structured-outputs-with-tools)\n\nOne of the key advantages of using structured output with `generateText` and `streamText` is the ability to combine it with tool calling.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, Output, tool, stepCountIs } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather for a location',\n\n9      inputSchema: z.object({ location: z.string() }),\n\n10      execute: async ({ location }) => {\n\n11        // fetch weather data\n\n12        return { temperature: 72, condition: 'sunny' };\n\n13      },\n\n14    }),\n\n15  },\n\n16  output: Output.object({\n\n17    schema: z.object({\n\n18      summary: z.string(),\n\n19      recommendation: z.string(),\n\n20    }),\n\n21  }),\n\n22  stopWhen: stepCountIs(5),\n\n23  prompt: 'What should I wear in San Francisco today?',\n\n24});\n```\n\nWhen using tools with structured output, remember that generating the\nstructured output counts as a step. Configure `stopWhen` to allow enough steps\nfor both tool execution and output generation.\n\n## [Property Descriptions](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#property-descriptions)\n\nYou can add `.describe(\"...\")` to individual schema properties to give the model hints about what each property is for. This helps improve the quality and accuracy of generated structured data:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      name: z.string().describe('The name of the recipe'),\n\n9      ingredients: z\n\n10        .array(\n\n11          z.object({\n\n12            name: z.string(),\n\n13            amount: z\n\n14              .string()\n\n15              .describe('The amount of the ingredient (grams or ml)'),\n\n16          }),\n\n17        )\n\n18        .describe('List of ingredients with amounts'),\n\n19      steps: z.array(z.string()).describe('Step-by-step cooking instructions'),\n\n20    }),\n\n21  }),\n\n22  prompt: 'Generate a lasagna recipe.',\n\n23});\n```\n\nProperty descriptions are particularly useful for:\n\n- Clarifying ambiguous property names\n- Specifying expected formats or conventions\n- Providing context for complex nested structures\n\n## [Output Name and Description](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#output-name-and-description)\n\nYou can optionally specify a `name` and `description` for the output. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { output } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: Output.object({\n\n7    name: 'Recipe',\n\n8    description: 'A recipe for a dish.',\n\n9    schema: z.object({\n\n10      name: z.string(),\n\n11      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n\n12      steps: z.array(z.string()),\n\n13    }),\n\n14  }),\n\n15  prompt: 'Generate a lasagna recipe.',\n\n16});\n```\n\nThis works with all output types that support structured generation:\n\n- `Output.object({ name, description, schema })`\n- `Output.array({ name, description, element })`\n- `Output.choice({ name, description, options })`\n- `Output.json({ name, description })`\n\n## [Accessing Reasoning](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#accessing-reasoning)\n\nYou can access the reasoning used by the language model to generate the object via the `reasoning` property on the result. This property contains a string with the model's thought process, if available.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, Output } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const result = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\", // must be a reasoning model\n\n6  output: Output.object({\n\n7    schema: z.object({\n\n8      recipe: z.object({\n\n9        name: z.string(),\n\n10        ingredients: z.array(\n\n11          z.object({\n\n12            name: z.string(),\n\n13            amount: z.string(),\n\n14          }),\n\n15        ),\n\n16        steps: z.array(z.string()),\n\n17      }),\n\n18    }),\n\n19  }),\n\n20  prompt: 'Generate a lasagna recipe.',\n\n21});\n\n22\n\n23console.log(result.reasoning);\n```\n\n## [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#error-handling)\n\nWhen `generateText` with structured output cannot generate a valid object, it throws a [`AI_NoObjectGeneratedError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-object-generated-error).\n\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema.\nIt can arise due to the following reasons:\n\n- The model failed to generate a response.\n- The model generated a response that could not be parsed.\n- The model generated a response that could not be validated against the schema.\n\nThe error preserves the following information to help you log the issue:\n\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\n- `response`: Metadata about the language model response, including response id, timestamp, and model.\n- `usage`: Request token usage.\n- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\n\n```ts\n1import { generateText, Output, NoObjectGeneratedError } from 'ai';\n\n2\n\n3try {\n\n4  await generateText({\n\n5    model,\n\n6    output: Output.object({ schema }),\n\n7    prompt,\n\n8  });\n\n9} catch (error) {\n\n10  if (NoObjectGeneratedError.isInstance(error)) {\n\n11    console.log('NoObjectGeneratedError');\n\n12    console.log('Cause:', error.cause);\n\n13    console.log('Text:', error.text);\n\n14    console.log('Response:', error.response);\n\n15    console.log('Usage:', error.usage);\n\n16  }\n\n17}\n```\n\n## [generateObject and streamObject (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generateobject-and-streamobject-legacy)\n\n`generateObject` and `streamObject` are deprecated. Use `generateText` and\n`streamText` with the `output` property instead. The legacy functions will be\nremoved in a future major version.\n\nThe `generateObject` and `streamObject` functions are the legacy way to generate structured data. They work similarly to `generateText` and `streamText` with `Output.object()`, but as standalone functions.\n\n### [generateObject](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generateobject)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { object } = await generateObject({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  schema: z.object({\n\n7    recipe: z.object({\n\n8      name: z.string(),\n\n9      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n\n10      steps: z.array(z.string()),\n\n11    }),\n\n12  }),\n\n13  prompt: 'Generate a lasagna recipe.',\n\n14});\n```\n\n### [streamObject](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#streamobject)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { partialObjectStream } = streamObject({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  schema: z.object({\n\n7    recipe: z.object({\n\n8      name: z.string(),\n\n9      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n\n10      steps: z.array(z.string()),\n\n11    }),\n\n12  }),\n\n13  prompt: 'Generate a lasagna recipe.',\n\n14});\n\n15\n\n16for await (const partialObject of partialObjectStream) {\n\n17  console.log(partialObject);\n\n18}\n```\n\n### [Schema Name and Description (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#schema-name-and-description-legacy)\n\nYou can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { object } = await generateObject({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  schemaName: 'Recipe',\n\n7  schemaDescription: 'A recipe for a dish.',\n\n8  schema: z.object({\n\n9    name: z.string(),\n\n10    ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\n\n11    steps: z.array(z.string()),\n\n12  }),\n\n13  prompt: 'Generate a lasagna recipe.',\n\n14});\n```\n\n### [Output Strategy (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#output-strategy-legacy)\n\nThe legacy functions support different output strategies via the `output` parameter:\n\n#### [Array](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#array)\n\nGenerate an array of objects. The schema specifies the shape of an array element.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamObject } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const { elementStream } = streamObject({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  output: 'array',\n\n7  schema: z.object({\n\n8    name: z.string(),\n\n9    class: z\n\n10      .string()\n\n11      .describe('Character class, e.g. warrior, mage, or thief.'),\n\n12    description: z.string(),\n\n13  }),\n\n14  prompt: 'Generate 3 hero descriptions for a fantasy role playing game.',\n\n15});\n\n16\n\n17for await (const hero of elementStream) {\n\n18  console.log(hero);\n\n19}\n```\n\n#### [Enum](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#enum)\n\nGenerate a specific enum value for classification tasks.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject } from 'ai';\n\n2\n\n3const { object } = await generateObject({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  output: 'enum',\n\n6  enum: ['action', 'comedy', 'drama', 'horror', 'sci-fi'],\n\n7  prompt:\n\n8    'Classify the genre of this movie plot: ' +\n\n9    '\"A group of astronauts travel through a wormhole in search of a ' +\n\n10    'new habitable planet for humanity.\"',\n\n11});\n```\n\n#### [No Schema](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#no-schema)\n\nGenerate unstructured JSON without a schema.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateObject } from 'ai';\n\n2\n\n3const { object } = await generateObject({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  output: 'no-schema',\n\n6  prompt: 'Generate a lasagna recipe.',\n\n7});\n```\n\n### [Repairing Invalid JSON (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#repairing-invalid-json-legacy)\n\nThe `repairText` function is experimental and may change in the future.\n\nSometimes the model will generate invalid or malformed JSON.\nYou can use the `repairText` function to attempt to repair the JSON.\n\n```ts\n1import { generateObject } from 'ai';\n\n2\n\n3const { object } = await generateObject({\n\n4  model,\n\n5  schema,\n\n6  prompt,\n\n7  experimental_repairText: async ({ text, error }) => {\n\n8    // example: add a closing brace to the text\n\n9    return text + '}';\n\n10  },\n\n11});\n```\n\n## [More Examples](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#more-examples)\n\nYou can see `generateObject` and `streamObject` in action using various frameworks in the following examples:\n\n### [`generateObject`](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#generateobject-1)\n\n[Learn to generate objects in Node.js](https://ai-sdk.dev/examples/node/generating-structured-data/generate-object) [Learn to generate objects in Next.js with Route Handlers (AI SDK UI)](https://ai-sdk.dev/examples/next-pages/basics/generating-object) [Learn to generate objects in Next.js with Server Actions (AI SDK RSC)](https://ai-sdk.dev/examples/next-app/basics/generating-object)\n\n### [`streamText` with Output](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data\\#streamtext-with-output)\n\n[Learn to stream objects in Node.js](https://ai-sdk.dev/examples/node/streaming-structured-data/stream-object) [Learn to stream objects in Next.js with Route Handlers (AI SDK UI)](https://ai-sdk.dev/examples/next-pages/basics/streaming-object-generation) [Learn to stream objects in Next.js with Server Actions (AI SDK RSC)](https://ai-sdk.dev/examples/next-app/basics/streaming-object-generation)\n\nOn this page\n\n[Generating Structured Data](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generating-structured-data)\n\n[Generating Structured Outputs](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generating-structured-outputs)\n\n[Accessing response headers & body](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#accessing-response-headers--body)\n\n[Stream Structured Outputs](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#stream-structured-outputs)\n\n[Error Handling in Streams](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#error-handling-in-streams)\n\n[Output Types](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#output-types)\n\n[Output.text()](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#outputtext)\n\n[Output.object()](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#outputobject)\n\n[Output.array()](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#outputarray)\n\n[Output.choice()](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#outputchoice)\n\n[Output.json()](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#outputjson)\n\n[Generating Structured Outputs with Tools](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generating-structured-outputs-with-tools)\n\n[Property Descriptions](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#property-descriptions)\n\n[Output Name and Description](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#output-name-and-description)\n\n[Accessing Reasoning](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#accessing-reasoning)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#error-handling)\n\n[generateObject and streamObject (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generateobject-and-streamobject-legacy)\n\n[generateObject](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generateobject)\n\n[streamObject](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#streamobject)\n\n[Schema Name and Description (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#schema-name-and-description-legacy)\n\n[Output Strategy (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#output-strategy-legacy)\n\n[Array](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#array)\n\n[Enum](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#enum)\n\n[No Schema](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#no-schema)\n\n[Repairing Invalid JSON (Legacy)](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#repairing-invalid-json-legacy)\n\n[More Examples](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#more-examples)\n\n[generateObject](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#generateobject-1)\n\n[streamText with Output](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data#streamtext-with-output)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data"
      },
      {
        "title": "Tool Calling",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
        "content": "# [Tool Calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-calling)\n\nAs covered under Foundations, [tools](https://ai-sdk.dev/docs/foundations/tools) are objects that can be called by the model to perform a specific task.\nAI SDK Core tools contain several core elements:\n\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\n- **`inputSchema`**: A [Zod schema](https://ai-sdk.dev/docs/foundations/tools#schemas) or a [JSON schema](https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema) that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\n- **`execute`**: An optional async function that is called with the inputs from the tool call. It produces a value of type `RESULT` (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.\n- **`strict`**: _(optional, boolean)_ Enables strict tool calling when supported by the provider\n\nYou can use the [`tool`](https://ai-sdk.dev/docs/reference/ai-sdk-core/tool) helper function to\ninfer the types of the `execute` parameters.\n\nThe `tools` parameter of `generateText` and `streamText` is an object that has the tool names as keys and the tools as values:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { z } from 'zod';\n\n2import { generateText, tool, stopWhen } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather in a location',\n\n9      inputSchema: z.object({\n\n10        location: z.string().describe('The location to get the weather for'),\n\n11      }),\n\n12      execute: async ({ location }) => ({\n\n13        location,\n\n14        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n15      }),\n\n16    }),\n\n17  },\n\n18  stopWhen: stepCountIs(5),\n\n19  prompt: 'What is the weather in San Francisco?',\n\n20});\n```\n\nWhen a model uses a tool, it is called a \"tool call\" and the output of the\ntool is called a \"tool result\".\n\nTool calling is not restricted to only text generation.\nYou can also use it to render user interfaces (Generative UI).\n\n## [Strict Mode](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#strict-mode)\n\nWhen enabled, language model providers that support strict tool calling will only generate tool calls that are valid according to your defined `inputSchema`.\nThis increases the reliability of tool calling.\nHowever, not all schemas may be supported in strict mode, and what is supported depends on the specific provider.\n\nBy default, strict mode is disabled. You can enable it per-tool by setting `strict: true`:\n\n```ts\n1tool({\n\n2  description: 'Get the weather in a location',\n\n3  inputSchema: z.object({\n\n4    location: z.string(),\n\n5  }),\n\n6  strict: true, // Enable strict validation for this tool\n\n7  execute: async ({ location }) => ({\n\n8    // ...\n\n9  }),\n\n10});\n```\n\nNot all providers or models support strict mode. For those that do not, this\noption is ignored.\n\n## [Input Examples](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#input-examples)\n\nYou can specify example inputs for your tools to help guide the model on how input data should be structured.\nWhen supported by providers, input examples can help when JSON schema itself does not fully specify the intended\nusage or when there are optional values.\n\n```ts\n1tool({\n\n2  description: 'Get the weather in a location',\n\n3  inputSchema: z.object({\n\n4    location: z.string().describe('The location to get the weather for'),\n\n5  }),\n\n6  inputExamples: [\\\n\\\n7    { input: { location: 'San Francisco' } },\\\n\\\n8    { input: { location: 'London' } },\\\n\\\n9  ],\n\n10  execute: async ({ location }) => {\n\n11    // ...\n\n12  },\n\n13});\n```\n\nOnly the Anthropic providers supports tool input examples natively. Other\nproviders ignore the setting.\n\n## [Tool Execution Approval](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-execution-approval)\n\nBy default, tools with an `execute` function run automatically as the model calls them. You can require approval before execution by setting `needsApproval`:\n\n```ts\n1import { tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const runCommand = tool({\n\n5  description: 'Run a shell command',\n\n6  inputSchema: z.object({\n\n7    command: z.string().describe('The shell command to execute'),\n\n8  }),\n\n9  needsApproval: true,\n\n10  execute: async ({ command }) => {\n\n11    // your command execution logic here\n\n12  },\n\n13});\n```\n\nThis is useful for tools that perform sensitive operations like executing commands, processing payments, modifying data, and more potentially dangerous actions.\n\n### [How It Works](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#how-it-works)\n\nWhen a tool requires approval, `generateText` and `streamText` don't pause execution. Instead, they complete and return `tool-approval-request` parts in the result content. This means the approval flow requires two calls to the model: the first returns the approval request, and the second (after receiving the approval response) either executes the tool or informs the model that approval was denied.\n\nHere's the complete flow:\n\n1. Call `generateText` with a tool that has `needsApproval: true`\n2. Model generates a tool call\n3. `generateText` returns with `tool-approval-request` parts in `result.content`\n4. Your app requests an approval and collects the user's decision\n5. Add a `tool-approval-response` to the messages array\n6. Call `generateText` again with the updated messages\n7. If approved, the tool runs and returns a result. If denied, the model sees the denial and responds accordingly.\n\n### [Handling Approval Requests](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#handling-approval-requests)\n\nAfter calling `generateText` or `streamText`, check `result.content` for `tool-approval-request` parts:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { type ModelMessage, generateText } from 'ai';\n\n2\n\n3const messages: ModelMessage[] = [\\\n\\\n4  { role: 'user', content: 'Remove the most recent file' },\\\n\\\n5];\n\n6const result = await generateText({\n\n7  model: \"anthropic/claude-sonnet-4.5\",\n\n8  tools: { runCommand },\n\n9  messages,\n\n10});\n\n11\n\n12messages.push(...result.response.messages);\n\n13\n\n14for (const part of result.content) {\n\n15  if (part.type === 'tool-approval-request') {\n\n16    console.log(part.approvalId); // Unique ID for this approval request\n\n17    console.log(part.toolCall); // Contains toolName, input, etc.\n\n18  }\n\n19}\n```\n\nTo respond, create a `tool-approval-response` and add it to your messages:\n\n```ts\n1import { type ToolApprovalResponse } from 'ai';\n\n2\n\n3const approvals: ToolApprovalResponse[] = [];\n\n4\n\n5for (const part of result.content) {\n\n6  if (part.type === 'tool-approval-request') {\n\n7    const response: ToolApprovalResponse = {\n\n8      type: 'tool-approval-response',\n\n9      approvalId: part.approvalId,\n\n10      approved: true, // or false to deny\n\n11      reason: 'User confirmed the command', // Optional context for the model\n\n12    };\n\n13    approvals.push(response);\n\n14  }\n\n15}\n\n16\n\n17// add approvals to messages\n\n18messages.push({ role: 'tool', content: approvals });\n```\n\nThen call `generateText` again with the updated messages. If approved, the tool executes. If denied, the model receives the denial and can respond accordingly.\n\nWhen a tool execution is denied, consider adding a system instruction like\n\"When a tool execution is not approved, do not retry it\" to prevent the model\nfrom attempting the same call again.\n\n### [Dynamic Approval](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#dynamic-approval)\n\nYou can make approval decisions based on tool input by providing an async function:\n\n```ts\n1const paymentTool = tool({\n\n2  description: 'Process a payment',\n\n3  inputSchema: z.object({\n\n4    amount: z.number(),\n\n5    recipient: z.string(),\n\n6  }),\n\n7  needsApproval: async ({ amount }) => amount > 1000,\n\n8  execute: async ({ amount, recipient }) => {\n\n9    return await processPayment(amount, recipient);\n\n10  },\n\n11});\n```\n\nIn this example, only transactions over $1000 require approval. Smaller transactions execute automatically.\n\n### [Tool Execution Approval with useChat](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-execution-approval-with-usechat)\n\nWhen using `useChat`, the approval flow is handled through UI state. See [Chatbot Tool Usage](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#tool-execution-approval) for details on handling approvals in your UI with `addToolApprovalResponse`.\n\n## [Multi-Step Calls (using stopWhen)](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#multi-step-calls-using-stopwhen)\n\nWith the `stopWhen` setting, you can enable multi-step calls in `generateText` and `streamText`. When `stopWhen` is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.\n\nThe `stopWhen` conditions are only evaluated when the last step contains tool\nresults.\n\nBy default, when you use `generateText` or `streamText`, it triggers a single generation. This works well for many use cases where you can rely on the model's training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, its generation is complete and that step is finished.\n\nYou may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.\n\nYou can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.\n\n### [Example](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#example)\n\nIn the following example, there are two steps:\n\n1. **Step 1**\n1. The prompt `'What is the weather in San Francisco?'` is sent to the model.\n2. The model generates a tool call.\n3. The tool call is executed.\n2. **Step 2**\n1. The tool result is sent to the model.\n2. The model generates a response considering the tool result.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { z } from 'zod';\n\n2import { generateText, tool, stepCountIs } from 'ai';\n\n3\n\n4const { text, steps } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather in a location',\n\n9      inputSchema: z.object({\n\n10        location: z.string().describe('The location to get the weather for'),\n\n11      }),\n\n12      execute: async ({ location }) => ({\n\n13        location,\n\n14        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n15      }),\n\n16    }),\n\n17  },\n\n18  stopWhen: stepCountIs(5), // stop after a maximum of 5 steps if tools were called\n\n19  prompt: 'What is the weather in San Francisco?',\n\n20});\n```\n\nYou can use `streamText` in a similar way.\n\n### [Steps](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#steps)\n\nTo access intermediate tool calls and results, you can use the `steps` property in the result object\nor the `streamText``onFinish` callback.\nIt contains all the text, tool calls, tool results, and more from each step.\n\n#### [Example: Extract tool results from all steps](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#example-extract-tool-results-from-all-steps)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const { steps } = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  stopWhen: stepCountIs(10),\n\n6  // ...\n\n7});\n\n8\n\n9// extract all tool calls from the steps:\n\n10const allToolCalls = steps.flatMap(step => step.toolCalls);\n```\n\n### [`onStepFinish` callback](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#onstepfinish-callback)\n\nWhen using `generateText` or `streamText`, you can provide an `onStepFinish` callback that\nis triggered when a step is finished,\ni.e. all text deltas, tool calls, and tool results for the step are available.\nWhen you have multiple steps, the callback is triggered for each step.\n\n```tsx\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  // ...\n\n5  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\n\n6    // your own logic, e.g. for saving the chat history or recording usage\n\n7  },\n\n8});\n```\n\n### [`prepareStep` callback](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#preparestep-callback)\n\nThe `prepareStep` callback is called before a step is started.\n\nIt is called with the following parameters:\n\n- `model`: The model that was passed into `generateText`.\n- `stopWhen`: The stopping condition that was passed into `generateText`.\n- `stepNumber`: The number of the step that is being executed.\n- `steps`: The steps that have been executed so far.\n- `messages`: The messages that will be sent to the model for the current step.\n- `experimental_context`: The context passed via the `experimental_context` setting (experimental).\n\nYou can use it to provide different settings for a step, including modifying the input messages.\n\n```tsx\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  // ...\n\n5  prepareStep: async ({ model, stepNumber, steps, messages }) => {\n\n6    if (stepNumber === 0) {\n\n7      return {\n\n8        // use a different model for this step:\n\n9        model: modelForThisParticularStep,\n\n10        // force a tool choice for this step:\n\n11        toolChoice: { type: 'tool', toolName: 'tool1' },\n\n12        // limit the tools that are available for this step:\n\n13        activeTools: ['tool1'],\n\n14      };\n\n15    }\n\n16\n\n17    // when nothing is returned, the default settings are used\n\n18  },\n\n19});\n```\n\n#### [Message Modification for Longer Agentic Loops](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#message-modification-for-longer-agentic-loops)\n\nIn longer agentic loops, you can use the `messages` parameter to modify the input messages for each step. This is particularly useful for prompt compression:\n\n```tsx\n1prepareStep: async ({ stepNumber, steps, messages }) => {\n\n2  // Compress conversation history for longer loops\n\n3  if (messages.length > 20) {\n\n4    return {\n\n5      messages: messages.slice(-10),\n\n6    };\n\n7  }\n\n8\n\n9  return {};\n\n10},\n```\n\n#### [Provider Options for Step Configuration](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#provider-options-for-step-configuration)\n\nYou can use `providerOptions` in `prepareStep` to pass provider-specific configuration for each step. This is useful for features like Anthropic's code execution container persistence:\n\n```tsx\n1import { forwardAnthropicContainerIdFromLastStep } from '@ai-sdk/anthropic';\n\n2\n\n3// Propagate container ID from previous step for code execution continuity\n\n4prepareStep: forwardAnthropicContainerIdFromLastStep,\n```\n\n## [Response Messages](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#response-messages)\n\nAdding the generated assistant and tool messages to your conversation history is a common task,\nespecially if you are using multi-step tool calls.\n\nBoth `generateText` and `streamText` have a `response.messages` property that you can use to\nadd the assistant and tool messages to your conversation history.\nIt is also available in the `onFinish` callback of `streamText`.\n\nThe `response.messages` property contains an array of `ModelMessage` objects that you can add to your conversation history:\n\n```ts\n1import { generateText, ModelMessage } from 'ai';\n\n2\n\n3const messages: ModelMessage[] = [\\\n\\\n4  // ...\\\n\\\n5];\n\n6\n\n7const { response } = await generateText({\n\n8  // ...\n\n9  messages,\n\n10});\n\n11\n\n12// add the response messages to your conversation history:\n\n13messages.push(...response.messages); // streamText: ...((await response).messages)\n```\n\n## [Dynamic Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#dynamic-tools)\n\nAI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:\n\n- MCP (Model Context Protocol) tools without schemas\n- User-defined functions at runtime\n- Tools loaded from external sources\n\n### [Using dynamicTool](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#using-dynamictool)\n\nThe `dynamicTool` helper creates tools with unknown input/output types:\n\n```ts\n1import { dynamicTool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const customTool = dynamicTool({\n\n5  description: 'Execute a custom function',\n\n6  inputSchema: z.object({}),\n\n7  execute: async input => {\n\n8    // input is typed as 'unknown'\n\n9    // You need to validate/cast it at runtime\n\n10    const { action, parameters } = input as any;\n\n11\n\n12    // Execute your dynamic logic\n\n13    return { result: `Executed ${action}` };\n\n14  },\n\n15});\n```\n\n### [Type-Safe Handling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#type-safe-handling)\n\nWhen using both static and dynamic tools, use the `dynamic` flag for type narrowing:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  tools: {\n\n4    // Static tool with known types\n\n5    weather: weatherTool,\n\n6    // Dynamic tool\n\n7    custom: dynamicTool({\n\n8      /* ... */\n\n9    }),\n\n10  },\n\n11  onStepFinish: ({ toolCalls, toolResults }) => {\n\n12    // Type-safe iteration\n\n13    for (const toolCall of toolCalls) {\n\n14      if (toolCall.dynamic) {\n\n15        // Dynamic tool: input is 'unknown'\n\n16        console.log('Dynamic:', toolCall.toolName, toolCall.input);\n\n17        continue;\n\n18      }\n\n19\n\n20      // Static tool: full type inference\n\n21      switch (toolCall.toolName) {\n\n22        case 'weather':\n\n23          console.log(toolCall.input.location); // typed as string\n\n24          break;\n\n25      }\n\n26    }\n\n27  },\n\n28});\n```\n\n## [Preliminary Tool Results](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#preliminary-tool-results)\n\nYou can return an `AsyncIterable` over multiple results.\nIn this case, the last value from the iterable is the final tool result.\n\nThis can be used in combination with generator functions to e.g. stream status information\nduring the tool execution:\n\n```ts\n1tool({\n\n2  description: 'Get the current weather.',\n\n3  inputSchema: z.object({\n\n4    location: z.string(),\n\n5  }),\n\n6  async *execute({ location }) {\n\n7    yield {\n\n8      status: 'loading' as const,\n\n9      text: `Getting weather for ${location}`,\n\n10      weather: undefined,\n\n11    };\n\n12\n\n13    await new Promise(resolve => setTimeout(resolve, 3000));\n\n14\n\n15    const temperature = 72 + Math.floor(Math.random() * 21) - 10;\n\n16\n\n17    yield {\n\n18      status: 'success' as const,\n\n19      text: `The weather in ${location} is ${temperature}°F`,\n\n20      temperature,\n\n21    };\n\n22  },\n\n23});\n```\n\n## [Tool Choice](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-choice)\n\nYou can use the `toolChoice` setting to influence when a tool is selected.\nIt supports the following settings:\n\n- `auto` (default): the model can choose whether and which tools to call.\n- `required`: the model must call a tool. It can choose which tool to call.\n- `none`: the model must not call tools\n- `{ type: 'tool', toolName: string (typed) }`: the model must call the specified tool\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { z } from 'zod';\n\n2import { generateText, tool } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather in a location',\n\n9      inputSchema: z.object({\n\n10        location: z.string().describe('The location to get the weather for'),\n\n11      }),\n\n12      execute: async ({ location }) => ({\n\n13        location,\n\n14        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n15      }),\n\n16    }),\n\n17  },\n\n18  toolChoice: 'required', // force the model to call a tool\n\n19  prompt: 'What is the weather in San Francisco?',\n\n20});\n```\n\n## [Tool Execution Options](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-execution-options)\n\nWhen tools are called, they receive additional options as a second parameter.\n\n### [Tool Call ID](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-call-id)\n\nThe ID of the tool call is forwarded to the tool execution.\nYou can use it e.g. when sending tool-call related information with stream data.\n\n```ts\n1import {\n\n2  streamText,\n\n3  tool,\n\n4  createUIMessageStream,\n\n5  createUIMessageStreamResponse,\n\n6} from 'ai';\n\n7\n\n8export async function POST(req: Request) {\n\n9  const { messages } = await req.json();\n\n10\n\n11  const stream = createUIMessageStream({\n\n12    execute: ({ writer }) => {\n\n13      const result = streamText({\n\n14        // ...\n\n15        messages,\n\n16        tools: {\n\n17          myTool: tool({\n\n18            // ...\n\n19            execute: async (args, { toolCallId }) => {\n\n20              // return e.g. custom status for tool call\n\n21              writer.write({\n\n22                type: 'data-tool-status',\n\n23                id: toolCallId,\n\n24                data: {\n\n25                  name: 'myTool',\n\n26                  status: 'in-progress',\n\n27                },\n\n28              });\n\n29              // ...\n\n30            },\n\n31          }),\n\n32        },\n\n33      });\n\n34\n\n35      writer.merge(result.toUIMessageStream());\n\n36    },\n\n37  });\n\n38\n\n39  return createUIMessageStreamResponse({ stream });\n\n40}\n```\n\n### [Messages](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#messages)\n\nThe messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution.\nYou can access them in the second parameter of the `execute` function.\nIn multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.\n\n```ts\n1import { generateText, tool } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  // ...\n\n5  tools: {\n\n6    myTool: tool({\n\n7      // ...\n\n8      execute: async (args, { messages }) => {\n\n9        // use the message history in e.g. calls to other language models\n\n10        return { ... };\n\n11      },\n\n12    }),\n\n13  },\n\n14});\n```\n\n### [Abort Signals](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#abort-signals)\n\nThe abort signals from `generateText` and `streamText` are forwarded to the tool execution.\nYou can access them in the second parameter of the `execute` function and e.g. abort long-running computations or forward them to fetch calls inside tools.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { z } from 'zod';\n\n2import { generateText, tool } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  abortSignal: myAbortSignal, // signal that will be forwarded to tools\n\n7  tools: {\n\n8    weather: tool({\n\n9      description: 'Get the weather in a location',\n\n10      inputSchema: z.object({ location: z.string() }),\n\n11      execute: async ({ location }, { abortSignal }) => {\n\n12        return fetch(\n\n13          `https://api.weatherapi.com/v1/current.json?q=${location}`,\n\n14          { signal: abortSignal }, // forward the abort signal to fetch\n\n15        );\n\n16      },\n\n17    }),\n\n18  },\n\n19  prompt: 'What is the weather in San Francisco?',\n\n20});\n```\n\n### [Context (experimental)](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#context-experimental)\n\nYou can pass in arbitrary context from `generateText` or `streamText` via the `experimental_context` setting.\nThis context is available in the `experimental_context` tool execution option.\n\n```ts\n1const result = await generateText({\n\n2  // ...\n\n3  tools: {\n\n4    someTool: tool({\n\n5      // ...\n\n6      execute: async (input, { experimental_context: context }) => {\n\n7        const typedContext = context as { example: string }; // or use type validation library\n\n8        // ...\n\n9      },\n\n10    }),\n\n11  },\n\n12  experimental_context: { example: '123' },\n\n13});\n```\n\n## [Tool Input Lifecycle Hooks](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-input-lifecycle-hooks)\n\nThe following tool input lifecycle hooks are available:\n\n- **`onInputStart`**: Called when the model starts generating the input (arguments) for the tool call\n- **`onInputDelta`**: Called for each chunk of text as the input is streamed\n- **`onInputAvailable`**: Called when the complete input is available and validated\n\n`onInputStart` and `onInputDelta` are only called in streaming contexts (when using `streamText`). They are not called when using `generateText`.\n\n### [Example](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#example-1)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const result = streamText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: {\n\n7    getWeather: tool({\n\n8      description: 'Get the weather in a location',\n\n9      inputSchema: z.object({\n\n10        location: z.string().describe('The location to get the weather for'),\n\n11      }),\n\n12      execute: async ({ location }) => ({\n\n13        temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n14      }),\n\n15      onInputStart: () => {\n\n16        console.log('Tool call starting');\n\n17      },\n\n18      onInputDelta: ({ inputTextDelta }) => {\n\n19        console.log('Received input chunk:', inputTextDelta);\n\n20      },\n\n21      onInputAvailable: ({ input }) => {\n\n22        console.log('Complete input:', input);\n\n23      },\n\n24    }),\n\n25  },\n\n26  prompt: 'What is the weather in San Francisco?',\n\n27});\n```\n\n## [Types](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#types)\n\nModularizing your code often requires defining types to ensure type safety and reusability.\nTo enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.\n\nYou can use them to strongly type your variables, function parameters, and return types\nin parts of the code that are not directly related to `streamText` or `generateText`.\n\nEach tool call is typed with `ToolCall<NAME extends string, ARGS>`, depending\non the tool that has been invoked.\nSimilarly, the tool results are typed with `ToolResult<NAME extends string, ARGS, RESULT>`.\n\nThe tools in `streamText` and `generateText` are defined as a `ToolSet`.\nThe type inference helpers `TypedToolCall<TOOLS extends ToolSet>`\nand `TypedToolResult<TOOLS extends ToolSet>` can be used to\nextract the tool call and tool result types from the tools.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { TypedToolCall, TypedToolResult, generateText, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const myToolSet = {\n\n5  firstTool: tool({\n\n6    description: 'Greets the user',\n\n7    inputSchema: z.object({ name: z.string() }),\n\n8    execute: async ({ name }) => `Hello, ${name}!`,\n\n9  }),\n\n10  secondTool: tool({\n\n11    description: 'Tells the user their age',\n\n12    inputSchema: z.object({ age: z.number() }),\n\n13    execute: async ({ age }) => `You are ${age} years old!`,\n\n14  }),\n\n15};\n\n16\n\n17type MyToolCall = TypedToolCall<typeof myToolSet>;\n\n18type MyToolResult = TypedToolResult<typeof myToolSet>;\n\n19\n\n20async function generateSomething(prompt: string): Promise<{\n\n21  text: string;\n\n22  toolCalls: Array<MyToolCall>; // typed tool calls\n\n23  toolResults: Array<MyToolResult>; // typed tool results\n\n24}> {\n\n25  return generateText({\n\n26    model: \"anthropic/claude-sonnet-4.5\",\n\n27    tools: myToolSet,\n\n28    prompt,\n\n29  });\n\n30}\n```\n\n## [Handling Errors](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#handling-errors)\n\nThe AI SDK has three tool-call related errors:\n\n- [`NoSuchToolError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-tool-error): the model tries to call a tool that is not defined in the tools object\n- [`InvalidToolInputError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error): the model calls a tool with inputs that do not match the tool's input schema\n- [`ToolCallRepairError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-repair-error): an error that occurred during tool call repair\n\nWhen tool execution fails (errors thrown by your tool's `execute` function), the AI SDK adds them as `tool-error` content parts to enable automated LLM roundtrips in multi-step scenarios.\n\n### [`generateText`](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#generatetext)\n\n`generateText` throws errors for tool schema validation issues and other errors, and can be handled using a `try`/`catch` block. Tool execution errors appear as `tool-error` parts in the result steps:\n\n```ts\n1try {\n\n2  const result = await generateText({\n\n3    //...\n\n4  });\n\n5} catch (error) {\n\n6  if (NoSuchToolError.isInstance(error)) {\n\n7    // handle the no such tool error\n\n8  } else if (InvalidToolInputError.isInstance(error)) {\n\n9    // handle the invalid tool inputs error\n\n10  } else {\n\n11    // handle other errors\n\n12  }\n\n13}\n```\n\nTool execution errors are available in the result steps:\n\n```ts\n1const { steps } = await generateText({\n\n2  // ...\n\n3});\n\n4\n\n5// check for tool errors in the steps\n\n6const toolErrors = steps.flatMap(step =>\n\n7  step.content.filter(part => part.type === 'tool-error'),\n\n8);\n\n9\n\n10toolErrors.forEach(toolError => {\n\n11  console.log('Tool error:', toolError.error);\n\n12  console.log('Tool name:', toolError.toolName);\n\n13  console.log('Tool input:', toolError.input);\n\n14});\n```\n\n### [`streamText`](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#streamtext)\n\n`streamText` sends errors as part of the full stream. Tool execution errors appear as `tool-error` parts, while other errors appear as `error` parts.\n\nWhen using `toUIMessageStreamResponse`, you can pass an `onError` function to extract the error message from the error part and forward it as part of the stream response:\n\n```ts\n1const result = streamText({\n\n2  // ...\n\n3});\n\n4\n\n5return result.toUIMessageStreamResponse({\n\n6  onError: error => {\n\n7    if (NoSuchToolError.isInstance(error)) {\n\n8      return 'The model tried to call a unknown tool.';\n\n9    } else if (InvalidToolInputError.isInstance(error)) {\n\n10      return 'The model called a tool with invalid inputs.';\n\n11    } else {\n\n12      return 'An unknown error occurred.';\n\n13    }\n\n14  },\n\n15});\n```\n\n## [Tool Call Repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#tool-call-repair)\n\nThe tool call repair feature is experimental and may change in the future.\n\nLanguage models sometimes fail to generate valid tool calls,\nespecially when the input schema is complex or the model is smaller.\n\nIf you use multiple steps, those failed tool calls will be sent back to the LLM\nin the next step to give it an opportunity to fix it.\nHowever, you may want to control how invalid tool calls are repaired without requiring\nadditional steps that pollute the message history.\n\nYou can use the `experimental_repairToolCall` function to attempt to repair the tool call\nwith a custom function.\n\nYou can use different strategies to repair the tool call:\n\n- Use a model with structured outputs to generate the inputs.\n- Send the messages, system prompt, and tool schema to a stronger model to generate the inputs.\n- Provide more specific repair instructions based on which tool was called.\n\n### [Example: Use a model with structured outputs for repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#example-use-a-model-with-structured-outputs-for-repair)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model,\n\n6  tools,\n\n7  prompt,\n\n8\n\n9  experimental_repairToolCall: async ({\n\n10    toolCall,\n\n11    tools,\n\n12    inputSchema,\n\n13    error,\n\n14  }) => {\n\n15    if (NoSuchToolError.isInstance(error)) {\n\n16      return null; // do not attempt to fix invalid tool names\n\n17    }\n\n18\n\n19    const tool = tools[toolCall.toolName as keyof typeof tools];\n\n20\n\n21    const { object: repairedArgs } = await generateObject({\n\n22      model: \"anthropic/claude-sonnet-4.5\",\n\n23      schema: tool.inputSchema,\n\n24      prompt: [\\\n\\\n25        `The model tried to call the tool \"${toolCall.toolName}\"` +\\\n\\\n26          ` with the following inputs:`,\\\n\\\n27        JSON.stringify(toolCall.input),\\\n\\\n28        `The tool accepts the following schema:`,\\\n\\\n29        JSON.stringify(inputSchema(toolCall)),\\\n\\\n30        'Please fix the inputs.',\\\n\\\n31      ].join('\\n'),\n\n32    });\n\n33\n\n34    return { ...toolCall, input: JSON.stringify(repairedArgs) };\n\n35  },\n\n36});\n```\n\n### [Example: Use the re-ask strategy for repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#example-use-the-re-ask-strategy-for-repair)\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateObject, generateText, NoSuchToolError, tool } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model,\n\n6  tools,\n\n7  prompt,\n\n8\n\n9  experimental_repairToolCall: async ({\n\n10    toolCall,\n\n11    tools,\n\n12    error,\n\n13    messages,\n\n14    system,\n\n15  }) => {\n\n16    const result = await generateText({\n\n17      model,\n\n18      system,\n\n19      messages: [\\\n\\\n20        ...messages,\\\n\\\n21        {\\\n\\\n22          role: 'assistant',\\\n\\\n23          content: [\\\n\\\n24            {\\\n\\\n25              type: 'tool-call',\\\n\\\n26              toolCallId: toolCall.toolCallId,\\\n\\\n27              toolName: toolCall.toolName,\\\n\\\n28              input: toolCall.input,\\\n\\\n29            },\\\n\\\n30          ],\\\n\\\n31        },\\\n\\\n32        {\\\n\\\n33          role: 'tool' as const,\\\n\\\n34          content: [\\\n\\\n35            {\\\n\\\n36              type: 'tool-result',\\\n\\\n37              toolCallId: toolCall.toolCallId,\\\n\\\n38              toolName: toolCall.toolName,\\\n\\\n39              output: error.message,\\\n\\\n40            },\\\n\\\n41          ],\\\n\\\n42        },\\\n\\\n43      ],\n\n44      tools,\n\n45    });\n\n46\n\n47    const newToolCall = result.toolCalls.find(\n\n48      newToolCall => newToolCall.toolName === toolCall.toolName,\n\n49    );\n\n50\n\n51    return newToolCall != null\n\n52      ? {\n\n53          toolCallType: 'function' as const,\n\n54          toolCallId: toolCall.toolCallId,\n\n55          toolName: toolCall.toolName,\n\n56          input: JSON.stringify(newToolCall.input),\n\n57        }\n\n58      : null;\n\n59  },\n\n60});\n```\n\n## [Active Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#active-tools)\n\nLanguage models can only handle a limited number of tools at a time, depending on the model.\nTo allow for static typing using a large number of tools and limiting the available tools to the model at the same time,\nthe AI SDK provides the `activeTools` property.\n\nIt is an array of tool names that are currently active.\nBy default, the value is `undefined` and all tools are active.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateText } from 'ai';\n\n3\n\n4const { text } = await generateText({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  tools: myToolSet,\n\n7  activeTools: ['firstTool'],\n\n8});\n```\n\n## [Multi-modal Tool Results](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#multi-modal-tool-results)\n\nMulti-modal tool results are experimental and only supported by Anthropic and\nOpenAI.\n\nIn order to send multi-modal tool results, e.g. screenshots, back to the model,\nthey need to be converted into a specific format.\n\nAI SDK Core tools have an optional `toModelOutput` function\nthat converts the tool result into a content part.\n\nHere is an example for converting a screenshot into a content part:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  tools: {\n\n4    computer: anthropic.tools.computer_20241022({\n\n5      // ...\n\n6      async execute({ action, coordinate, text }) {\n\n7        switch (action) {\n\n8          case 'screenshot': {\n\n9            return {\n\n10              type: 'image',\n\n11              data: fs\n\n12                .readFileSync('./data/screenshot-editor.png')\n\n13                .toString('base64'),\n\n14            };\n\n15          }\n\n16          default: {\n\n17            return `executed ${action}`;\n\n18          }\n\n19        }\n\n20      },\n\n21\n\n22      // map to tool result content for LLM consumption:\n\n23      toModelOutput({ output }) {\n\n24        return {\n\n25          type: 'content',\n\n26          value:\n\n27            typeof output === 'string'\n\n28              ? [{ type: 'text', text: output }]\n\n29              : [{ type: 'media', data: output.data, mediaType: 'image/png' }],\n\n30        };\n\n31      },\n\n32    }),\n\n33  },\n\n34  // ...\n\n35});\n```\n\n## [Extracting Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#extracting-tools)\n\nOnce you start having many tools, you might want to extract them into separate files.\nThe `tool` helper function is crucial for this, because it ensures correct type inference.\n\nHere is an example of an extracted tool:\n\ntools/weather-tool.ts\n\n```ts\n1import { tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4// the `tool` helper function ensures correct type inference:\n\n5export const weatherTool = tool({\n\n6  description: 'Get the weather in a location',\n\n7  inputSchema: z.object({\n\n8    location: z.string().describe('The location to get the weather for'),\n\n9  }),\n\n10  execute: async ({ location }) => ({\n\n11    location,\n\n12    temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n13  }),\n\n14});\n```\n\n## [MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#mcp-tools)\n\nThe AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools.\nMCP enables your AI applications to discover and use tools across various services through a standardized interface.\n\nFor detailed information about MCP tools, including initialization, transport options, and usage patterns, see the [MCP Tools documentation](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools).\n\n### [AI SDK Tools vs MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#ai-sdk-tools-vs-mcp-tools)\n\nIn most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.\n\n| Aspect | AI SDK Tools | MCP Tools |\n| --- | --- | --- |\n| **Type Safety** | Full static typing end-to-end | Dynamic discovery at runtime |\n| **Execution** | Same process as your request (low latency) | Separate server (network overhead) |\n| **Prompt Control** | Full control over descriptions and schemas | Controlled by MCP server owner |\n| **Schema Control** | You define and optimize for your model | Controlled by MCP server owner |\n| **Version Management** | Full visibility over updates | Can update independently (version skew risk) |\n| **Authentication** | Same process, no additional auth required | Separate server introduces additional auth complexity |\n| **Best For** | Production applications requiring control and performance | Development iteration, user-provided tools |\n\n## [Examples](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling\\#examples)\n\nYou can see tools in action using various frameworks in the following examples:\n\n[Learn to use tools in Node.js](https://ai-sdk.dev/cookbook/node/call-tools) [Learn to use tools in Next.js with Route Handlers](https://ai-sdk.dev/cookbook/next/call-tools) [Learn to use MCP tools in Node.js](https://ai-sdk.dev/cookbook/node/mcp-tools)\n\nOn this page\n\n[Tool Calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-calling)\n\n[Strict Mode](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#strict-mode)\n\n[Input Examples](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#input-examples)\n\n[Tool Execution Approval](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-execution-approval)\n\n[How It Works](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#how-it-works)\n\n[Handling Approval Requests](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#handling-approval-requests)\n\n[Dynamic Approval](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#dynamic-approval)\n\n[Tool Execution Approval with useChat](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-execution-approval-with-usechat)\n\n[Multi-Step Calls (using stopWhen)](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls-using-stopwhen)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#example)\n\n[Steps](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#steps)\n\n[Example: Extract tool results from all steps](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#example-extract-tool-results-from-all-steps)\n\n[onStepFinish callback](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#onstepfinish-callback)\n\n[prepareStep callback](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#preparestep-callback)\n\n[Message Modification for Longer Agentic Loops](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#message-modification-for-longer-agentic-loops)\n\n[Provider Options for Step Configuration](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#provider-options-for-step-configuration)\n\n[Response Messages](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#response-messages)\n\n[Dynamic Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#dynamic-tools)\n\n[Using dynamicTool](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#using-dynamictool)\n\n[Type-Safe Handling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#type-safe-handling)\n\n[Preliminary Tool Results](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#preliminary-tool-results)\n\n[Tool Choice](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-choice)\n\n[Tool Execution Options](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-execution-options)\n\n[Tool Call ID](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-call-id)\n\n[Messages](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#messages)\n\n[Abort Signals](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#abort-signals)\n\n[Context (experimental)](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#context-experimental)\n\n[Tool Input Lifecycle Hooks](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-input-lifecycle-hooks)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#example-1)\n\n[Types](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#types)\n\n[Handling Errors](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#handling-errors)\n\n[generateText](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#generatetext)\n\n[streamText](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#streamtext)\n\n[Tool Call Repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-call-repair)\n\n[Example: Use a model with structured outputs for repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#example-use-a-model-with-structured-outputs-for-repair)\n\n[Example: Use the re-ask strategy for repair](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#example-use-the-re-ask-strategy-for-repair)\n\n[Active Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#active-tools)\n\n[Multi-modal Tool Results](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#multi-modal-tool-results)\n\n[Extracting Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#extracting-tools)\n\n[MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#mcp-tools)\n\n[AI SDK Tools vs MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#ai-sdk-tools-vs-mcp-tools)\n\n[Examples](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#examples)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling"
      },
      {
        "title": "Model Context Protocol (MCP)",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools",
        "content": "# [Model Context Protocol (MCP)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#model-context-protocol-mcp)\n\nThe AI SDK supports connecting to [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers to access their tools, resources, and prompts.\nThis enables your AI applications to discover and use capabilities across various services through a standardized interface.\n\nIf you're using OpenAI's Responses API, you can also use the built-in\n`openai.tools.mcp` tool, which provides direct MCP server integration without\nneeding to convert tools. See the [OpenAI provider\\\\\ndocumentation](https://ai-sdk.dev/providers/ai-sdk-providers/openai#mcp-tool) for details.\n\n## [Initializing an MCP Client](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#initializing-an-mcp-client)\n\nWe recommend using HTTP transport (like `StreamableHTTPClientTransport`) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.\n\nCreate an MCP client using one of the following transport options:\n\n- **HTTP transport (Recommended)**: Either configure HTTP directly via the client using `transport: { type: 'http', ... }`, or use MCP's official TypeScript SDK `StreamableHTTPClientTransport`\n- SSE (Server-Sent Events): An alternative HTTP-based transport\n- `stdio`: For local development only. Uses standard input/output streams for local MCP servers\n\n### [HTTP Transport (Recommended)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#http-transport-recommended)\n\nFor production deployments, we recommend using the HTTP transport. You can configure it directly on the client:\n\n```typescript\n1import { createMCPClient } from '@ai-sdk/mcp';\n\n2\n\n3const mcpClient = await createMCPClient({\n\n4  transport: {\n\n5    type: 'http',\n\n6    url: 'https://your-server.com/mcp',\n\n7\n\n8    // optional: configure HTTP headers\n\n9    headers: { Authorization: 'Bearer my-api-key' },\n\n10\n\n11    // optional: provide an OAuth client provider for automatic authorization\n\n12    authProvider: myOAuthClientProvider,\n\n13  },\n\n14});\n```\n\nAlternatively, you can use `StreamableHTTPClientTransport` from MCP's official TypeScript SDK:\n\n```typescript\n1import { createMCPClient } from '@ai-sdk/mcp';\n\n2import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';\n\n3\n\n4const url = new URL('https://your-server.com/mcp');\n\n5const mcpClient = await createMCPClient({\n\n6  transport: new StreamableHTTPClientTransport(url, {\n\n7    sessionId: 'session_123',\n\n8  }),\n\n9});\n```\n\n### [SSE Transport](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#sse-transport)\n\nSSE provides an alternative HTTP-based transport option. Configure it with a `type` and `url` property. You can also provide an `authProvider` for OAuth:\n\n```typescript\n1import { createMCPClient } from '@ai-sdk/mcp';\n\n2\n\n3const mcpClient = await createMCPClient({\n\n4  transport: {\n\n5    type: 'sse',\n\n6    url: 'https://my-server.com/sse',\n\n7\n\n8    // optional: configure HTTP headers\n\n9    headers: { Authorization: 'Bearer my-api-key' },\n\n10\n\n11    // optional: provide an OAuth client provider for automatic authorization\n\n12    authProvider: myOAuthClientProvider,\n\n13  },\n\n14});\n```\n\n### [Stdio Transport (Local Servers)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#stdio-transport-local-servers)\n\nThe stdio transport should only be used for local servers.\n\nThe Stdio transport can be imported from either the MCP SDK or the AI SDK:\n\n```typescript\n1import { createMCPClient } from '@ai-sdk/mcp';\n\n2import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';\n\n3// Or use the AI SDK's stdio transport:\n\n4// import { Experimental_StdioMCPTransport as StdioClientTransport } from '@ai-sdk/mcp/mcp-stdio';\n\n5\n\n6const mcpClient = await createMCPClient({\n\n7  transport: new StdioClientTransport({\n\n8    command: 'node',\n\n9    args: ['src/stdio/dist/server.js'],\n\n10  }),\n\n11});\n```\n\n### [Custom Transport](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#custom-transport)\n\nYou can also bring your own transport by implementing the `MCPTransport` interface for specific requirements not covered by the standard transports.\n\nThe client returned by the `createMCPClient` function is a\nlightweight client intended for use in tool conversion. It currently does not\nsupport all features of the full MCP client, such as: session\nmanagement, resumable streams, and receiving notifications.\n\nAuthorization via OAuth is supported when using the AI SDK MCP HTTP or SSE\ntransports by providing an `authProvider`.\n\n### [Closing the MCP Client](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#closing-the-mcp-client)\n\nAfter initialization, you should close the MCP client based on your usage pattern:\n\n- For short-lived usage (e.g., single requests), close the client when the response is finished\n- For long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates\n\nWhen streaming responses, you can close the client when the LLM response has finished. For example, when using `streamText`, you should use the `onFinish` callback:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```typescript\n1const mcpClient = await createMCPClient({\n\n2  // ...\n\n3});\n\n4\n\n5const tools = await mcpClient.tools();\n\n6\n\n7const result = await streamText({\n\n8  model: \"anthropic/claude-sonnet-4.5\",\n\n9  tools,\n\n10  prompt: 'What is the weather in Brooklyn, New York?',\n\n11  onFinish: async () => {\n\n12    await mcpClient.close();\n\n13  },\n\n14});\n```\n\nWhen generating responses without streaming, you can use try/finally or cleanup functions in your framework:\n\n```typescript\n1let mcpClient: MCPClient | undefined;\n\n2\n\n3try {\n\n4  mcpClient = await createMCPClient({\n\n5    // ...\n\n6  });\n\n7} finally {\n\n8  await mcpClient?.close();\n\n9}\n```\n\n## [Using MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#using-mcp-tools)\n\nThe client's `tools` method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:\n\n### [Schema Discovery](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#schema-discovery)\n\nWith schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:\n\n```typescript\n1const tools = await mcpClient.tools();\n```\n\nThis approach is simpler to implement and automatically stays in sync with server changes. However, you won't have TypeScript type safety during development, and all tools from the server will be loaded\n\n### [Schema Definition](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#schema-definition)\n\nFor better type safety and control, you can define the tools and their input schemas explicitly in your client code:\n\n```typescript\n1import { z } from 'zod';\n\n2\n\n3const tools = await mcpClient.tools({\n\n4  schemas: {\n\n5    'get-data': {\n\n6      inputSchema: z.object({\n\n7        query: z.string().describe('The data query'),\n\n8        format: z.enum(['json', 'text']).optional(),\n\n9      }),\n\n10    },\n\n11    // For tools with zero inputs, you should use an empty object:\n\n12    'tool-with-no-args': {\n\n13      inputSchema: z.object({}),\n\n14    },\n\n15  },\n\n16});\n```\n\nThis approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define `schemas`, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs\n\n## [Using MCP Resources](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#using-mcp-resources)\n\nAccording to the [MCP specification](https://modelcontextprotocol.io/docs/learn/server-concepts#resources), resources are **application-driven** data sources that provide context to the model. Unlike tools (which are model-controlled), your application decides when to fetch and pass resources as context.\n\nThe MCP client provides three methods for working with resources:\n\n### [Listing Resources](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#listing-resources)\n\nList all available resources from the MCP server:\n\n```typescript\n1const resources = await mcpClient.listResources();\n```\n\n### [Reading Resource Contents](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#reading-resource-contents)\n\nRead the contents of a specific resource by its URI:\n\n```typescript\n1const resourceData = await mcpClient.readResource({\n\n2  uri: 'file:///example/document.txt',\n\n3});\n```\n\n### [Listing Resource Templates](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#listing-resource-templates)\n\nResource templates are dynamic URI patterns that allow flexible queries. List all available templates:\n\n```typescript\n1const templates = await mcpClient.listResourceTemplates();\n```\n\n## [Using MCP Prompts](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#using-mcp-prompts)\n\nMCP Prompts is an experimental feature and may change in the future.\n\nAccording to the MCP specification, prompts are user-controlled templates that servers expose for clients to list and retrieve with optional arguments.\n\n### [Listing Prompts](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#listing-prompts)\n\n```typescript\n1const prompts = await mcpClient.experimental_listPrompts();\n```\n\n### [Getting a Prompt](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#getting-a-prompt)\n\nRetrieve prompt messages, optionally passing arguments defined by the server:\n\n```typescript\n1const prompt = await mcpClient.experimental_getPrompt({\n\n2  name: 'code_review',\n\n3  arguments: { code: 'function add(a, b) { return a + b; }' },\n\n4});\n```\n\n## [Handling Elicitation Requests](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#handling-elicitation-requests)\n\nElicitation is a mechanism where MCP servers can request additional information from the client during tool execution. For example, a server might need user input to complete a registration form or confirmation for a sensitive operation.\n\nIt is up to the client application to handle elicitation requests properly.\nThe MCP client simply surfaces these requests from the server to your\napplication code.\n\n### [Enabling Elicitation Support](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#enabling-elicitation-support)\n\nTo enable elicitation, you need to advertise the capability when creating the MCP client:\n\n```typescript\n1const mcpClient = await createMCPClient({\n\n2  transport: {\n\n3    type: 'sse',\n\n4    url: 'https://your-server.com/sse',\n\n5  },\n\n6  capabilities: {\n\n7    elicitation: {},\n\n8  },\n\n9});\n```\n\n### [Registering an Elicitation Handler](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#registering-an-elicitation-handler)\n\nUse the `onElicitationRequest` method to register a handler that will be called when the server requests input:\n\n```typescript\n1import { ElicitationRequestSchema } from '@ai-sdk/mcp';\n\n2\n\n3mcpClient.onElicitationRequest(ElicitationRequestSchema, async request => {\n\n4  // request.params.message: A message describing what input is needed\n\n5  // request.params.requestedSchema: JSON schema defining the expected input structure\n\n6\n\n7  // Get input from the user (implement according to your application's needs)\n\n8  const userInput = await getInputFromUser(\n\n9    request.params.message,\n\n10    request.params.requestedSchema,\n\n11  );\n\n12\n\n13  // Return the result with one of three actions:\n\n14  return {\n\n15    action: 'accept', // or 'decline' or 'cancel'\n\n16    content: userInput, // only required when action is 'accept'\n\n17  };\n\n18});\n```\n\n### [Elicitation Response Actions](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#elicitation-response-actions)\n\nYour handler must return an object with an `action` field that can be one of:\n\n- `'accept'`: User provided the requested information. Must include `content` with the data.\n- `'decline'`: User chose not to provide the information.\n- `'cancel'`: User cancelled the operation entirely.\n\n## [Examples](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools\\#examples)\n\nYou can see MCP in action in the following examples:\n\n[Learn to use MCP tools in Node.js](https://ai-sdk.dev/cookbook/node/mcp-tools) [Learn to handle MCP elicitation requests in Node.js](https://ai-sdk.dev/cookbook/node/mcp-elicitation)\n\nOn this page\n\n[Model Context Protocol (MCP)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#model-context-protocol-mcp)\n\n[Initializing an MCP Client](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#initializing-an-mcp-client)\n\n[HTTP Transport (Recommended)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#http-transport-recommended)\n\n[SSE Transport](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#sse-transport)\n\n[Stdio Transport (Local Servers)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#stdio-transport-local-servers)\n\n[Custom Transport](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#custom-transport)\n\n[Closing the MCP Client](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#closing-the-mcp-client)\n\n[Using MCP Tools](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#using-mcp-tools)\n\n[Schema Discovery](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#schema-discovery)\n\n[Schema Definition](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#schema-definition)\n\n[Using MCP Resources](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#using-mcp-resources)\n\n[Listing Resources](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#listing-resources)\n\n[Reading Resource Contents](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#reading-resource-contents)\n\n[Listing Resource Templates](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#listing-resource-templates)\n\n[Using MCP Prompts](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#using-mcp-prompts)\n\n[Listing Prompts](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#listing-prompts)\n\n[Getting a Prompt](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#getting-a-prompt)\n\n[Handling Elicitation Requests](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#handling-elicitation-requests)\n\n[Enabling Elicitation Support](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#enabling-elicitation-support)\n\n[Registering an Elicitation Handler](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#registering-an-elicitation-handler)\n\n[Elicitation Response Actions](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#elicitation-response-actions)\n\n[Examples](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools#examples)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools"
      },
      {
        "title": "Prompt Engineering",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
        "content": "# [Prompt Engineering](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#prompt-engineering)\n\n## [Tips](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#tips)\n\n### [Prompts for Tools](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#prompts-for-tools)\n\nWhen you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.\n\nHere are a few tips to help you get the best results:\n\n1. Use a model that is strong at tool calling, such as `gpt-5` or `gpt-4.1`. Weaker models will often struggle to call tools effectively and flawlessly.\n2. Keep the number of tools low, e.g. to 5 or less.\n3. Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.\n4. Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.\n5. Add `.describe(\"...\")` to your Zod schema properties to give the model hints about what a particular property is for.\n6. When the output of a tool might be unclear to the model and there are dependencies between tools, use the `description` field of a tool to provide information about the output of the tool execution.\n7. You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.\n\nIn general, the goal should be to give the model all information it needs in a clear way.\n\n### [Tool & Structured Data Schemas](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#tool--structured-data-schemas)\n\nThe mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.\n\n#### [Zod Dates](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#zod-dates)\n\nZod expects JavaScript Date objects, but models return dates as strings.\nYou can specify and validate the date format using `z.string().datetime()` or `z.string().date()`,\nand then use a Zod transformer to convert the string to a Date object.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateObject({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  schema: z.object({\n\n4    events: z.array(\n\n5      z.object({\n\n6        event: z.string(),\n\n7        date: z\n\n8          .string()\n\n9          .date()\n\n10          .transform(value => new Date(value)),\n\n11      }),\n\n12    ),\n\n13  }),\n\n14  prompt: 'List 5 important events from the year 2000.',\n\n15});\n```\n\n#### [Optional Parameters](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#optional-parameters)\n\nWhen working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.\n\nThis is particularly relevant for OpenAI models with structured outputs\n(strict mode).\n\nFor maximum compatibility, optional parameters should use `.nullable()` instead of `.optional()`:\n\n```ts\n1// This may fail with strict schema validation\n\n2const failingTool = tool({\n\n3  description: 'Execute a command',\n\n4  inputSchema: z.object({\n\n5    command: z.string(),\n\n6    workdir: z.string().optional(), // This can cause errors\n\n7    timeout: z.string().optional(),\n\n8  }),\n\n9});\n\n10\n\n11// This works with strict schema validation\n\n12const workingTool = tool({\n\n13  description: 'Execute a command',\n\n14  inputSchema: z.object({\n\n15    command: z.string(),\n\n16    workdir: z.string().nullable(), // Use nullable instead\n\n17    timeout: z.string().nullable(),\n\n18  }),\n\n19});\n```\n\n#### [Temperature Settings](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#temperature-settings)\n\nFor tool calls and object generation, it's recommended to use `temperature: 0` to ensure deterministic and consistent results:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  temperature: 0, // Recommended for tool calls\n\n4  tools: {\n\n5    myTool: tool({\n\n6      description: 'Execute a command',\n\n7      inputSchema: z.object({\n\n8        command: z.string(),\n\n9      }),\n\n10    }),\n\n11  },\n\n12  prompt: 'Execute the ls command',\n\n13});\n```\n\nLower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:\n\n- Generate structured data with specific formats\n- Make precise tool calls with correct parameters\n- Follow strict schemas consistently\n\n## [Debugging](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#debugging)\n\n### [Inspecting Warnings](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#inspecting-warnings)\n\nNot all providers support all AI SDK features.\nProviders either throw exceptions or return warnings when they do not support a feature.\nTo check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Hello, world!',\n\n4});\n\n5\n\n6console.log(result.warnings);\n```\n\n### [HTTP Request Bodies](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering\\#http-request-bodies)\n\nYou can inspect the raw HTTP request bodies for models that expose them, e.g. [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai).\nThis allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.\n\nRequest bodies are available via the `request.body` property of the response:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Hello, world!',\n\n4});\n\n5\n\n6console.log(result.request.body);\n```\n\nOn this page\n\n[Prompt Engineering](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#prompt-engineering)\n\n[Tips](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#tips)\n\n[Prompts for Tools](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#prompts-for-tools)\n\n[Tool & Structured Data Schemas](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#tool--structured-data-schemas)\n\n[Zod Dates](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#zod-dates)\n\n[Optional Parameters](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#optional-parameters)\n\n[Temperature Settings](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#temperature-settings)\n\n[Debugging](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#debugging)\n\n[Inspecting Warnings](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#inspecting-warnings)\n\n[HTTP Request Bodies](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering#http-request-bodies)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering"
      },
      {
        "title": "Settings",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
        "content": "# [Settings](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#settings)\n\nLarge language models (LLMs) typically provide settings to augment their output.\n\nAll AI SDK functions support the following common settings in addition to the model, the [prompt](https://ai-sdk.dev/docs/ai-sdk-core/prompts), and additional provider-specific settings:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  maxOutputTokens: 512,\n\n4  temperature: 0.3,\n\n5  maxRetries: 5,\n\n6  prompt: 'Invent a new holiday and describe its traditions.',\n\n7});\n```\n\nSome providers do not support all common settings. If you use a setting with a\nprovider that does not support it, a warning will be generated. You can check\nthe `warnings` property in the result object to see if any warnings were\ngenerated.\n\n### [`maxOutputTokens`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#maxoutputtokens)\n\nMaximum number of tokens to generate.\n\n### [`temperature`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#temperature)\n\nTemperature setting.\n\nThe value is passed through to the provider. The range depends on the provider and model.\nFor most providers, `0` means almost deterministic results, and higher values mean more randomness.\n\nIt is recommended to set either `temperature` or `topP`, but not both.\n\nIn AI SDK 5.0, temperature is no longer set to `0` by default.\n\n### [`topP`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#topp)\n\nNucleus sampling.\n\nThe value is passed through to the provider. The range depends on the provider and model.\nFor most providers, nucleus sampling is a number between 0 and 1.\nE.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.\n\nIt is recommended to set either `temperature` or `topP`, but not both.\n\n### [`topK`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#topk)\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\n### [`presencePenalty`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#presencepenalty)\n\nThe presence penalty affects the likelihood of the model to repeat information that is already in the prompt.\n\nThe value is passed through to the provider. The range depends on the provider and model.\nFor most providers, `0` means no penalty.\n\n### [`frequencyPenalty`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#frequencypenalty)\n\nThe frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.\n\nThe value is passed through to the provider. The range depends on the provider and model.\nFor most providers, `0` means no penalty.\n\n### [`stopSequences`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#stopsequences)\n\nThe stop sequences to use for stopping the text generation.\n\nIf set, the model will stop generating text when one of the stop sequences is generated.\nProviders may have limits on the number of stop sequences.\n\n### [`seed`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#seed)\n\nIt is the seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n### [`maxRetries`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#maxretries)\n\nMaximum number of retries. Set to 0 to disable retries. Default: `2`.\n\n### [`abortSignal`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#abortsignal)\n\nAn optional abort signal that can be used to cancel the call.\n\nThe abort signal can e.g. be forwarded from a user interface to cancel the call,\nor to define a timeout.\n\n#### [Example: Timeout](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#example-timeout)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Invent a new holiday and describe its traditions.',\n\n4  abortSignal: AbortSignal.timeout(5000), // 5 seconds\n\n5});\n```\n\n### [`headers`](https://ai-sdk.dev/docs/ai-sdk-core/settings\\#headers)\n\nAdditional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\nYou can use the request headers to provide additional information to the provider,\ndepending on what the provider supports. For example, some observability providers support\nheaders such as `Prompt-Id`.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6  headers: {\n\n7    'Prompt-Id': 'my-prompt-id',\n\n8  },\n\n9});\n```\n\nThe `headers` setting is for request-specific headers. You can also set\n`headers` in the provider configuration. These headers will be sent with every\nrequest made by the provider.\n\nOn this page\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/settings#settings)\n\n[maxOutputTokens](https://ai-sdk.dev/docs/ai-sdk-core/settings#maxoutputtokens)\n\n[temperature](https://ai-sdk.dev/docs/ai-sdk-core/settings#temperature)\n\n[topP](https://ai-sdk.dev/docs/ai-sdk-core/settings#topp)\n\n[topK](https://ai-sdk.dev/docs/ai-sdk-core/settings#topk)\n\n[presencePenalty](https://ai-sdk.dev/docs/ai-sdk-core/settings#presencepenalty)\n\n[frequencyPenalty](https://ai-sdk.dev/docs/ai-sdk-core/settings#frequencypenalty)\n\n[stopSequences](https://ai-sdk.dev/docs/ai-sdk-core/settings#stopsequences)\n\n[seed](https://ai-sdk.dev/docs/ai-sdk-core/settings#seed)\n\n[maxRetries](https://ai-sdk.dev/docs/ai-sdk-core/settings#maxretries)\n\n[abortSignal](https://ai-sdk.dev/docs/ai-sdk-core/settings#abortsignal)\n\n[Example: Timeout](https://ai-sdk.dev/docs/ai-sdk-core/settings#example-timeout)\n\n[headers](https://ai-sdk.dev/docs/ai-sdk-core/settings#headers)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/settings"
      },
      {
        "title": "Embeddings",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
        "content": "# [Embeddings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embeddings)\n\nEmbeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space.\nIn this space, similar words are close to each other, and the distance between words can be used to measure their similarity.\n\n## [Embedding a Single Value](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embedding-a-single-value)\n\nThe AI SDK provides the [`embed`](https://ai-sdk.dev/docs/reference/ai-sdk-core/embed) function to embed single values, which is useful for tasks such as finding similar words\nor phrases or clustering text.\nYou can use it with embeddings models, e.g. `openai.embeddingModel('text-embedding-3-large')` or `mistral.embeddingModel('mistral-embed')`.\n\n```tsx\n1import { embed } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4// 'embedding' is a single embedding object (number[])\n\n5const { embedding } = await embed({\n\n6  model: 'openai/text-embedding-3-small',\n\n7  value: 'sunny day at the beach',\n\n8});\n```\n\n## [Embedding Many Values](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embedding-many-values)\n\nWhen loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),\nit is often useful to embed many values at once (batch embedding).\n\nThe AI SDK provides the [`embedMany`](https://ai-sdk.dev/docs/reference/ai-sdk-core/embed-many) function for this purpose.\nSimilar to `embed`, you can use it with embeddings models,\ne.g. `openai.embeddingModel('text-embedding-3-large')` or `mistral.embeddingModel('mistral-embed')`.\n\n```tsx\n1import { openai } from '@ai-sdk/openai';\n\n2import { embedMany } from 'ai';\n\n3\n\n4// 'embeddings' is an array of embedding objects (number[][]).\n\n5// It is sorted in the same order as the input values.\n\n6const { embeddings } = await embedMany({\n\n7  model: 'openai/text-embedding-3-small',\n\n8  values: [\\\n\\\n9    'sunny day at the beach',\\\n\\\n10    'rainy afternoon in the city',\\\n\\\n11    'snowy night in the mountains',\\\n\\\n12  ],\n\n13});\n```\n\n## [Embedding Similarity](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embedding-similarity)\n\nAfter embedding values, you can calculate the similarity between them using the [`cosineSimilarity`](https://ai-sdk.dev/docs/reference/ai-sdk-core/cosine-similarity) function.\nThis is useful to e.g. find similar words or phrases in a dataset.\nYou can also rank and filter related items based on their similarity.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { cosineSimilarity, embedMany } from 'ai';\n\n3\n\n4const { embeddings } = await embedMany({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  values: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7});\n\n8\n\n9console.log(\n\n10  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\n\n11);\n```\n\n## [Token Usage](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#token-usage)\n\nMany providers charge based on the number of tokens used to generate embeddings.\nBoth `embed` and `embedMany` provide token usage information in the `usage` property of the result object:\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding, usage } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7});\n\n8\n\n9console.log(usage); // { tokens: 10 }\n```\n\n## [Settings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#settings)\n\n### [Provider Options](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#provider-options)\n\nEmbedding model settings can be configured using `providerOptions` for provider-specific parameters:\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7  providerOptions: {\n\n8    openai: {\n\n9      dimensions: 512, // Reduce embedding dimensions\n\n10    },\n\n11  },\n\n12});\n```\n\n### [Parallel Requests](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#parallel-requests)\n\nThe `embedMany` function now supports parallel processing with configurable `maxParallelCalls` to optimize performance:\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embedMany } from 'ai';\n\n3\n\n4const { embeddings, usage } = await embedMany({\n\n5  maxParallelCalls: 2, // Limit parallel requests\n\n6  model: 'openai/text-embedding-3-small',\n\n7  values: [\\\n\\\n8    'sunny day at the beach',\\\n\\\n9    'rainy afternoon in the city',\\\n\\\n10    'snowy night in the mountains',\\\n\\\n11  ],\n\n12});\n```\n\n### [Retries](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#retries)\n\nBoth `embed` and `embedMany` accept an optional `maxRetries` parameter of type `number`\nthat you can use to set the maximum number of retries for the embedding process.\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7  maxRetries: 0, // Disable retries\n\n8});\n```\n\n### [Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#abort-signals-and-timeouts)\n\nBoth `embed` and `embedMany` accept an optional `abortSignal` parameter of\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\nthat you can use to abort the embedding process or set a timeout.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n\n8});\n```\n\n### [Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#custom-headers)\n\nBoth `embed` and `embedMany` accept an optional `headers` parameter of type `Record<string, string>`\nthat you can use to add custom headers to the embedding request.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7  headers: { 'X-Custom-Header': 'custom-value' },\n\n8});\n```\n\n## [Response Information](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#response-information)\n\nBoth `embed` and `embedMany` return response information that includes the raw provider response:\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { embed } from 'ai';\n\n3\n\n4const { embedding, response } = await embed({\n\n5  model: 'openai/text-embedding-3-small',\n\n6  value: 'sunny day at the beach',\n\n7});\n\n8\n\n9console.log(response); // Raw provider response\n```\n\n## [Embedding Middleware](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embedding-middleware)\n\nYou can enhance embedding models, e.g. to set default values, using\n`wrapEmbeddingModel` and `EmbeddingModelV3Middleware`.\n\nHere is an example that uses the built-in `defaultEmbeddingSettingsMiddleware`:\n\n```ts\n1import {\n\n2  customProvider,\n\n3  defaultEmbeddingSettingsMiddleware,\n\n4  embed,\n\n5  wrapEmbeddingModel,\n\n6  gateway,\n\n7} from 'ai';\n\n8\n\n9const embeddingModelWithDefaults = wrapEmbeddingModel({\n\n10  model: gateway.embeddingModel('google/gemini-embedding-001'),\n\n11  middleware: defaultEmbeddingSettingsMiddleware({\n\n12    settings: {\n\n13      providerOptions: {\n\n14        google: {\n\n15          outputDimensionality: 256,\n\n16          taskType: 'CLASSIFICATION',\n\n17        },\n\n18      },\n\n19    },\n\n20  }),\n\n21});\n```\n\n## [Embedding Providers & Models](https://ai-sdk.dev/docs/ai-sdk-core/embeddings\\#embedding-providers--models)\n\nSeveral providers offer embedding models:\n\n| Provider | Model | Embedding Dimensions |\n| --- | --- | --- |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-3-large` | 3072 |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-3-small` | 1536 |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#embedding-models) | `text-embedding-ada-002` | 1536 |\n| [Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `gemini-embedding-001` | 3072 |\n| [Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `text-embedding-004` | 768 |\n| [Mistral](https://ai-sdk.dev/providers/ai-sdk-providers/mistral#embedding-models) | `mistral-embed` | 1024 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-v3.0` | 1024 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-v3.0` | 1024 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-light-v3.0` | 384 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-light-v3.0` | 384 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-v2.0` | 4096 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-english-light-v2.0` | 1024 |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#embedding-models) | `embed-multilingual-v2.0` | 768 |\n| [Amazon Bedrock](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock#embedding-models) | `amazon.titan-embed-text-v1` | 1536 |\n| [Amazon Bedrock](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock#embedding-models) | `amazon.titan-embed-text-v2:0` | 1024 |\n\nOn this page\n\n[Embeddings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embeddings)\n\n[Embedding a Single Value](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embedding-a-single-value)\n\n[Embedding Many Values](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embedding-many-values)\n\n[Embedding Similarity](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embedding-similarity)\n\n[Token Usage](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#token-usage)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#settings)\n\n[Provider Options](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#provider-options)\n\n[Parallel Requests](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#parallel-requests)\n\n[Retries](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#retries)\n\n[Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#abort-signals-and-timeouts)\n\n[Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#custom-headers)\n\n[Response Information](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#response-information)\n\n[Embedding Middleware](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embedding-middleware)\n\n[Embedding Providers & Models](https://ai-sdk.dev/docs/ai-sdk-core/embeddings#embedding-providers--models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings"
      },
      {
        "title": "Reranking",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/reranking",
        "content": "# [Reranking](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#reranking)\n\nReranking is a technique used to improve search relevance by reordering a set of documents based on their relevance to a query.\nUnlike embedding-based similarity search, reranking models are specifically trained to understand the relationship between queries and documents,\noften producing more accurate relevance scores.\n\n## [Reranking Documents](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#reranking-documents)\n\nThe AI SDK provides the [`rerank`](https://ai-sdk.dev/docs/reference/ai-sdk-core/rerank) function to rerank documents based on their relevance to a query.\nYou can use it with reranking models, e.g. `cohere.reranking('rerank-v3.5')` or `bedrock.reranking('cohere.rerank-v3-5:0')`.\n\n```tsx\n1import { rerank } from 'ai';\n\n2import { cohere } from '@ai-sdk/cohere';\n\n3\n\n4const documents = [\\\n\\\n5  'sunny day at the beach',\\\n\\\n6  'rainy afternoon in the city',\\\n\\\n7  'snowy night in the mountains',\\\n\\\n8];\n\n9\n\n10const { ranking } = await rerank({\n\n11  model: cohere.reranking('rerank-v3.5'),\n\n12  documents,\n\n13  query: 'talk about rain',\n\n14  topN: 2, // Return top 2 most relevant documents\n\n15});\n\n16\n\n17console.log(ranking);\n\n18// [\\\n\\\n19//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },\\\n\\\n20//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }\\\n\\\n21// ]\n```\n\n## [Working with Object Documents](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#working-with-object-documents)\n\nReranking also supports structured documents (JSON objects), making it ideal for searching through databases, emails, or other structured content:\n\n```tsx\n1import { rerank } from 'ai';\n\n2import { cohere } from '@ai-sdk/cohere';\n\n3\n\n4const documents = [\\\n\\\n5  {\\\n\\\n6    from: 'Paul Doe',\\\n\\\n7    subject: 'Follow-up',\\\n\\\n8    text: 'We are happy to give you a discount of 20% on your next order.',\\\n\\\n9  },\\\n\\\n10  {\\\n\\\n11    from: 'John McGill',\\\n\\\n12    subject: 'Missing Info',\\\n\\\n13    text: 'Sorry, but here is the pricing information from Oracle: $5000/month',\\\n\\\n14  },\\\n\\\n15];\n\n16\n\n17const { ranking, rerankedDocuments } = await rerank({\n\n18  model: cohere.reranking('rerank-v3.5'),\n\n19  documents,\n\n20  query: 'Which pricing did we get from Oracle?',\n\n21  topN: 1,\n\n22});\n\n23\n\n24console.log(rerankedDocuments[0]);\n\n25// { from: 'John McGill', subject: 'Missing Info', text: '...' }\n```\n\n## [Understanding the Results](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#understanding-the-results)\n\nThe `rerank` function returns a comprehensive result object:\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking, rerankedDocuments, originalDocuments } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8});\n\n9\n\n10// ranking: sorted array of { originalIndex, score, document }\n\n11// rerankedDocuments: documents sorted by relevance (convenience property)\n\n12// originalDocuments: original documents array\n```\n\nEach item in the `ranking` array contains:\n\n- `originalIndex`: Position in the original documents array\n- `score`: Relevance score (typically 0-1, where higher is more relevant)\n- `document`: The original document\n\n## [Settings](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#settings)\n\n### [Top-N Results](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#top-n-results)\n\nUse `topN` to limit the number of results returned. This is useful for retrieving only the most relevant documents:\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['doc1', 'doc2', 'doc3', 'doc4', 'doc5'],\n\n7  query: 'relevant information',\n\n8  topN: 3, // Return only top 3 most relevant documents\n\n9});\n```\n\n### [Provider Options](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#provider-options)\n\nReranking model settings can be configured using `providerOptions` for provider-specific parameters:\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8  providerOptions: {\n\n9    cohere: {\n\n10      maxTokensPerDoc: 1000, // Limit tokens per document\n\n11    },\n\n12  },\n\n13});\n```\n\n### [Retries](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#retries)\n\nThe `rerank` function accepts an optional `maxRetries` parameter of type `number`\nthat you can use to set the maximum number of retries for the reranking process.\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8  maxRetries: 0, // Disable retries\n\n9});\n```\n\n### [Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#abort-signals-and-timeouts)\n\nThe `rerank` function accepts an optional `abortSignal` parameter of\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\nthat you can use to abort the reranking process or set a timeout.\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8  abortSignal: AbortSignal.timeout(5000), // Abort after 5 seconds\n\n9});\n```\n\n### [Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#custom-headers)\n\nThe `rerank` function accepts an optional `headers` parameter of type `Record<string, string>`\nthat you can use to add custom headers to the reranking request.\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8  headers: { 'X-Custom-Header': 'custom-value' },\n\n9});\n```\n\n## [Response Information](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#response-information)\n\nThe `rerank` function returns response information that includes the raw provider response:\n\n```ts\n1import { cohere } from '@ai-sdk/cohere';\n\n2import { rerank } from 'ai';\n\n3\n\n4const { ranking, response } = await rerank({\n\n5  model: cohere.reranking('rerank-v3.5'),\n\n6  documents: ['sunny day at the beach', 'rainy afternoon in the city'],\n\n7  query: 'talk about rain',\n\n8});\n\n9\n\n10console.log(response); // { id, timestamp, modelId, headers, body }\n```\n\n## [Reranking Providers & Models](https://ai-sdk.dev/docs/ai-sdk-core/reranking\\#reranking-providers--models)\n\nSeveral providers offer reranking models:\n\n| Provider | Model |\n| --- | --- |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-v3.5` |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-english-v3.0` |\n| [Cohere](https://ai-sdk.dev/providers/ai-sdk-providers/cohere#reranking-models) | `rerank-multilingual-v3.0` |\n| [Amazon Bedrock](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `amazon.rerank-v1:0` |\n| [Amazon Bedrock](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `cohere.rerank-v3-5:0` |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#reranking-models) | `Salesforce/Llama-Rank-v1` |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#reranking-models) | `mixedbread-ai/Mxbai-Rerank-Large-V2` |\n\nOn this page\n\n[Reranking](https://ai-sdk.dev/docs/ai-sdk-core/reranking#reranking)\n\n[Reranking Documents](https://ai-sdk.dev/docs/ai-sdk-core/reranking#reranking-documents)\n\n[Working with Object Documents](https://ai-sdk.dev/docs/ai-sdk-core/reranking#working-with-object-documents)\n\n[Understanding the Results](https://ai-sdk.dev/docs/ai-sdk-core/reranking#understanding-the-results)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/reranking#settings)\n\n[Top-N Results](https://ai-sdk.dev/docs/ai-sdk-core/reranking#top-n-results)\n\n[Provider Options](https://ai-sdk.dev/docs/ai-sdk-core/reranking#provider-options)\n\n[Retries](https://ai-sdk.dev/docs/ai-sdk-core/reranking#retries)\n\n[Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/reranking#abort-signals-and-timeouts)\n\n[Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/reranking#custom-headers)\n\n[Response Information](https://ai-sdk.dev/docs/ai-sdk-core/reranking#response-information)\n\n[Reranking Providers & Models](https://ai-sdk.dev/docs/ai-sdk-core/reranking#reranking-providers--models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/reranking",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/reranking",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/reranking"
      },
      {
        "title": "Image Generation",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
        "content": "# [Image Generation](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#image-generation)\n\nThe AI SDK provides the [`generateImage`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image)\nfunction to generate images based on a given prompt using an image model.\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7});\n```\n\nYou can access the image data using the `base64` or `uint8Array` properties:\n\n```tsx\n1const base64 = image.base64; // base64 image data\n\n2const uint8Array = image.uint8Array; // Uint8Array image data\n```\n\n## [Settings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#settings)\n\n### [Size and Aspect Ratio](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#size-and-aspect-ratio)\n\nDepending on the model, you can either specify the size or the aspect ratio.\n\n##### [Size](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#size)\n\nThe size is specified as a string in the format `{width}x{height}`.\nModels only support a few sizes, and the supported sizes are different for each model and provider.\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  size: '1024x1024',\n\n8});\n```\n\n##### [Aspect Ratio](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#aspect-ratio)\n\nThe aspect ratio is specified as a string in the format `{width}:{height}`.\nModels only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { vertex } from '@ai-sdk/google-vertex';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: vertex.image('imagen-4.0-generate-001'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  aspectRatio: '16:9',\n\n8});\n```\n\n### [Generating Multiple Images](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#generating-multiple-images)\n\n`generateImage` also supports generating multiple images at once:\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const { images } = await generateImage({\n\n5  model: openai.image('dall-e-2'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  n: 4, // number of images to generate\n\n8});\n```\n\n`generateImage` will automatically call the model as often as needed (in\nparallel) to generate the requested number of images.\n\nEach image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the `n` parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).\n\nIf needed, you can override this behavior using the `maxImagesPerCall` setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:\n\n```tsx\n1const { images } = await generateImage({\n\n2  model: openai.image('dall-e-2'),\n\n3  prompt: 'Santa Claus driving a Cadillac',\n\n4  maxImagesPerCall: 5, // Override the default batch size\n\n5  n: 10, // Will make 2 calls of 5 images each\n\n6});\n```\n\n### [Providing a Seed](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#providing-a-seed)\n\nYou can provide a seed to the `generateImage` function to control the output of the image generation process.\nIf supported by the model, the same seed will always produce the same image.\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  seed: 1234567890,\n\n8});\n```\n\n### [Provider-specific Settings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#provider-specific-settings)\n\nImage models often have provider- or even model-specific settings.\nYou can pass such settings to the `generateImage` function\nusing the `providerOptions` parameter. The options for the provider\n(`openai` in the example below) become request body properties.\n\n```tsx\n1import { generateImage } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  size: '1024x1024',\n\n8  providerOptions: {\n\n9    openai: { style: 'vivid', quality: 'hd' },\n\n10  },\n\n11});\n```\n\n### [Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#abort-signals-and-timeouts)\n\n`generateImage` accepts an optional `abortSignal` parameter of\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\nthat you can use to abort the image generation process or set a timeout.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateImage } from 'ai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n\n8});\n```\n\n### [Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#custom-headers)\n\n`generateImage` accepts an optional `headers` parameter of type `Record<string, string>`\nthat you can use to add custom headers to the image generation request.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { generateImage } from 'ai';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: openai.image('dall-e-3'),\n\n6  prompt: 'Santa Claus driving a Cadillac',\n\n7  headers: { 'X-Custom-Header': 'custom-value' },\n\n8});\n```\n\n### [Warnings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#warnings)\n\nIf the model returns warnings, e.g. for unsupported parameters, they will be available in the `warnings` property of the response.\n\n```tsx\n1const { image, warnings } = await generateImage({\n\n2  model: openai.image('dall-e-3'),\n\n3  prompt: 'Santa Claus driving a Cadillac',\n\n4});\n```\n\n### [Additional provider-specific meta data](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#additional-provider-specific-meta-data)\n\nSome providers expose additional meta data for the result overall or per image.\n\n```tsx\n1const prompt = 'Santa Claus driving a Cadillac';\n\n2\n\n3const { image, providerMetadata } = await generateImage({\n\n4  model: openai.image('dall-e-3'),\n\n5  prompt,\n\n6});\n\n7\n\n8const revisedPrompt = providerMetadata.openai.images[0]?.revisedPrompt;\n\n9\n\n10console.log({\n\n11  prompt,\n\n12  revisedPrompt,\n\n13});\n```\n\nThe outer key of the returned `providerMetadata` is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key.\n\n### [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#error-handling)\n\nWhen `generateImage` cannot generate a valid image, it throws a [`AI_NoImageGeneratedError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-image-generated-error).\n\nThis error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:\n\n- The model failed to generate a response\n- The model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\n- `responses`: Metadata about the image model responses, including timestamp, model, and headers.\n- `cause`: The cause of the error. You can use this for more detailed error handling\n\n```ts\n1import { generateImage, NoImageGeneratedError } from 'ai';\n\n2\n\n3try {\n\n4  await generateImage({ model, prompt });\n\n5} catch (error) {\n\n6  if (NoImageGeneratedError.isInstance(error)) {\n\n7    console.log('NoImageGeneratedError');\n\n8    console.log('Cause:', error.cause);\n\n9    console.log('Responses:', error.responses);\n\n10  }\n\n11}\n```\n\n## [Generating Images with Language Models](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#generating-images-with-language-models)\n\nSome language models such as Google `gemini-2.5-flash-image-preview` support multi-modal outputs including images.\nWith such models, you can access the generated images using the `files` property of the response.\n\n```ts\n1import { google } from '@ai-sdk/google';\n\n2import { generateText } from 'ai';\n\n3\n\n4const result = await generateText({\n\n5  model: google('gemini-2.5-flash-image-preview'),\n\n6  prompt: 'Generate an image of a comic cat',\n\n7});\n\n8\n\n9for (const file of result.files) {\n\n10  if (file.mediaType.startsWith('image/')) {\n\n11    // The file object provides multiple data formats:\n\n12    // Access images as base64 string, Uint8Array binary data, or check type\n\n13    // - file.base64: string (data URL format)\n\n14    // - file.uint8Array: Uint8Array (binary data)\n\n15    // - file.mediaType: string (e.g. \"image/png\")\n\n16  }\n\n17}\n```\n\n## [Image Models](https://ai-sdk.dev/docs/ai-sdk-core/image-generation\\#image-models)\n\n| Provider | Model | Support sizes (`width x height`) or aspect ratios (`width : height`) |\n| --- | --- | --- |\n| [xAI Grok](https://ai-sdk.dev/providers/ai-sdk-providers/xai#image-models) | `grok-2-image` | 1024x768 (default) |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#image-models) | `gpt-image-1` | 1024x1024, 1536x1024, 1024x1536 |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#image-models) | `dall-e-3` | 1024x1024, 1792x1024, 1024x1792 |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#image-models) | `dall-e-2` | 256x256, 512x512, 1024x1024 |\n| [Amazon Bedrock](https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock#image-models) | `amazon.nova-canvas-v1:0` | 320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux/dev` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux-lora` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/fast-sdxl` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/flux-pro/v1.1-ultra` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/ideogram/v2` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/recraft-v3` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/stable-diffusion-3.5-large` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#image-models) | `fal-ai/hyper-sdxl` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sd3.5` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1.1-pro` | 256-1440 (multiples of 32) |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1-schnell` | 256-1440 (multiples of 32) |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-1-dev` | 256-1440 (multiples of 32) |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `black-forest-labs/FLUX-pro` | 256-1440 (multiples of 32) |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sd3.5-medium` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |\n| [DeepInfra](https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra#image-models) | `stabilityai/sdxl-turbo` | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21 |\n| [Replicate](https://ai-sdk.dev/providers/ai-sdk-providers/replicate) | `black-forest-labs/flux-schnell` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |\n| [Replicate](https://ai-sdk.dev/providers/ai-sdk-providers/replicate) | `recraft-ai/recraft-v3` | 1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024 |\n| [Google](https://ai-sdk.dev/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google](https://ai-sdk.dev/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google](https://ai-sdk.dev/providers/ai-sdk-providers/google#image-models) | `imagen-4.0-ultra-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-4.0-ultra-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Google Vertex](https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex#image-models) | `imagen-3.0-fast-generate-001` | 1:1, 3:4, 4:3, 9:16, 16:9 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/flux-1-dev-fp8` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/flux-1-schnell-fp8` | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/playground-v2-5-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/japanese-stable-diffusion-xl` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/playground-v2-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/SSD-1B` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |\n| [Fireworks](https://ai-sdk.dev/providers/ai-sdk-providers/fireworks#image-models) | `accounts/fireworks/models/stable-diffusion-xl-1024-v1-0` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640 |\n| [Luma](https://ai-sdk.dev/providers/ai-sdk-providers/luma#image-models) | `photon-1` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Luma](https://ai-sdk.dev/providers/ai-sdk-providers/luma#image-models) | `photon-flash-1` | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `stabilityai/stable-diffusion-xl-base-1.0` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-dev` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-dev-lora` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-schnell` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-canny` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-depth` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-redux` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1.1-pro` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-pro` | 512x512, 768x768, 1024x1024 |\n| [Together.ai](https://ai-sdk.dev/providers/ai-sdk-providers/togetherai#image-models) | `black-forest-labs/FLUX.1-schnell-Free` | 512x512, 768x768, 1024x1024 |\n| [Black Forest Labs](https://ai-sdk.dev/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-pro` | From 3:7 (portrait) to 7:3 (landscape) |\n| [Black Forest Labs](https://ai-sdk.dev/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-max` | From 3:7 (portrait) to 7:3 (landscape) |\n| [Black Forest Labs](https://ai-sdk.dev/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1-ultra` | From 3:7 (portrait) to 7:3 (landscape) |\n| [Black Forest Labs](https://ai-sdk.dev/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1` | From 3:7 (portrait) to 7:3 (landscape) |\n| [Black Forest Labs](https://ai-sdk.dev/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.0-fill` | From 3:7 (portrait) to 7:3 (landscape) |\n\nAbove are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nOn this page\n\n[Image Generation](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#image-generation)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#settings)\n\n[Size and Aspect Ratio](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#size-and-aspect-ratio)\n\n[Size](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#size)\n\n[Aspect Ratio](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#aspect-ratio)\n\n[Generating Multiple Images](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#generating-multiple-images)\n\n[Providing a Seed](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#providing-a-seed)\n\n[Provider-specific Settings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#provider-specific-settings)\n\n[Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#abort-signals-and-timeouts)\n\n[Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#custom-headers)\n\n[Warnings](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#warnings)\n\n[Additional provider-specific meta data](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#additional-provider-specific-meta-data)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#error-handling)\n\n[Generating Images with Language Models](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#generating-images-with-language-models)\n\n[Image Models](https://ai-sdk.dev/docs/ai-sdk-core/image-generation#image-models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation"
      },
      {
        "title": "Transcription",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
        "content": "# [Transcription](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#transcription)\n\nTranscription is an experimental feature.\n\nThe AI SDK provides the [`transcribe`](https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe)\nfunction to transcribe audio using a transcription model.\n\n```ts\n1import { experimental_transcribe as transcribe } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3import { readFile } from 'fs/promises';\n\n4\n\n5const transcript = await transcribe({\n\n6  model: openai.transcription('whisper-1'),\n\n7  audio: await readFile('audio.mp3'),\n\n8});\n```\n\nThe `audio` property can be a `Uint8Array`, `ArrayBuffer`, `Buffer`, `string` (base64 encoded audio data), or a `URL`.\n\nTo access the generated transcript:\n\n```ts\n1const text = transcript.text; // transcript text e.g. \"Hello, world!\"\n\n2const segments = transcript.segments; // array of segments with start and end times, if available\n\n3const language = transcript.language; // language of the transcript e.g. \"en\", if available\n\n4const durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available\n```\n\n## [Settings](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#settings)\n\n### [Provider-Specific settings](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#provider-specific-settings)\n\nTranscription models often have provider or model-specific settings which you can set using the `providerOptions` parameter.\n\n```ts\n1import { experimental_transcribe as transcribe } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3import { readFile } from 'fs/promises';\n\n4\n\n5const transcript = await transcribe({\n\n6  model: openai.transcription('whisper-1'),\n\n7  audio: await readFile('audio.mp3'),\n\n8  providerOptions: {\n\n9    openai: {\n\n10      timestampGranularities: ['word'],\n\n11    },\n\n12  },\n\n13});\n```\n\n### [Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#abort-signals-and-timeouts)\n\n`transcribe` accepts an optional `abortSignal` parameter of\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\nthat you can use to abort the transcription process or set a timeout.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_transcribe as transcribe } from 'ai';\n\n3import { readFile } from 'fs/promises';\n\n4\n\n5const transcript = await transcribe({\n\n6  model: openai.transcription('whisper-1'),\n\n7  audio: await readFile('audio.mp3'),\n\n8  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n\n9});\n```\n\n### [Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#custom-headers)\n\n`transcribe` accepts an optional `headers` parameter of type `Record<string, string>`\nthat you can use to add custom headers to the transcription request.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_transcribe as transcribe } from 'ai';\n\n3import { readFile } from 'fs/promises';\n\n4\n\n5const transcript = await transcribe({\n\n6  model: openai.transcription('whisper-1'),\n\n7  audio: await readFile('audio.mp3'),\n\n8  headers: { 'X-Custom-Header': 'custom-value' },\n\n9});\n```\n\n### [Warnings](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#warnings)\n\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_transcribe as transcribe } from 'ai';\n\n3import { readFile } from 'fs/promises';\n\n4\n\n5const transcript = await transcribe({\n\n6  model: openai.transcription('whisper-1'),\n\n7  audio: await readFile('audio.mp3'),\n\n8});\n\n9\n\n10const warnings = transcript.warnings;\n```\n\n### [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#error-handling)\n\nWhen `transcribe` cannot generate a valid transcript, it throws a [`AI_NoTranscriptGeneratedError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error).\n\nThis error can arise for any the following reasons:\n\n- The model failed to generate a response\n- The model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\n- `responses`: Metadata about the transcription model responses, including timestamp, model, and headers.\n- `cause`: The cause of the error. You can use this for more detailed error handling.\n\n```ts\n1import {\n\n2  experimental_transcribe as transcribe,\n\n3  NoTranscriptGeneratedError,\n\n4} from 'ai';\n\n5import { openai } from '@ai-sdk/openai';\n\n6import { readFile } from 'fs/promises';\n\n7\n\n8try {\n\n9  await transcribe({\n\n10    model: openai.transcription('whisper-1'),\n\n11    audio: await readFile('audio.mp3'),\n\n12  });\n\n13} catch (error) {\n\n14  if (NoTranscriptGeneratedError.isInstance(error)) {\n\n15    console.log('NoTranscriptGeneratedError');\n\n16    console.log('Cause:', error.cause);\n\n17    console.log('Responses:', error.responses);\n\n18  }\n\n19}\n```\n\n## [Transcription Models](https://ai-sdk.dev/docs/ai-sdk-core/transcription\\#transcription-models)\n\n| Provider | Model |\n| --- | --- |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#transcription-models) | `whisper-1` |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#transcription-models) | `gpt-4o-transcribe` |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#transcription-models) | `gpt-4o-mini-transcribe` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1_experimental` |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq#transcription-models) | `whisper-large-v3-turbo` |\n| [Groq](https://ai-sdk.dev/providers/ai-sdk-providers/groq#transcription-models) | `whisper-large-v3` |\n| [Azure OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/azure#transcription-models) | `whisper-1` |\n| [Azure OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/azure#transcription-models) | `gpt-4o-transcribe` |\n| [Azure OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/azure#transcription-models) | `gpt-4o-mini-transcribe` |\n| [Rev.ai](https://ai-sdk.dev/providers/ai-sdk-providers/revai#transcription-models) | `machine` |\n| [Rev.ai](https://ai-sdk.dev/providers/ai-sdk-providers/revai#transcription-models) | `low_cost` |\n| [Rev.ai](https://ai-sdk.dev/providers/ai-sdk-providers/revai#transcription-models) | `fusion` |\n| [Deepgram](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram#transcription-models) | `base` (\\+ variants) |\n| [Deepgram](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram#transcription-models) | `enhanced` (\\+ variants) |\n| [Deepgram](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram#transcription-models) | `nova` (\\+ variants) |\n| [Deepgram](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram#transcription-models) | `nova-2` (\\+ variants) |\n| [Deepgram](https://ai-sdk.dev/providers/ai-sdk-providers/deepgram#transcription-models) | `nova-3` (\\+ variants) |\n| [Gladia](https://ai-sdk.dev/providers/ai-sdk-providers/gladia#transcription-models) | `default` |\n| [AssemblyAI](https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai#transcription-models) | `best` |\n| [AssemblyAI](https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai#transcription-models) | `nano` |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#transcription-models) | `whisper` |\n| [Fal](https://ai-sdk.dev/providers/ai-sdk-providers/fal#transcription-models) | `wizper` |\n\nAbove are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nOn this page\n\n[Transcription](https://ai-sdk.dev/docs/ai-sdk-core/transcription#transcription)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/transcription#settings)\n\n[Provider-Specific settings](https://ai-sdk.dev/docs/ai-sdk-core/transcription#provider-specific-settings)\n\n[Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/transcription#abort-signals-and-timeouts)\n\n[Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/transcription#custom-headers)\n\n[Warnings](https://ai-sdk.dev/docs/ai-sdk-core/transcription#warnings)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/transcription#error-handling)\n\n[Transcription Models](https://ai-sdk.dev/docs/ai-sdk-core/transcription#transcription-models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/transcription"
      },
      {
        "title": "Speech",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
        "content": "# [Speech](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#speech)\n\nSpeech is an experimental feature.\n\nThe AI SDK provides the [`generateSpeech`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech)\nfunction to generate speech from text using a speech model.\n\n```ts\n1import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: openai.speech('tts-1'),\n\n6  text: 'Hello, world!',\n\n7  voice: 'alloy',\n\n8});\n```\n\n### [Language Setting](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#language-setting)\n\nYou can specify the language for speech generation (provider support varies):\n\n```ts\n1import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n2import { lmnt } from '@ai-sdk/lmnt';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: lmnt.speech('aurora'),\n\n6  text: 'Hola, mundo!',\n\n7  language: 'es', // Spanish\n\n8});\n```\n\nTo access the generated audio:\n\n```ts\n1const audio = audio.audioData; // audio data e.g. Uint8Array\n```\n\n## [Settings](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#settings)\n\n### [Provider-Specific settings](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#provider-specific-settings)\n\nYou can set model-specific settings with the `providerOptions` parameter.\n\n```ts\n1import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n2import { openai } from '@ai-sdk/openai';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: openai.speech('tts-1'),\n\n6  text: 'Hello, world!',\n\n7  providerOptions: {\n\n8    openai: {\n\n9      // ...\n\n10    },\n\n11  },\n\n12});\n```\n\n### [Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#abort-signals-and-timeouts)\n\n`generateSpeech` accepts an optional `abortSignal` parameter of\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\nthat you can use to abort the speech generation process or set a timeout.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: openai.speech('tts-1'),\n\n6  text: 'Hello, world!',\n\n7  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\n\n8});\n```\n\n### [Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#custom-headers)\n\n`generateSpeech` accepts an optional `headers` parameter of type `Record<string, string>`\nthat you can use to add custom headers to the speech generation request.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: openai.speech('tts-1'),\n\n6  text: 'Hello, world!',\n\n7  headers: { 'X-Custom-Header': 'custom-value' },\n\n8});\n```\n\n### [Warnings](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#warnings)\n\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { experimental_generateSpeech as generateSpeech } from 'ai';\n\n3\n\n4const audio = await generateSpeech({\n\n5  model: openai.speech('tts-1'),\n\n6  text: 'Hello, world!',\n\n7});\n\n8\n\n9const warnings = audio.warnings;\n```\n\n### [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#error-handling)\n\nWhen `generateSpeech` cannot generate a valid audio, it throws a [`AI_NoSpeechGeneratedError`](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-speech-generated-error).\n\nThis error can arise for any the following reasons:\n\n- The model failed to generate a response\n- The model generated a response that could not be parsed\n\nThe error preserves the following information to help you log the issue:\n\n- `responses`: Metadata about the speech model responses, including timestamp, model, and headers.\n- `cause`: The cause of the error. You can use this for more detailed error handling.\n\n```ts\n1import {\n\n2  experimental_generateSpeech as generateSpeech,\n\n3  NoSpeechGeneratedError,\n\n4} from 'ai';\n\n5import { openai } from '@ai-sdk/openai';\n\n6\n\n7try {\n\n8  await generateSpeech({\n\n9    model: openai.speech('tts-1'),\n\n10    text: 'Hello, world!',\n\n11  });\n\n12} catch (error) {\n\n13  if (NoSpeechGeneratedError.isInstance(error)) {\n\n14    console.log('AI_NoSpeechGeneratedError');\n\n15    console.log('Cause:', error.cause);\n\n16    console.log('Responses:', error.responses);\n\n17  }\n\n18}\n```\n\n## [Speech Models](https://ai-sdk.dev/docs/ai-sdk-core/speech\\#speech-models)\n\n| Provider | Model |\n| --- | --- |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#speech-models) | `tts-1` |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#speech-models) | `tts-1-hd` |\n| [OpenAI](https://ai-sdk.dev/providers/ai-sdk-providers/openai#speech-models) | `gpt-4o-mini-tts` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_v3` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_multilingual_v2` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2_5` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2_5` |\n| [ElevenLabs](https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2` |\n| [LMNT](https://ai-sdk.dev/providers/ai-sdk-providers/lmnt#speech-models) | `aurora` |\n| [LMNT](https://ai-sdk.dev/providers/ai-sdk-providers/lmnt#speech-models) | `blizzard` |\n| [Hume](https://ai-sdk.dev/providers/ai-sdk-providers/hume#speech-models) | `default` |\n\nAbove are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.\n\nOn this page\n\n[Speech](https://ai-sdk.dev/docs/ai-sdk-core/speech#speech)\n\n[Language Setting](https://ai-sdk.dev/docs/ai-sdk-core/speech#language-setting)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/speech#settings)\n\n[Provider-Specific settings](https://ai-sdk.dev/docs/ai-sdk-core/speech#provider-specific-settings)\n\n[Abort Signals and Timeouts](https://ai-sdk.dev/docs/ai-sdk-core/speech#abort-signals-and-timeouts)\n\n[Custom Headers](https://ai-sdk.dev/docs/ai-sdk-core/speech#custom-headers)\n\n[Warnings](https://ai-sdk.dev/docs/ai-sdk-core/speech#warnings)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/speech#error-handling)\n\n[Speech Models](https://ai-sdk.dev/docs/ai-sdk-core/speech#speech-models)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/speech"
      },
      {
        "title": "Language Model Middleware",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
        "content": "# [Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#language-model-middleware)\n\nLanguage model middleware is a way to enhance the behavior of language models\nby intercepting and modifying the calls to the language model.\n\nIt can be used to add features like guardrails, RAG, caching, and logging\nin a language model agnostic way. Such middleware can be developed and\ndistributed independently from the language models that they are applied to.\n\n## [Using Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#using-language-model-middleware)\n\nYou can use language model middleware with the `wrapLanguageModel` function.\nIt takes a language model and a language model middleware and returns a new\nlanguage model that incorporates the middleware.\n\n```ts\n1import { wrapLanguageModel } from 'ai';\n\n2\n\n3const wrappedLanguageModel = wrapLanguageModel({\n\n4  model: yourModel,\n\n5  middleware: yourLanguageModelMiddleware,\n\n6});\n```\n\nThe wrapped language model can be used just like any other language model, e.g. in `streamText`:\n\n```ts\n1const result = streamText({\n\n2  model: wrappedLanguageModel,\n\n3  prompt: 'What cities are in the United States?',\n\n4});\n```\n\n## [Multiple middlewares](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#multiple-middlewares)\n\nYou can provide multiple middlewares to the `wrapLanguageModel` function.\nThe middlewares will be applied in the order they are provided.\n\n```ts\n1const wrappedLanguageModel = wrapLanguageModel({\n\n2  model: yourModel,\n\n3  middleware: [firstMiddleware, secondMiddleware],\n\n4});\n\n5\n\n6// applied as: firstMiddleware(secondMiddleware(yourModel))\n```\n\n## [Built-in Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#built-in-middleware)\n\nThe AI SDK comes with several built-in middlewares that you can use to configure language models:\n\n- `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.\n- `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.\n- `defaultSettingsMiddleware`: Applies default settings to a language model.\n- `addToolInputExamplesMiddleware`: Adds tool input examples to tool descriptions for providers that don't natively support the `inputExamples` property.\n\n### [Extract Reasoning](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#extract-reasoning)\n\nSome providers and models expose reasoning information in the generated text using special tags,\ne.g. <think> and </think>.\n\nThe `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.\n\n```ts\n1import { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\n2\n\n3const model = wrapLanguageModel({\n\n4  model: yourModel,\n\n5  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n\n6});\n```\n\nYou can then use that enhanced model in functions like `generateText` and `streamText`.\n\nThe `extractReasoningMiddleware` function also includes a `startWithReasoning` option.\nWhen set to `true`, the reasoning tag will be prepended to the generated text.\nThis is useful for models that do not include the reasoning tag at the beginning of the response.\nFor more details, see the [DeepSeek R1 guide](https://ai-sdk.dev/docs/guides/r1#deepseek-r1-middleware).\n\n### [Simulate Streaming](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#simulate-streaming)\n\nThe `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models.\nThis is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\n\n```ts\n1import { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';\n\n2\n\n3const model = wrapLanguageModel({\n\n4  model: yourModel,\n\n5  middleware: simulateStreamingMiddleware(),\n\n6});\n```\n\n### [Default Settings](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#default-settings)\n\nThe `defaultSettingsMiddleware` function can be used to apply default settings to a language model.\n\n```ts\n1import { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';\n\n2\n\n3const model = wrapLanguageModel({\n\n4  model: yourModel,\n\n5  middleware: defaultSettingsMiddleware({\n\n6    settings: {\n\n7      temperature: 0.5,\n\n8      maxOutputTokens: 800,\n\n9      providerOptions: { openai: { store: false } },\n\n10    },\n\n11  }),\n\n12});\n```\n\n### [Add Tool Input Examples](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#add-tool-input-examples)\n\nThe `addToolInputExamplesMiddleware` function adds tool input examples to tool descriptions.\nThis is useful for providers that don't natively support the `inputExamples` property on tools.\nThe middleware serializes the examples into the tool's description text so models can still benefit from seeing example inputs.\n\n```ts\n1import { wrapLanguageModel, addToolInputExamplesMiddleware } from 'ai';\n\n2\n\n3const model = wrapLanguageModel({\n\n4  model: yourModel,\n\n5  middleware: addToolInputExamplesMiddleware({\n\n6    examplesPrefix: 'Input Examples:',\n\n7  }),\n\n8});\n```\n\nWhen you define a tool with `inputExamples`, the middleware will append them to the tool's description:\n\n```ts\n1import { generateText, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const result = await generateText({\n\n5  model, // wrapped model from above\n\n6  tools: {\n\n7    weather: tool({\n\n8      description: 'Get the weather in a location',\n\n9      inputSchema: z.object({\n\n10        location: z.string(),\n\n11      }),\n\n12      inputExamples: [\\\n\\\n13        { input: { location: 'San Francisco' } },\\\n\\\n14        { input: { location: 'London' } },\\\n\\\n15      ],\n\n16    }),\n\n17  },\n\n18  prompt: 'What is the weather in Tokyo?',\n\n19});\n```\n\nThe tool description will be transformed to:\n\n```typescript\n1Get the weather in a location\n\n2\n\n3Input Examples:\n\n4{\"location\":\"San Francisco\"}\n\n5{\"location\":\"London\"}\n```\n\n#### [Options](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#options)\n\n- `examplesPrefix` (required): A prefix text to prepend before the examples.\n- `formatExample` (optional): A custom formatter function for each example. Receives the example object and its index. Default: `JSON.stringify(example.input)`.\n- `removeInputExamples` (optional): Whether to remove the `inputExamples` property from the tool after adding them to the description. Default: `true`.\n\n```ts\n1const model = wrapLanguageModel({\n\n2  model: yourModel,\n\n3  middleware: addToolInputExamplesMiddleware({\n\n4    examplesPrefix: 'Input Examples:',\n\n5    formatExample: (example, index) =>\n\n6      `${index + 1}. ${JSON.stringify(example.input)}`,\n\n7    removeInputExamples: true,\n\n8  }),\n\n9});\n```\n\n## [Community Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#community-middleware)\n\nThe AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.\n\nHere are some community middlewares that you can explore:\n\n### [Custom tool call parser](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#custom-tool-call-parser)\n\nThe [Custom tool call parser](https://github.com/minpeter/ai-sdk-tool-call-middleware) middleware extends tool call capabilities to models that don't natively support the OpenAI-style `tools` parameter. This includes many self-hosted and third-party models that lack native function calling features.\n\nUsing this middleware on models that support native function calls may result\nin unintended performance degradation, so check whether your model supports\nnative function calls before deciding to use it.\n\nThis middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model's responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don't natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.\n\nThe `@ai-sdk-tool/parser` package offers three middleware variants:\n\n- `createToolMiddleware`: A flexible function for creating custom tool call middleware tailored to specific models\n- `hermesToolMiddleware`: Ready-to-use middleware for Hermes & Qwen format function calls\n- `gemmaToolMiddleware`: Pre-configured middleware for Gemma 3 model series function call format\n\nHere's how you can enable function calls with Gemma models that don't support them natively:\n\n```ts\n1import { wrapLanguageModel } from 'ai';\n\n2import { gemmaToolMiddleware } from '@ai-sdk-tool/parser';\n\n3\n\n4const model = wrapLanguageModel({\n\n5  model: openrouter('google/gemma-3-27b-it'),\n\n6  middleware: gemmaToolMiddleware,\n\n7});\n```\n\nFind more examples at this [link](https://github.com/minpeter/ai-sdk-tool-call-middleware/tree/main/examples/core/src).\n\n## [Implementing Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#implementing-language-model-middleware)\n\nImplementing language model middleware is advanced functionality and requires\na solid understanding of the [language model\\\\\nspecification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\n\nYou can implement any of the following three function to modify the behavior of the language model:\n\n1. `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.\n2. `wrapGenerate`: Wraps the `doGenerate` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\nYou can modify the parameters, call the language model, and modify the result.\n3. `wrapStream`: Wraps the `doStream` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\nYou can modify the parameters, call the language model, and modify the result.\n\nHere are some examples of how to implement language model middleware:\n\n## [Examples](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#examples)\n\nThese examples are not meant to be used in production. They are just to show\nhow you can use middleware to enhance the behavior of language models.\n\n### [Logging](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#logging)\n\nThis example shows how to log the parameters and generated text of a language model call.\n\n```ts\n1import type {\n\n2  LanguageModelV3Middleware,\n\n3  LanguageModelV3StreamPart,\n\n4} from '@ai-sdk/provider';\n\n5\n\n6export const yourLogMiddleware: LanguageModelV3Middleware = {\n\n7  wrapGenerate: async ({ doGenerate, params }) => {\n\n8    console.log('doGenerate called');\n\n9    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n10\n\n11    const result = await doGenerate();\n\n12\n\n13    console.log('doGenerate finished');\n\n14    console.log(`generated text: ${result.text}`);\n\n15\n\n16    return result;\n\n17  },\n\n18\n\n19  wrapStream: async ({ doStream, params }) => {\n\n20    console.log('doStream called');\n\n21    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n22\n\n23    const { stream, ...rest } = await doStream();\n\n24\n\n25    let generatedText = '';\n\n26    const textBlocks = new Map<string, string>();\n\n27\n\n28    const transformStream = new TransformStream<\n\n29      LanguageModelV3StreamPart,\n\n30      LanguageModelV3StreamPart\n\n31    >({\n\n32      transform(chunk, controller) {\n\n33        switch (chunk.type) {\n\n34          case 'text-start': {\n\n35            textBlocks.set(chunk.id, '');\n\n36            break;\n\n37          }\n\n38          case 'text-delta': {\n\n39            const existing = textBlocks.get(chunk.id) || '';\n\n40            textBlocks.set(chunk.id, existing + chunk.delta);\n\n41            generatedText += chunk.delta;\n\n42            break;\n\n43          }\n\n44          case 'text-end': {\n\n45            console.log(\n\n46              `Text block ${chunk.id} completed:`,\n\n47              textBlocks.get(chunk.id),\n\n48            );\n\n49            break;\n\n50          }\n\n51        }\n\n52\n\n53        controller.enqueue(chunk);\n\n54      },\n\n55\n\n56      flush() {\n\n57        console.log('doStream finished');\n\n58        console.log(`generated text: ${generatedText}`);\n\n59      },\n\n60    });\n\n61\n\n62    return {\n\n63      stream: stream.pipeThrough(transformStream),\n\n64      ...rest,\n\n65    };\n\n66  },\n\n67};\n```\n\n### [Caching](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#caching)\n\nThis example shows how to build a simple cache for the generated text of a language model call.\n\n```ts\n1import type { LanguageModelV3Middleware } from '@ai-sdk/provider';\n\n2\n\n3const cache = new Map<string, any>();\n\n4\n\n5export const yourCacheMiddleware: LanguageModelV3Middleware = {\n\n6  wrapGenerate: async ({ doGenerate, params }) => {\n\n7    const cacheKey = JSON.stringify(params);\n\n8\n\n9    if (cache.has(cacheKey)) {\n\n10      return cache.get(cacheKey);\n\n11    }\n\n12\n\n13    const result = await doGenerate();\n\n14\n\n15    cache.set(cacheKey, result);\n\n16\n\n17    return result;\n\n18  },\n\n19\n\n20  // here you would implement the caching logic for streaming\n\n21};\n```\n\n### [Retrieval Augmented Generation (RAG)](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#retrieval-augmented-generation-rag)\n\nThis example shows how to use RAG as middleware.\n\nHelper functions like `getLastUserMessageText` and `findSources` are not part\nof the AI SDK. They are just used in this example to illustrate the concept of\nRAG.\n\n```ts\n1import type { LanguageModelV3Middleware } from '@ai-sdk/provider';\n\n2\n\n3export const yourRagMiddleware: LanguageModelV3Middleware = {\n\n4  transformParams: async ({ params }) => {\n\n5    const lastUserMessageText = getLastUserMessageText({\n\n6      prompt: params.prompt,\n\n7    });\n\n8\n\n9    if (lastUserMessageText == null) {\n\n10      return params; // do not use RAG (send unmodified parameters)\n\n11    }\n\n12\n\n13    const instruction =\n\n14      'Use the following information to answer the question:\\n' +\n\n15      findSources({ text: lastUserMessageText })\n\n16        .map(chunk => JSON.stringify(chunk))\n\n17        .join('\\n');\n\n18\n\n19    return addToLastUserMessage({ params, text: instruction });\n\n20  },\n\n21};\n```\n\n### [Guardrails](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#guardrails)\n\nGuard rails are a way to ensure that the generated text of a language model call\nis safe and appropriate. This example shows how to use guardrails as middleware.\n\n```ts\n1import type { LanguageModelV3Middleware } from '@ai-sdk/provider';\n\n2\n\n3export const yourGuardrailMiddleware: LanguageModelV3Middleware = {\n\n4  wrapGenerate: async ({ doGenerate }) => {\n\n5    const { text, ...rest } = await doGenerate();\n\n6\n\n7    // filtering approach, e.g. for PII or other sensitive information:\n\n8    const cleanedText = text?.replace(/badword/g, '<REDACTED>');\n\n9\n\n10    return { text: cleanedText, ...rest };\n\n11  },\n\n12\n\n13  // here you would implement the guardrail logic for streaming\n\n14  // Note: streaming guardrails are difficult to implement, because\n\n15  // you do not know the full content of the stream until it's finished.\n\n16};\n```\n\n## [Configuring Per Request Custom Metadata](https://ai-sdk.dev/docs/ai-sdk-core/middleware\\#configuring-per-request-custom-metadata)\n\nTo send and access custom metadata in Middleware, you can use `providerOptions`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText, wrapLanguageModel } from 'ai';\n\n2import type { LanguageModelV3Middleware } from '@ai-sdk/provider';\n\n3\n\n4export const yourLogMiddleware: LanguageModelV3Middleware = {\n\n5  wrapGenerate: async ({ doGenerate, params }) => {\n\n6    console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);\n\n7    const result = await doGenerate();\n\n8    return result;\n\n9  },\n\n10};\n\n11\n\n12const { text } = await generateText({\n\n13  model: wrapLanguageModel({\n\n14    model: \"anthropic/claude-sonnet-4.5\",\n\n15    middleware: yourLogMiddleware,\n\n16  }),\n\n17  prompt: 'Invent a new holiday and describe its traditions.',\n\n18  providerOptions: {\n\n19    yourLogMiddleware: {\n\n20      hello: 'world',\n\n21    },\n\n22  },\n\n23});\n\n24\n\n25console.log(text);\n```\n\nOn this page\n\n[Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware#language-model-middleware)\n\n[Using Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware#using-language-model-middleware)\n\n[Multiple middlewares](https://ai-sdk.dev/docs/ai-sdk-core/middleware#multiple-middlewares)\n\n[Built-in Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware#built-in-middleware)\n\n[Extract Reasoning](https://ai-sdk.dev/docs/ai-sdk-core/middleware#extract-reasoning)\n\n[Simulate Streaming](https://ai-sdk.dev/docs/ai-sdk-core/middleware#simulate-streaming)\n\n[Default Settings](https://ai-sdk.dev/docs/ai-sdk-core/middleware#default-settings)\n\n[Add Tool Input Examples](https://ai-sdk.dev/docs/ai-sdk-core/middleware#add-tool-input-examples)\n\n[Options](https://ai-sdk.dev/docs/ai-sdk-core/middleware#options)\n\n[Community Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware#community-middleware)\n\n[Custom tool call parser](https://ai-sdk.dev/docs/ai-sdk-core/middleware#custom-tool-call-parser)\n\n[Implementing Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware#implementing-language-model-middleware)\n\n[Examples](https://ai-sdk.dev/docs/ai-sdk-core/middleware#examples)\n\n[Logging](https://ai-sdk.dev/docs/ai-sdk-core/middleware#logging)\n\n[Caching](https://ai-sdk.dev/docs/ai-sdk-core/middleware#caching)\n\n[Retrieval Augmented Generation (RAG)](https://ai-sdk.dev/docs/ai-sdk-core/middleware#retrieval-augmented-generation-rag)\n\n[Guardrails](https://ai-sdk.dev/docs/ai-sdk-core/middleware#guardrails)\n\n[Configuring Per Request Custom Metadata](https://ai-sdk.dev/docs/ai-sdk-core/middleware#configuring-per-request-custom-metadata)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/middleware"
      },
      {
        "title": "Provider & Model Management",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
        "content": "# [Provider & Model Management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#provider--model-management)\n\nWhen you work with multiple providers and models, it is often desirable to manage them in a central place\nand access the models through simple string ids.\n\nThe AI SDK offers [custom providers](https://ai-sdk.dev/docs/reference/ai-sdk-core/custom-provider) and\na [provider registry](https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry) for this purpose:\n\n- With **custom providers**, you can pre-configure model settings, provide model name aliases,\nand limit the available models.\n- The **provider registry** lets you mix multiple providers and access them through simple string ids.\n\nYou can mix and match custom providers, the provider registry, and [middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware) in your application.\n\n## [Custom Providers](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#custom-providers)\n\nYou can create a [custom provider](https://ai-sdk.dev/docs/reference/ai-sdk-core/custom-provider) using `customProvider`.\n\n### [Example: custom model settings](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-custom-model-settings)\n\nYou might want to override the default model settings for a provider or provide model name aliases\nwith pre-configured settings.\n\n```ts\n1import {\n\n2  gateway,\n\n3  customProvider,\n\n4  defaultSettingsMiddleware,\n\n5  wrapLanguageModel,\n\n6} from 'ai';\n\n7\n\n8// custom provider with different provider options:\n\n9export const openai = customProvider({\n\n10  languageModels: {\n\n11    // replacement model with custom provider options:\n\n12    'gpt-5.1': wrapLanguageModel({\n\n13      model: gateway('openai/gpt-5.1'),\n\n14      middleware: defaultSettingsMiddleware({\n\n15        settings: {\n\n16          providerOptions: {\n\n17            openai: {\n\n18              reasoningEffort: 'high',\n\n19            },\n\n20          },\n\n21        },\n\n22      }),\n\n23    }),\n\n24    // alias model with custom provider options:\n\n25    'gpt-5.1-high-reasoning': wrapLanguageModel({\n\n26      model: gateway('openai/gpt-5.1'),\n\n27      middleware: defaultSettingsMiddleware({\n\n28        settings: {\n\n29          providerOptions: {\n\n30            openai: {\n\n31              reasoningEffort: 'high',\n\n32            },\n\n33          },\n\n34        },\n\n35      }),\n\n36    }),\n\n37  },\n\n38  fallbackProvider: gateway,\n\n39});\n```\n\n### [Example: model name alias](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-model-name-alias)\n\nYou can also provide model name aliases, so you can update the model version in one place in the future:\n\n```ts\n1import { customProvider, gateway } from 'ai';\n\n2\n\n3// custom provider with alias names:\n\n4export const anthropic = customProvider({\n\n5  languageModels: {\n\n6    opus: gateway('anthropic/claude-opus-4.1'),\n\n7    sonnet: gateway('anthropic/claude-sonnet-4.5'),\n\n8    haiku: gateway('anthropic/claude-haiku-4.5'),\n\n9  },\n\n10  fallbackProvider: gateway,\n\n11});\n```\n\n### [Example: limit available models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-limit-available-models)\n\nYou can limit the available models in the system, even if you have multiple providers.\n\n```ts\n1import {\n\n2  customProvider,\n\n3  defaultSettingsMiddleware,\n\n4  wrapLanguageModel,\n\n5  gateway,\n\n6} from 'ai';\n\n7\n\n8export const myProvider = customProvider({\n\n9  languageModels: {\n\n10    'text-medium': gateway('anthropic/claude-3-5-sonnet-20240620'),\n\n11    'text-small': gateway('openai/gpt-5-mini'),\n\n12    'reasoning-medium': wrapLanguageModel({\n\n13      model: gateway('openai/gpt-5.1'),\n\n14      middleware: defaultSettingsMiddleware({\n\n15        settings: {\n\n16          providerOptions: {\n\n17            openai: {\n\n18              reasoningEffort: 'high',\n\n19            },\n\n20          },\n\n21        },\n\n22      }),\n\n23    }),\n\n24    'reasoning-fast': wrapLanguageModel({\n\n25      model: gateway('openai/gpt-5.1'),\n\n26      middleware: defaultSettingsMiddleware({\n\n27        settings: {\n\n28          providerOptions: {\n\n29            openai: {\n\n30              reasoningEffort: 'low',\n\n31            },\n\n32          },\n\n33        },\n\n34      }),\n\n35    }),\n\n36  },\n\n37  embeddingModels: {\n\n38    embedding: gateway.embeddingModel('openai/text-embedding-3-small'),\n\n39  },\n\n40  // no fallback provider\n\n41});\n```\n\n## [Provider Registry](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#provider-registry)\n\nYou can create a [provider registry](https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry) with multiple providers and models using `createProviderRegistry`.\n\n### [Setup](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#setup)\n\nregistry.ts\n\n```ts\n1import { anthropic } from '@ai-sdk/anthropic';\n\n2import { openai } from '@ai-sdk/openai';\n\n3import { createProviderRegistry, gateway } from 'ai';\n\n4\n\n5export const registry = createProviderRegistry({\n\n6  // register provider with prefix and default setup using gateway:\n\n7  gateway,\n\n8\n\n9  // register provider with prefix and direct provider import:\n\n10  anthropic,\n\n11  openai,\n\n12});\n```\n\n### [Setup with Custom Separator](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#setup-with-custom-separator)\n\nBy default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator:\n\nregistry.ts\n\n```ts\n1import { anthropic } from '@ai-sdk/anthropic';\n\n2import { openai } from '@ai-sdk/openai';\n\n3import { createProviderRegistry, gateway } from 'ai';\n\n4\n\n5export const customSeparatorRegistry = createProviderRegistry(\n\n6  {\n\n7    gateway,\n\n8    anthropic,\n\n9    openai,\n\n10  },\n\n11  { separator: ' > ' },\n\n12);\n```\n\n### [Example: Use language models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-use-language-models)\n\nYou can access language models by using the `languageModel` method on the registry.\nThe provider id will become the prefix of the model id: `providerId:modelId`.\n\n```ts\n1import { generateText } from 'ai';\n\n2import { registry } from './registry';\n\n3\n\n4const { text } = await generateText({\n\n5  model: registry.languageModel('openai:gpt-5.1'), // default separator\n\n6  // or with custom separator:\n\n7  // model: customSeparatorRegistry.languageModel('openai > gpt-5.1'),\n\n8  prompt: 'Invent a new holiday and describe its traditions.',\n\n9});\n```\n\n### [Example: Use text embedding models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-use-text-embedding-models)\n\nYou can access text embedding models by using the `.embeddingModel` method on the registry.\nThe provider id will become the prefix of the model id: `providerId:modelId`.\n\n```ts\n1import { embed } from 'ai';\n\n2import { registry } from './registry';\n\n3\n\n4const { embedding } = await embed({\n\n5  model: registry.embeddingModel('openai:text-embedding-3-small'),\n\n6  value: 'sunny day at the beach',\n\n7});\n```\n\n### [Example: Use image models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#example-use-image-models)\n\nYou can access image models by using the `imageModel` method on the registry.\nThe provider id will become the prefix of the model id: `providerId:modelId`.\n\n```ts\n1import { generateImage } from 'ai';\n\n2import { registry } from './registry';\n\n3\n\n4const { image } = await generateImage({\n\n5  model: registry.imageModel('openai:dall-e-3'),\n\n6  prompt: 'A beautiful sunset over a calm ocean',\n\n7});\n```\n\n## [Combining Custom Providers, Provider Registry, and Middleware](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#combining-custom-providers-provider-registry-and-middleware)\n\nThe central idea of provider management is to set up a file that contains all the providers and models you want to use.\nYou may want to pre-configure model settings, provide model name aliases, limit the available models, and more.\n\nHere is an example that implements the following concepts:\n\n- pass through gateway with a namespace prefix (here: `gateway > *`)\n- pass through a full provider with a namespace prefix (here: `xai > *`)\n- setup an OpenAI-compatible provider with custom api key and base URL (here: `custom > *`)\n- setup model name aliases (here: `anthropic > fast`, `anthropic > writing`, `anthropic > reasoning`)\n- pre-configure model settings (here: `anthropic > reasoning`)\n- validate the provider-specific options (here: `AnthropicProviderOptions`)\n- use a fallback provider (here: `anthropic > *`)\n- limit a provider to certain models without a fallback (here: `groq > gemma2-9b-it`, `groq > qwen-qwq-32b`)\n- define a custom separator for the provider registry (here: `>`)\n\n```ts\n1import { anthropic, AnthropicProviderOptions } from '@ai-sdk/anthropic';\n\n2import { createOpenAICompatible } from '@ai-sdk/openai-compatible';\n\n3import { xai } from '@ai-sdk/xai';\n\n4import { groq } from '@ai-sdk/groq';\n\n5import {\n\n6  createProviderRegistry,\n\n7  customProvider,\n\n8  defaultSettingsMiddleware,\n\n9  gateway,\n\n10  wrapLanguageModel,\n\n11} from 'ai';\n\n12\n\n13export const registry = createProviderRegistry(\n\n14  {\n\n15    // pass through gateway with a namespace prefix\n\n16    gateway,\n\n17\n\n18    // pass through full providers with namespace prefixes\n\n19    xai,\n\n20\n\n21    // access an OpenAI-compatible provider with custom setup\n\n22    custom: createOpenAICompatible({\n\n23      name: 'provider-name',\n\n24      apiKey: process.env.CUSTOM_API_KEY,\n\n25      baseURL: 'https://api.custom.com/v1',\n\n26    }),\n\n27\n\n28    // setup model name aliases\n\n29    anthropic: customProvider({\n\n30      languageModels: {\n\n31        fast: anthropic('claude-haiku-4-5'),\n\n32\n\n33        // simple model\n\n34        writing: anthropic('claude-sonnet-4-5'),\n\n35\n\n36        // extended reasoning model configuration:\n\n37        reasoning: wrapLanguageModel({\n\n38          model: anthropic('claude-sonnet-4-5'),\n\n39          middleware: defaultSettingsMiddleware({\n\n40            settings: {\n\n41              maxOutputTokens: 100000, // example default setting\n\n42              providerOptions: {\n\n43                anthropic: {\n\n44                  thinking: {\n\n45                    type: 'enabled',\n\n46                    budgetTokens: 32000,\n\n47                  },\n\n48                } satisfies AnthropicProviderOptions,\n\n49              },\n\n50            },\n\n51          }),\n\n52        }),\n\n53      },\n\n54      fallbackProvider: anthropic,\n\n55    }),\n\n56\n\n57    // limit a provider to certain models without a fallback\n\n58    groq: customProvider({\n\n59      languageModels: {\n\n60        'gemma2-9b-it': groq('gemma2-9b-it'),\n\n61        'qwen-qwq-32b': groq('qwen-qwq-32b'),\n\n62      },\n\n63    }),\n\n64  },\n\n65  { separator: ' > ' },\n\n66);\n\n67\n\n68// usage:\n\n69const model = registry.languageModel('anthropic > reasoning');\n```\n\n## [Global Provider Configuration](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#global-provider-configuration)\n\nThe AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const result = await streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\", // Uses the global provider (defaults to gateway)\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6});\n```\n\nBy default, the global provider is set to the Vercel AI Gateway.\n\n### [Customizing the Global Provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management\\#customizing-the-global-provider)\n\nYou can set your own preferred global provider:\n\nsetup.ts\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2\n\n3// Initialize once during startup:\n\n4globalThis.AI_SDK_DEFAULT_PROVIDER = openai;\n```\n\napp.ts\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const result = await streamText({\n\n4  model: 'gpt-5.1', // Uses OpenAI provider without prefix\n\n5  prompt: 'Invent a new holiday and describe its traditions.',\n\n6});\n```\n\nThis simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.\n\nOn this page\n\n[Provider & Model Management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#provider--model-management)\n\n[Custom Providers](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#custom-providers)\n\n[Example: custom model settings](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-custom-model-settings)\n\n[Example: model name alias](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-model-name-alias)\n\n[Example: limit available models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-limit-available-models)\n\n[Provider Registry](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#provider-registry)\n\n[Setup](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#setup)\n\n[Setup with Custom Separator](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#setup-with-custom-separator)\n\n[Example: Use language models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-use-language-models)\n\n[Example: Use text embedding models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-use-text-embedding-models)\n\n[Example: Use image models](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#example-use-image-models)\n\n[Combining Custom Providers, Provider Registry, and Middleware](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#combining-custom-providers-provider-registry-and-middleware)\n\n[Global Provider Configuration](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#global-provider-configuration)\n\n[Customizing the Global Provider](https://ai-sdk.dev/docs/ai-sdk-core/provider-management#customizing-the-global-provider)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management"
      },
      {
        "title": "Error Handling",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
        "content": "# [Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/error-handling\\#error-handling)\n\n## [Handling regular errors](https://ai-sdk.dev/docs/ai-sdk-core/error-handling\\#handling-regular-errors)\n\nRegular errors are thrown and can be handled using the `try/catch` block.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3try {\n\n4  const { text } = await generateText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n7  });\n\n8} catch (error) {\n\n9  // handle error\n\n10}\n```\n\nSee [Error Types](https://ai-sdk.dev/docs/reference/ai-sdk-errors) for more information on the different types of errors that may be thrown.\n\n## [Handling streaming errors (simple streams)](https://ai-sdk.dev/docs/ai-sdk-core/error-handling\\#handling-streaming-errors-simple-streams)\n\nWhen errors occur during streams that do not support error chunks,\nthe error is thrown as a regular error.\nYou can handle these errors using the `try/catch` block.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3try {\n\n4  const { textStream } = streamText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n7  });\n\n8\n\n9  for await (const textPart of textStream) {\n\n10    process.stdout.write(textPart);\n\n11  }\n\n12} catch (error) {\n\n13  // handle error\n\n14}\n```\n\n## [Handling streaming errors (streaming with `error` support)](https://ai-sdk.dev/docs/ai-sdk-core/error-handling\\#handling-streaming-errors-streaming-with-error-support)\n\nFull streams support error parts.\nYou can handle those parts similar to other parts.\nIt is recommended to also add a try-catch block for errors that\nhappen outside of the streaming.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3try {\n\n4  const { fullStream } = streamText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n7  });\n\n8\n\n9  for await (const part of fullStream) {\n\n10    switch (part.type) {\n\n11      // ... handle other part types\n\n12\n\n13      case 'error': {\n\n14        const error = part.error;\n\n15        // handle error\n\n16        break;\n\n17      }\n\n18\n\n19      case 'abort': {\n\n20        // handle stream abort\n\n21        break;\n\n22      }\n\n23\n\n24      case 'tool-error': {\n\n25        const error = part.error;\n\n26        // handle error\n\n27        break;\n\n28      }\n\n29    }\n\n30  }\n\n31} catch (error) {\n\n32  // handle error\n\n33}\n```\n\n## [Handling stream aborts](https://ai-sdk.dev/docs/ai-sdk-core/error-handling\\#handling-stream-aborts)\n\nWhen streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the `onAbort` callback to handle these cases.\n\nThe `onAbort` callback is called when a stream is aborted via `AbortSignal`, but `onFinish` is not called. This ensures you can still update your UI state appropriately.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const { textStream } = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n6  onAbort: ({ steps }) => {\n\n7    // Update stored messages or perform cleanup\n\n8    console.log('Stream aborted after', steps.length, 'steps');\n\n9  },\n\n10  onFinish: ({ steps, totalUsage }) => {\n\n11    // This is called on normal completion\n\n12    console.log('Stream completed normally');\n\n13  },\n\n14});\n\n15\n\n16for await (const textPart of textStream) {\n\n17  process.stdout.write(textPart);\n\n18}\n```\n\nThe `onAbort` callback receives:\n\n- `steps`: An array of all completed steps before the abort\n\nYou can also handle abort events directly in the stream:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3const { fullStream } = streamText({\n\n4  model: \"anthropic/claude-sonnet-4.5\",\n\n5  prompt: 'Write a vegetarian lasagna recipe for 4 people.',\n\n6});\n\n7\n\n8for await (const chunk of fullStream) {\n\n9  switch (chunk.type) {\n\n10    case 'abort': {\n\n11      // Handle abort directly in stream\n\n12      console.log('Stream was aborted');\n\n13      break;\n\n14    }\n\n15    // ... handle other part types\n\n16  }\n\n17}\n```\n\nOn this page\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/error-handling#error-handling)\n\n[Handling regular errors](https://ai-sdk.dev/docs/ai-sdk-core/error-handling#handling-regular-errors)\n\n[Handling streaming errors (simple streams)](https://ai-sdk.dev/docs/ai-sdk-core/error-handling#handling-streaming-errors-simple-streams)\n\n[Handling streaming errors (streaming with error support)](https://ai-sdk.dev/docs/ai-sdk-core/error-handling#handling-streaming-errors-streaming-with-error-support)\n\n[Handling stream aborts](https://ai-sdk.dev/docs/ai-sdk-core/error-handling#handling-stream-aborts)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling"
      },
      {
        "title": "Testing",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
        "content": "# [Testing](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#testing)\n\nTesting language models can be challenging, because they are non-deterministic\nand calling them is slow and expensive.\n\nTo enable you to unit test your code that uses the AI SDK, the AI SDK Core\nincludes mock providers and test helpers. You can import the following helpers from `ai/test`:\n\n- `MockEmbeddingModelV3`: A mock embedding model using the [embedding model v3 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/embedding-model/v3/embedding-model-v3.ts).\n- `MockLanguageModelV3`: A mock language model using the [language model v3 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v3/language-model-v3.ts).\n- `mockId`: Provides an incrementing integer ID.\n- `mockValues`: Iterates over an array of values with each call. Returns the last value when the array is exhausted.\n- [`simulateReadableStream`](https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-readable-stream): Simulates a readable stream with delays.\n\nWith mock providers and test helpers, you can control the output of the AI SDK\nand test your code in a repeatable and deterministic way without actually calling\na language model provider.\n\n## [Examples](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#examples)\n\nYou can use the test helpers with the AI Core functions in your unit tests:\n\n### [generateText](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#generatetext)\n\n```ts\n1import { generateText } from 'ai';\n\n2import { MockLanguageModelV3 } from 'ai/test';\n\n3\n\n4const result = await generateText({\n\n5  model: new MockLanguageModelV3({\n\n6    doGenerate: async () => ({\n\n7      finishReason: 'stop',\n\n8      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\n\n9      content: [{ type: 'text', text: `Hello, world!` }],\n\n10      warnings: [],\n\n11    }),\n\n12  }),\n\n13  prompt: 'Hello, test!',\n\n14});\n```\n\n### [streamText](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#streamtext)\n\n```ts\n1import { streamText, simulateReadableStream } from 'ai';\n\n2import { MockLanguageModelV3 } from 'ai/test';\n\n3\n\n4const result = streamText({\n\n5  model: new MockLanguageModelV3({\n\n6    doStream: async () => ({\n\n7      stream: simulateReadableStream({\n\n8        chunks: [\\\n\\\n9          { type: 'text-start', id: 'text-1' },\\\n\\\n10          { type: 'text-delta', id: 'text-1', delta: 'Hello' },\\\n\\\n11          { type: 'text-delta', id: 'text-1', delta: ', ' },\\\n\\\n12          { type: 'text-delta', id: 'text-1', delta: 'world!' },\\\n\\\n13          { type: 'text-end', id: 'text-1' },\\\n\\\n14          {\\\n\\\n15            type: 'finish',\\\n\\\n16            finishReason: 'stop',\\\n\\\n17            logprobs: undefined,\\\n\\\n18            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\\n\\\n19          },\\\n\\\n20        ],\n\n21      }),\n\n22    }),\n\n23  }),\n\n24  prompt: 'Hello, test!',\n\n25});\n```\n\n### [generateObject](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#generateobject)\n\n```ts\n1import { generateObject } from 'ai';\n\n2import { MockLanguageModelV3 } from 'ai/test';\n\n3import { z } from 'zod';\n\n4\n\n5const result = await generateObject({\n\n6  model: new MockLanguageModelV3({\n\n7    doGenerate: async () => ({\n\n8      finishReason: 'stop',\n\n9      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\n\n10      content: [{ type: 'text', text: `{\"content\":\"Hello, world!\"}` }],\n\n11      warnings: [],\n\n12    }),\n\n13  }),\n\n14  schema: z.object({ content: z.string() }),\n\n15  prompt: 'Hello, test!',\n\n16});\n```\n\n### [streamObject](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#streamobject)\n\n```ts\n1import { streamObject, simulateReadableStream } from 'ai';\n\n2import { MockLanguageModelV3 } from 'ai/test';\n\n3import { z } from 'zod';\n\n4\n\n5const result = streamObject({\n\n6  model: new MockLanguageModelV3({\n\n7    doStream: async () => ({\n\n8      stream: simulateReadableStream({\n\n9        chunks: [\\\n\\\n10          { type: 'text-start', id: 'text-1' },\\\n\\\n11          { type: 'text-delta', id: 'text-1', delta: '{ ' },\\\n\\\n12          { type: 'text-delta', id: 'text-1', delta: '\"content\": ' },\\\n\\\n13          { type: 'text-delta', id: 'text-1', delta: `\"Hello, ` },\\\n\\\n14          { type: 'text-delta', id: 'text-1', delta: `world` },\\\n\\\n15          { type: 'text-delta', id: 'text-1', delta: `!\"` },\\\n\\\n16          { type: 'text-delta', id: 'text-1', delta: ' }' },\\\n\\\n17          { type: 'text-end', id: 'text-1' },\\\n\\\n18          {\\\n\\\n19            type: 'finish',\\\n\\\n20            finishReason: 'stop',\\\n\\\n21            logprobs: undefined,\\\n\\\n22            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\\n\\\n23          },\\\n\\\n24        ],\n\n25      }),\n\n26    }),\n\n27  }),\n\n28  schema: z.object({ content: z.string() }),\n\n29  prompt: 'Hello, test!',\n\n30});\n```\n\n### [Simulate UI Message Stream Responses](https://ai-sdk.dev/docs/ai-sdk-core/testing\\#simulate-ui-message-stream-responses)\n\nYou can also simulate [UI Message Stream](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#ui-message-stream) responses for testing,\ndebugging, or demonstration purposes.\n\nHere is a Next example:\n\nroute.ts\n\n```ts\n1import { simulateReadableStream } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  return new Response(\n\n5    simulateReadableStream({\n\n6      initialDelayInMs: 1000, // Delay before the first chunk\n\n7      chunkDelayInMs: 300, // Delay between chunks\n\n8      chunks: [\\\n\\\n9        `data: {\"type\":\"start\",\"messageId\":\"msg-123\"}\\n\\n`,\\\n\\\n10        `data: {\"type\":\"text-start\",\"id\":\"text-1\"}\\n\\n`,\\\n\\\n11        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\"This\"}\\n\\n`,\\\n\\\n12        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" is an\"}\\n\\n`,\\\n\\\n13        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" example.\"}\\n\\n`,\\\n\\\n14        `data: {\"type\":\"text-end\",\"id\":\"text-1\"}\\n\\n`,\\\n\\\n15        `data: {\"type\":\"finish\"}\\n\\n`,\\\n\\\n16        `data: [DONE]\\n\\n`,\\\n\\\n17      ],\n\n18    }).pipeThrough(new TextEncoderStream()),\n\n19    {\n\n20      status: 200,\n\n21      headers: {\n\n22        'Content-Type': 'text/event-stream',\n\n23        'Cache-Control': 'no-cache',\n\n24        Connection: 'keep-alive',\n\n25        'x-vercel-ai-ui-message-stream': 'v1',\n\n26      },\n\n27    },\n\n28  );\n\n29}\n```\n\nOn this page\n\n[Testing](https://ai-sdk.dev/docs/ai-sdk-core/testing#testing)\n\n[Examples](https://ai-sdk.dev/docs/ai-sdk-core/testing#examples)\n\n[generateText](https://ai-sdk.dev/docs/ai-sdk-core/testing#generatetext)\n\n[streamText](https://ai-sdk.dev/docs/ai-sdk-core/testing#streamtext)\n\n[generateObject](https://ai-sdk.dev/docs/ai-sdk-core/testing#generateobject)\n\n[streamObject](https://ai-sdk.dev/docs/ai-sdk-core/testing#streamobject)\n\n[Simulate UI Message Stream Responses](https://ai-sdk.dev/docs/ai-sdk-core/testing#simulate-ui-message-stream-responses)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/testing"
      },
      {
        "title": "Telemetry",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
        "content": "# [Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#telemetry)\n\nAI SDK Telemetry is experimental and may change in the future.\n\nThe AI SDK uses [OpenTelemetry](https://opentelemetry.io/) to collect telemetry data.\nOpenTelemetry is an open-source observability framework designed to provide\nstandardized instrumentation for collecting telemetry data.\n\nCheck out the [AI SDK Observability Integrations](https://ai-sdk.dev/providers/observability)\nto see providers that offer monitoring and tracing for AI SDK applications.\n\n## [Enabling telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#enabling-telemetry)\n\nFor Next.js applications, please follow the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.\n\nYou can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Write a short story about a cat.',\n\n4  experimental_telemetry: { isEnabled: true },\n\n5});\n```\n\nWhen telemetry is enabled, you can also control if you want to record the input values and the output values for the function.\nBy default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.\n\nDisabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.\nYou might for example want to disable recording inputs if they contain sensitive information.\n\n## [Telemetry Metadata](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#telemetry-metadata)\n\nYou can provide a `functionId` to identify the function that the telemetry data is for,\nand `metadata` to include additional information in the telemetry data.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const result = await generateText({\n\n2  model: \"anthropic/claude-sonnet-4.5\",\n\n3  prompt: 'Write a short story about a cat.',\n\n4  experimental_telemetry: {\n\n5    isEnabled: true,\n\n6    functionId: 'my-awesome-function',\n\n7    metadata: {\n\n8      something: 'custom',\n\n9      someOtherThing: 'other-value',\n\n10    },\n\n11  },\n\n12});\n```\n\n## [Custom Tracer](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#custom-tracer)\n\nYou may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where\nyou want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1const tracerProvider = new NodeTracerProvider();\n\n2const result = await generateText({\n\n3  model: \"anthropic/claude-sonnet-4.5\",\n\n4  prompt: 'Write a short story about a cat.',\n\n5  experimental_telemetry: {\n\n6    isEnabled: true,\n\n7    tracer: tracerProvider.getTracer('ai'),\n\n8  },\n\n9});\n```\n\n## [Collected Data](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#collected-data)\n\n### [generateText function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#generatetext-function)\n\n`generateText` records 3 types of spans:\n\n- `ai.generateText` (span): the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans.\nIt contains the [basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.generateText\"`\n  - `ai.prompt`: the prompt that was used when calling `generateText`\n  - `ai.response.text`: the text that was generated\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\n  - `ai.response.finishReason`: the reason why the generation finished\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\n- `ai.generateText.doGenerate` (span): a provider doGenerate call. It can contain `ai.toolCall` spans.\nIt contains the [call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#call-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.generateText.doGenerate\"`\n  - `ai.prompt.messages`: the messages that were passed into the provider\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\n  - `ai.response.text`: the text that was generated\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\n  - `ai.response.finishReason`: the reason why the generation finished\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#tool-call-spans) for more details.\n\n\n### [streamText function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#streamtext-function)\n\n`streamText` records 3 types of spans and 2 types of events:\n\n- `ai.streamText` (span): the full length of the streamText call. It contains a `ai.streamText.doStream` span.\nIt contains the [basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.streamText\"`\n  - `ai.prompt`: the prompt that was used when calling `streamText`\n  - `ai.response.text`: the text that was generated\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\n  - `ai.response.finishReason`: the reason why the generation finished\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\n- `ai.streamText.doStream` (span): a provider doStream call.\nThis span contains an `ai.stream.firstChunk` event and `ai.toolCall` spans.\nIt contains the [call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#call-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.streamText.doStream\"`\n  - `ai.prompt.messages`: the messages that were passed into the provider\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\n  - `ai.response.text`: the text that was generated\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk in milliseconds\n  - `ai.response.msToFinish`: the time it took to receive the finish part of the LLM stream in milliseconds\n  - `ai.response.avgCompletionTokensPerSecond`: the average number of completion tokens per second\n  - `ai.response.finishReason`: the reason why the generation finished\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#tool-call-spans) for more details.\n\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\n- `ai.stream.finish` (event): an event that is emitted when the finish part of the LLM stream is received.\n\n\nIt also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.\n\n### [generateObject function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#generateobject-function)\n\n`generateObject` records 2 types of spans:\n\n- `ai.generateObject` (span): the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans.\nIt contains the [basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.generateObject\"`\n  - `ai.prompt`: the prompt that was used when calling `generateObject`\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function\n  - `ai.schema.name`: the name of the schema that was passed into the `generateObject` function\n  - `ai.schema.description`: the description of the schema that was passed into the `generateObject` function\n  - `ai.response.object`: the object that was generated (stringified JSON)\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\n- `ai.generateObject.doGenerate` (span): a provider doGenerate call.\nIt contains the [call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#call-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.generateObject.doGenerate\"`\n  - `ai.prompt.messages`: the messages that were passed into the provider\n  - `ai.response.object`: the object that was generated (stringified JSON)\n  - `ai.response.finishReason`: the reason why the generation finished\n\n### [streamObject function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#streamobject-function)\n\n`streamObject` records 2 types of spans and 1 type of event:\n\n- `ai.streamObject` (span): the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans.\nIt contains the [basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.streamObject\"`\n  - `ai.prompt`: the prompt that was used when calling `streamObject`\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function\n  - `ai.schema.name`: the name of the schema that was passed into the `streamObject` function\n  - `ai.schema.description`: the description of the schema that was passed into the `streamObject` function\n  - `ai.response.object`: the object that was generated (stringified JSON)\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\n- `ai.streamObject.doStream` (span): a provider doStream call.\nThis span contains an `ai.stream.firstChunk` event.\nIt contains the [call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#call-llm-span-information) and the following attributes:\n  - `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.streamObject.doStream\"`\n  - `ai.prompt.messages`: the messages that were passed into the provider\n  - `ai.response.object`: the object that was generated (stringified JSON)\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\n  - `ai.response.finishReason`: the reason why the generation finished\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\n\n### [embed function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#embed-function)\n\n`embed` records 2 types of spans:\n\n- `ai.embed` (span): the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans.\nIt contains the [basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-embedding-span-information) and the following attributes:\n  - `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.embed\"`\n  - `ai.value`: the value that was passed into the `embed` function\n  - `ai.embedding`: a JSON-stringified embedding\n- `ai.embed.doEmbed` (span): a provider doEmbed call.\nIt contains the [basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-embedding-span-information) and the following attributes:\n  - `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.embed.doEmbed\"`\n  - `ai.values`: the values that were passed into the provider (array)\n  - `ai.embeddings`: an array of JSON-stringified embeddings\n\n### [embedMany function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#embedmany-function)\n\n`embedMany` records 2 types of spans:\n\n- `ai.embedMany` (span): the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans.\nIt contains the [basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-embedding-span-information) and the following attributes:\n  - `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.embedMany\"`\n  - `ai.values`: the values that were passed into the `embedMany` function\n  - `ai.embeddings`: an array of JSON-stringified embedding\n- `ai.embedMany.doEmbed` (span): a provider doEmbed call.\nIt contains the [basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-embedding-span-information) and the following attributes:\n  - `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`\n  - `ai.operationId`: `\"ai.embedMany.doEmbed\"`\n  - `ai.values`: the values that were sent to the provider\n  - `ai.embeddings`: an array of JSON-stringified embeddings for each value\n\n## [Span Details](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#span-details)\n\n### [Basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#basic-llm-span-information)\n\nMany spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`,\n`ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:\n\n- `resource.name`: the functionId that was set through `telemetry.functionId`\n- `ai.model.id`: the id of the model\n- `ai.model.provider`: the provider of the model\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\n- `ai.response.providerMetadata`: provider specific metadata returned with the generation response\n- `ai.settings.maxRetries`: the maximum number of retries that were set\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\n- `ai.usage.completionTokens`: the number of completion tokens that were used\n- `ai.usage.promptTokens`: the number of prompt tokens that were used\n\n### [Call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#call-llm-span-information)\n\nSpans that correspond to individual LLM calls (`ai.generateText.doGenerate`, `ai.streamText.doStream`, `ai.generateObject.doGenerate`, `ai.streamObject.doStream`) contain\n[basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information) and the following attributes:\n\n- `ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\n- `ai.response.id`: the id of the response. Uses the ID from the provider when available.\n- `ai.response.timestamp`: the timestamp of the response. Uses the timestamp from the provider when available.\n- [Semantic Conventions for GenAI operations](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/)\n  - `gen_ai.system`: the provider that was used\n  - `gen_ai.request.model`: the model that was requested\n  - `gen_ai.request.temperature`: the temperature that was set\n  - `gen_ai.request.max_tokens`: the maximum number of tokens that were set\n  - `gen_ai.request.frequency_penalty`: the frequency penalty that was set\n  - `gen_ai.request.presence_penalty`: the presence penalty that was set\n  - `gen_ai.request.top_k`: the topK parameter value that was set\n  - `gen_ai.request.top_p`: the topP parameter value that was set\n  - `gen_ai.request.stop_sequences`: the stop sequences\n  - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider\n  - `gen_ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\n  - `gen_ai.response.id`: the id of the response. Uses the ID from the provider when available.\n  - `gen_ai.usage.input_tokens`: the number of prompt tokens that were used\n  - `gen_ai.usage.output_tokens`: the number of completion tokens that were used\n\n### [Basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#basic-embedding-span-information)\n\nMany spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:\n\n- `ai.model.id`: the id of the model\n- `ai.model.provider`: the provider of the model\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\n- `ai.settings.maxRetries`: the maximum number of retries that were set\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\n- `ai.usage.tokens`: the number of tokens that were used\n- `resource.name`: the functionId that was set through `telemetry.functionId`\n\n### [Tool call spans](https://ai-sdk.dev/docs/ai-sdk-core/telemetry\\#tool-call-spans)\n\nTool call spans (`ai.toolCall`) contain the following attributes:\n\n- `operation.name`: `\"ai.toolCall\"`\n- `ai.operationId`: `\"ai.toolCall\"`\n- `ai.toolCall.name`: the name of the tool\n- `ai.toolCall.id`: the id of the tool call\n- `ai.toolCall.args`: the input parameters of the tool call\n- `ai.toolCall.result`: the output result of the tool call. Only available if the tool call is successful and the result is serializable.\n\nOn this page\n\n[Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#telemetry)\n\n[Enabling telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#enabling-telemetry)\n\n[Telemetry Metadata](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#telemetry-metadata)\n\n[Custom Tracer](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#custom-tracer)\n\n[Collected Data](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#collected-data)\n\n[generateText function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#generatetext-function)\n\n[streamText function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#streamtext-function)\n\n[generateObject function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#generateobject-function)\n\n[streamObject function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#streamobject-function)\n\n[embed function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#embed-function)\n\n[embedMany function](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#embedmany-function)\n\n[Span Details](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#span-details)\n\n[Basic LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-llm-span-information)\n\n[Call LLM span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#call-llm-span-information)\n\n[Basic embedding span information](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#basic-embedding-span-information)\n\n[Tool call spans](https://ai-sdk.dev/docs/ai-sdk-core/telemetry#tool-call-spans)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry"
      },
      {
        "title": "DevTools",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-core/devtools",
        "content": "# [DevTools](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#devtools)\n\nAI SDK DevTools is experimental and intended for local development only. Do\nnot use in production environments.\n\nAI SDK DevTools gives you full visibility over your AI SDK calls with [`generateText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text), [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), and [`ToolLoopAgent`](https://ai-sdk.dev/docs/reference/ai-sdk-core/tool-loop-agent). It helps you debug and inspect LLM requests, responses, tool calls, and multi-step interactions through a web-based UI.\n\nDevTools is composed of two parts:\n\n1. **Middleware**: Captures runs and steps from your AI SDK calls\n2. **Viewer**: A web UI to inspect the captured data\n\n## [Installation](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#installation)\n\nInstall the DevTools package:\n\n```bash\n1pnpm add @ai-sdk/devtools\n```\n\n## [Requirements](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#requirements)\n\n- AI SDK v6 beta (`ai@^6.0.0-beta.0`)\n- Node.js compatible runtime\n\n## [Using DevTools](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#using-devtools)\n\n### [Add the middleware](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#add-the-middleware)\n\nWrap your language model with the DevTools middleware using [`wrapLanguageModel`](https://ai-sdk.dev/docs/ai-sdk-core/middleware):\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { wrapLanguageModel } from 'ai';\n\n2import { devToolsMiddleware } from '@ai-sdk/devtools';\n\n3\n\n4const model = wrapLanguageModel({\n\n5  model: \"anthropic/claude-sonnet-4.5\",\n\n6  middleware: devToolsMiddleware,\n\n7});\n```\n\nThe wrapped model can be used with any AI SDK Core function:\n\n```ts\n1import { generateText } from 'ai';\n\n2\n\n3const result = await generateText({\n\n4  model, // wrapped model with DevTools\n\n5  prompt: 'What cities are in the United States?',\n\n6});\n```\n\n### [Launch the viewer](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#launch-the-viewer)\n\nStart the DevTools viewer:\n\n```bash\n1npx @ai-sdk/devtools\n```\n\nOpen [http://localhost:4983](http://localhost:4983/) to view your AI SDK interactions.\n\n## [Captured data](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#captured-data)\n\nThe DevTools middleware captures the following information from your AI SDK calls:\n\n- **Input parameters and prompts**: View the complete input sent to your LLM\n- **Output content and tool calls**: Inspect generated text and tool invocations\n- **Token usage and timing**: Monitor resource consumption and performance\n- **Raw provider data**: Access complete request and response payloads\n\n### [Runs and steps](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#runs-and-steps)\n\nDevTools organizes captured data into runs and steps:\n\n- **Run**: A complete multi-step AI interaction, grouped by the initial prompt\n- **Step**: A single LLM call within a run (e.g., one `generateText` or `streamText` call)\n\nMulti-step interactions, such as those created by tool calling or agent loops, are grouped together as a single run with multiple steps.\n\n## [How it works](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#how-it-works)\n\nThe DevTools middleware intercepts all `generateText` and `streamText` calls through the [language model middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware) system. Captured data is stored locally in a JSON file (`.devtools/generations.json`) and served through a web UI built with Hono and React.\n\nThe middleware automatically adds `.devtools` to your `.gitignore` file.\nVerify that `.devtools` is in your `.gitignore` to ensure you don't commit\nsensitive AI interaction data to your repository.\n\n## [Security considerations](https://ai-sdk.dev/docs/ai-sdk-core/devtools\\#security-considerations)\n\nDevTools stores all AI interactions locally in plain text files, including:\n\n- User prompts and messages\n- LLM responses\n- Tool call arguments and results\n- API request and response data\n\n**Only use DevTools in local development environments.** Do not enable DevTools in production or when handling sensitive data.\n\nOn this page\n\n[DevTools](https://ai-sdk.dev/docs/ai-sdk-core/devtools#devtools)\n\n[Installation](https://ai-sdk.dev/docs/ai-sdk-core/devtools#installation)\n\n[Requirements](https://ai-sdk.dev/docs/ai-sdk-core/devtools#requirements)\n\n[Using DevTools](https://ai-sdk.dev/docs/ai-sdk-core/devtools#using-devtools)\n\n[Add the middleware](https://ai-sdk.dev/docs/ai-sdk-core/devtools#add-the-middleware)\n\n[Launch the viewer](https://ai-sdk.dev/docs/ai-sdk-core/devtools#launch-the-viewer)\n\n[Captured data](https://ai-sdk.dev/docs/ai-sdk-core/devtools#captured-data)\n\n[Runs and steps](https://ai-sdk.dev/docs/ai-sdk-core/devtools#runs-and-steps)\n\n[How it works](https://ai-sdk.dev/docs/ai-sdk-core/devtools#how-it-works)\n\n[Security considerations](https://ai-sdk.dev/docs/ai-sdk-core/devtools#security-considerations)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-core/devtools",
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/devtools",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-core/devtools"
      }
    ],
    "ai_sdk_ui": [
      {
        "title": "AI SDK UI",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
        "content": "# [AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview\\#ai-sdk-ui)\n\nAI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a **framework-agnostic toolkit**, streamlining the integration of advanced AI functionalities into your applications.\n\nAI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — **`useChat`**, **`useCompletion`**, and **`useObject`** — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.\n\n- **[`useChat`](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot)** offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.\n- **[`useCompletion`](https://ai-sdk.dev/docs/ai-sdk-ui/completion)** enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.\n- **[`useObject`](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation)** is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.\n\nThese hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.\n\n## [UI Framework Support](https://ai-sdk.dev/docs/ai-sdk-ui/overview\\#ui-framework-support)\n\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/),\n[Angular](https://angular.dev/), and [SolidJS](https://www.solidjs.com/).\n\nHere is a comparison of the supported functions across these frameworks:\n\n|  | [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) | [useCompletion](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion) | [useObject](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) |\n| --- | --- | --- | --- |\n| React `@ai-sdk/react` |  |  |  |\n| Vue.js `@ai-sdk/vue` |  |  |  |\n| Svelte `@ai-sdk/svelte` | Chat | Completion | StructuredObject |\n| Angular `@ai-sdk/angular` | Chat | Completion | StructuredObject |\n| [SolidJS](https://github.com/kodehort/ai-sdk-solid) (community) |  |  |  |\n\n## [Framework Examples](https://ai-sdk.dev/docs/ai-sdk-ui/overview\\#framework-examples)\n\nExplore these example implementations for different frameworks:\n\n- [**Next.js**](https://github.com/vercel/ai/tree/main/examples/next-openai)\n- [**Nuxt**](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)\n- [**SvelteKit**](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)\n- [**Angular**](https://github.com/vercel/ai/tree/main/examples/angular)\n\n## [API Reference](https://ai-sdk.dev/docs/ai-sdk-ui/overview\\#api-reference)\n\nPlease check out the [AI SDK UI API Reference](https://ai-sdk.dev/docs/reference/ai-sdk-ui) for more details on each function.\n\nOn this page\n\n[AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui/overview#ai-sdk-ui)\n\n[UI Framework Support](https://ai-sdk.dev/docs/ai-sdk-ui/overview#ui-framework-support)\n\n[Framework Examples](https://ai-sdk.dev/docs/ai-sdk-ui/overview#framework-examples)\n\n[API Reference](https://ai-sdk.dev/docs/ai-sdk-ui/overview#api-reference)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/overview"
      },
      {
        "title": "Chatbot",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
        "content": "# [Chatbot](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#chatbot)\n\nThe `useChat` hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.\n\nTo summarize, the `useChat` hook provides the following features:\n\n- **Message Streaming**: All the messages from the AI provider are streamed to the chat UI in real-time.\n- **Managed States**: The hook manages the states for input, messages, status, error and more for you.\n- **Seamless Integration**: Easily integrate your chat AI into any design or layout with minimal effort.\n\nIn this guide, you will learn how to use the `useChat` hook to create a chatbot application with real-time message streaming.\nCheck out our [chatbot with tools guide](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-with-tool-calling) to learn how to use tools in your chatbot.\nLet's start with the following example first.\n\n## [Example](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#example)\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport } from 'ai';\n\n5import { useState } from 'react';\n\n6\n\n7export default function Page() {\n\n8  const { messages, sendMessage, status } = useChat({\n\n9    transport: new DefaultChatTransport({\n\n10      api: '/api/chat',\n\n11    }),\n\n12  });\n\n13  const [input, setInput] = useState('');\n\n14\n\n15  return (\n\n16    <>\n\n17      {messages.map(message => (\n\n18        <div key={message.id}>\n\n19          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n20          {message.parts.map((part, index) =>\n\n21            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n\n22          )}\n\n23        </div>\n\n24      ))}\n\n25\n\n26      <form\n\n27        onSubmit={e => {\n\n28          e.preventDefault();\n\n29          if (input.trim()) {\n\n30            sendMessage({ text: input });\n\n31            setInput('');\n\n32          }\n\n33        }}\n\n34      >\n\n35        <input\n\n36          value={input}\n\n37          onChange={e => setInput(e.target.value)}\n\n38          disabled={status !== 'ready'}\n\n39          placeholder=\"Say something...\"\n\n40        />\n\n41        <button type=\"submit\" disabled={status !== 'ready'}>\n\n42          Submit\n\n43        </button>\n\n44      </form>\n\n45    </>\n\n46  );\n\n47}\n```\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2\n\n3// Allow streaming responses up to 30 seconds\n\n4export const maxDuration = 30;\n\n5\n\n6export async function POST(req: Request) {\n\n7  const { messages }: { messages: UIMessage[] } = await req.json();\n\n8\n\n9  const result = streamText({\n\n10    model: \"anthropic/claude-sonnet-4.5\",\n\n11    system: 'You are a helpful assistant.',\n\n12    messages: await convertToModelMessages(messages),\n\n13  });\n\n14\n\n15  return result.toUIMessageStreamResponse();\n\n16}\n```\n\nThe UI messages have a new `parts` property that contains the message parts.\nWe recommend rendering the messages using the `parts` property instead of the\n`content` property. The parts property supports different message types,\nincluding text, tool invocation, and tool result, and allows for more flexible\nand complex chat UIs.\n\nIn the `Page` component, the `useChat` hook will request to your AI provider endpoint whenever the user sends a message using `sendMessage`.\nThe messages are then streamed back in real-time and displayed in the chat UI.\n\nThis enables a seamless chat experience where the user can see the AI response as soon as it is available,\nwithout having to wait for the entire response to be received.\n\n## [Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#customized-ui)\n\n`useChat` also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.\n\n### [Status](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#status)\n\nThe `useChat` hook returns a `status`. It has the following possible values:\n\n- `submitted`: The message has been sent to the API and we're awaiting the start of the response stream.\n- `streaming`: The response is actively streaming in from the API, receiving chunks of data.\n- `ready`: The full response has been received and processed; a new user message can be submitted.\n- `error`: An error occurred during the API request, preventing successful completion.\n\nYou can use `status` for e.g. the following purposes:\n\n- To show a loading spinner while the chatbot is processing the user's message.\n- To show a \"Stop\" button to abort the current message.\n- To disable the submit button.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport } from 'ai';\n\n5import { useState } from 'react';\n\n6\n\n7export default function Page() {\n\n8  const { messages, sendMessage, status, stop } = useChat({\n\n9    transport: new DefaultChatTransport({\n\n10      api: '/api/chat',\n\n11    }),\n\n12  });\n\n13  const [input, setInput] = useState('');\n\n14\n\n15  return (\n\n16    <>\n\n17      {messages.map(message => (\n\n18        <div key={message.id}>\n\n19          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n20          {message.parts.map((part, index) =>\n\n21            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n\n22          )}\n\n23        </div>\n\n24      ))}\n\n25\n\n26      {(status === 'submitted' || status === 'streaming') && (\n\n27        <div>\n\n28          {status === 'submitted' && <Spinner />}\n\n29          <button type=\"button\" onClick={() => stop()}>\n\n30            Stop\n\n31          </button>\n\n32        </div>\n\n33      )}\n\n34\n\n35      <form\n\n36        onSubmit={e => {\n\n37          e.preventDefault();\n\n38          if (input.trim()) {\n\n39            sendMessage({ text: input });\n\n40            setInput('');\n\n41          }\n\n42        }}\n\n43      >\n\n44        <input\n\n45          value={input}\n\n46          onChange={e => setInput(e.target.value)}\n\n47          disabled={status !== 'ready'}\n\n48          placeholder=\"Say something...\"\n\n49        />\n\n50        <button type=\"submit\" disabled={status !== 'ready'}>\n\n51          Submit\n\n52        </button>\n\n53      </form>\n\n54    </>\n\n55  );\n\n56}\n```\n\n### [Error State](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#error-state)\n\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\nIt can be used to display an error message, disable the submit button, or show a retry button:\n\nWe recommend showing a generic error message to the user, such as \"Something\nwent wrong.\" This is a good practice to avoid leaking information from the\nserver.\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport } from 'ai';\n\n5import { useState } from 'react';\n\n6\n\n7export default function Chat() {\n\n8  const { messages, sendMessage, error, reload } = useChat({\n\n9    transport: new DefaultChatTransport({\n\n10      api: '/api/chat',\n\n11    }),\n\n12  });\n\n13  const [input, setInput] = useState('');\n\n14\n\n15  return (\n\n16    <div>\n\n17      {messages.map(m => (\n\n18        <div key={m.id}>\n\n19          {m.role}:{' '}\n\n20          {m.parts.map((part, index) =>\n\n21            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n\n22          )}\n\n23        </div>\n\n24      ))}\n\n25\n\n26      {error && (\n\n27        <>\n\n28          <div>An error occurred.</div>\n\n29          <button type=\"button\" onClick={() => reload()}>\n\n30            Retry\n\n31          </button>\n\n32        </>\n\n33      )}\n\n34\n\n35      <form\n\n36        onSubmit={e => {\n\n37          e.preventDefault();\n\n38          if (input.trim()) {\n\n39            sendMessage({ text: input });\n\n40            setInput('');\n\n41          }\n\n42        }}\n\n43      >\n\n44        <input\n\n45          value={input}\n\n46          onChange={e => setInput(e.target.value)}\n\n47          disabled={error != null}\n\n48        />\n\n49      </form>\n\n50    </div>\n\n51  );\n\n52}\n```\n\nPlease also see the [error handling](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling) guide for more information.\n\n### [Modify messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#modify-messages)\n\nSometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.\n\nThe `setMessages` function can help you achieve these tasks:\n\n```tsx\n1const { messages, setMessages } = useChat()\n\n2\n\n3const handleDelete = (id) => {\n\n4  setMessages(messages.filter(message => message.id !== id))\n\n5}\n\n6\n\n7return <>\n\n8  {messages.map(message => (\n\n9    <div key={message.id}>\n\n10      {message.role === 'user' ? 'User: ' : 'AI: '}\n\n11      {message.parts.map((part, index) => (\n\n12        part.type === 'text' ? (\n\n13          <span key={index}>{part.text}</span>\n\n14        ) : null\n\n15      ))}\n\n16      <button onClick={() => handleDelete(message.id)}>Delete</button>\n\n17    </div>\n\n18  ))}\n\n19  ...\n```\n\nYou can think of `messages` and `setMessages` as a pair of `state` and `setState` in React.\n\n### [Cancellation and regeneration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#cancellation-and-regeneration)\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useChat` hook.\n\n```tsx\n1const { stop, status } = useChat()\n\n2\n\n3return <>\n\n4  <button onClick={stop} disabled={!(status === 'streaming' || status === 'submitted')}>Stop</button>\n\n5  ...\n```\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.\n\nSimilarly, you can also request the AI provider to reprocess the last message by calling the `regenerate` function returned by the `useChat` hook:\n\n```tsx\n1const { regenerate, status } = useChat();\n\n2\n\n3return (\n\n4  <>\n\n5    <button\n\n6      onClick={regenerate}\n\n7      disabled={!(status === 'ready' || status === 'error')}\n\n8    >\n\n9      Regenerate\n\n10    </button>\n\n11    ...\n\n12  </>\n\n13);\n```\n\nWhen the user clicks the \"Regenerate\" button, the AI provider will regenerate the last message and replace the current one correspondingly.\n\n### [Throttling UI Updates](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#throttling-ui-updates)\n\nThis feature is currently only available for React.\n\nBy default, the `useChat` hook will trigger a render every time a new chunk is received.\nYou can throttle the UI updates with the `experimental_throttle` option.\n\npage.tsx\n\n```tsx\n1const { messages, ... } = useChat({\n\n2  // Throttle the messages and data updates to 50ms:\n\n3  experimental_throttle: 50\n\n4})\n```\n\n## [Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#event-callbacks)\n\n`useChat` provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:\n\n- `onFinish`: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.\n- `onError`: Called when an error occurs during the fetch request.\n- `onData`: Called whenever a data part is received.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\n```tsx\n1import { UIMessage } from 'ai';\n\n2\n\n3const {\n\n4  /* ... */\n\n5} = useChat({\n\n6  onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {\n\n7    // use information to e.g. update other UI states\n\n8  },\n\n9  onError: error => {\n\n10    console.error('An error occurred:', error);\n\n11  },\n\n12  onData: data => {\n\n13    console.log('Received data part from server:', data);\n\n14  },\n\n15});\n```\n\nIt's worth noting that you can abort the processing by throwing an error in the `onData` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\n## [Request Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#request-configuration)\n\n### [Custom headers, body, and credentials](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#custom-headers-body-and-credentials)\n\nBy default, the `useChat` hook sends a HTTP POST request to the `/api/chat` endpoint with the message list as the request body. You can customize the request in two ways:\n\n#### [Hook-Level Configuration (Applied to all requests)](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#hook-level-configuration-applied-to-all-requests)\n\nYou can configure transport-level options that will be applied to all requests made by the hook:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4const { messages, sendMessage } = useChat({\n\n5  transport: new DefaultChatTransport({\n\n6    api: '/api/custom-chat',\n\n7    headers: {\n\n8      Authorization: 'your_token',\n\n9    },\n\n10    body: {\n\n11      user_id: '123',\n\n12    },\n\n13    credentials: 'same-origin',\n\n14  }),\n\n15});\n```\n\n#### [Dynamic Hook-Level Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#dynamic-hook-level-configuration)\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4const { messages, sendMessage } = useChat({\n\n5  transport: new DefaultChatTransport({\n\n6    api: '/api/custom-chat',\n\n7    headers: () => ({\n\n8      Authorization: `Bearer ${getAuthToken()}`,\n\n9      'X-User-ID': getCurrentUserId(),\n\n10    }),\n\n11    body: () => ({\n\n12      sessionId: getCurrentSessionId(),\n\n13      preferences: getUserPreferences(),\n\n14    }),\n\n15    credentials: () => 'include',\n\n16  }),\n\n17});\n```\n\nFor component state that changes over time, use `useRef` to store the current\nvalue and reference `ref.current` in your configuration function, or prefer\nrequest-level options (see next section) for better reliability.\n\n#### [Request-Level Configuration (Recommended)](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#request-level-configuration-recommended)\n\n**Recommended**: Use request-level options for better flexibility and control.\nRequest-level options take precedence over hook-level options and allow you to\ncustomize each request individually.\n\n```tsx\n1// Pass options as the second parameter to sendMessage\n\n2sendMessage(\n\n3  { text: input },\n\n4  {\n\n5    headers: {\n\n6      Authorization: 'Bearer token123',\n\n7      'X-Custom-Header': 'custom-value',\n\n8    },\n\n9    body: {\n\n10      temperature: 0.7,\n\n11      max_tokens: 100,\n\n12      user_id: '123',\n\n13    },\n\n14    metadata: {\n\n15      userId: 'user123',\n\n16      sessionId: 'session456',\n\n17    },\n\n18  },\n\n19);\n```\n\nThe request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.\n\n### [Setting custom body fields per request](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#setting-custom-body-fields-per-request)\n\nYou can configure custom `body` fields on a per-request basis using the second parameter of the `sendMessage` function.\nThis is useful if you want to pass in additional information to your backend that is not part of the message list.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const { messages, sendMessage } = useChat();\n\n8  const [input, setInput] = useState('');\n\n9\n\n10  return (\n\n11    <div>\n\n12      {messages.map(m => (\n\n13        <div key={m.id}>\n\n14          {m.role}:{' '}\n\n15          {m.parts.map((part, index) =>\n\n16            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n\n17          )}\n\n18        </div>\n\n19      ))}\n\n20\n\n21      <form\n\n22        onSubmit={event => {\n\n23          event.preventDefault();\n\n24          if (input.trim()) {\n\n25            sendMessage(\n\n26              { text: input },\n\n27              {\n\n28                body: {\n\n29                  customKey: 'customValue',\n\n30                },\n\n31              },\n\n32            );\n\n33            setInput('');\n\n34          }\n\n35        }}\n\n36      >\n\n37        <input value={input} onChange={e => setInput(e.target.value)} />\n\n38      </form>\n\n39    </div>\n\n40  );\n\n41}\n```\n\nYou can retrieve these custom fields on your server side by destructuring the request body:\n\napp/api/chat/route.ts\n\n```ts\n1export async function POST(req: Request) {\n\n2  // Extract additional information (\"customKey\") from the body of the request:\n\n3  const { messages, customKey }: { messages: UIMessage[]; customKey: string } =\n\n4    await req.json();\n\n5  //...\n\n6}\n```\n\n## [Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#message-metadata)\n\nYou can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.\n\n```ts\n1// Server: Send metadata about the message\n\n2return result.toUIMessageStreamResponse({\n\n3  messageMetadata: ({ part }) => {\n\n4    if (part.type === 'start') {\n\n5      return {\n\n6        createdAt: Date.now(),\n\n7        model: 'gpt-5.1',\n\n8      };\n\n9    }\n\n10\n\n11    if (part.type === 'finish') {\n\n12      return {\n\n13        totalTokens: part.totalUsage.totalTokens,\n\n14      };\n\n15    }\n\n16  },\n\n17});\n```\n\n```tsx\n1// Client: Access metadata via message.metadata\n\n2{\n\n3  messages.map(message => (\n\n4    <div key={message.id}>\n\n5      {message.role}:{' '}\n\n6      {message.metadata?.createdAt &&\n\n7        new Date(message.metadata.createdAt).toLocaleTimeString()}\n\n8      {/* Render message content */}\n\n9      {message.parts.map((part, index) =>\n\n10        part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n\n11      )}\n\n12      {/* Show token count if available */}\n\n13      {message.metadata?.totalTokens && (\n\n14        <span>{message.metadata.totalTokens} tokens</span>\n\n15      )}\n\n16    </div>\n\n17  ));\n\n18}\n```\n\nFor complete examples with type safety and advanced use cases, see the [Message Metadata documentation](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata).\n\n## [Transport Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#transport-configuration)\n\nYou can configure custom transport behavior using the `transport` option to customize how messages are sent to your API:\n\napp/page.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4export default function Chat() {\n\n5  const { messages, sendMessage } = useChat({\n\n6    id: 'my-chat',\n\n7    transport: new DefaultChatTransport({\n\n8      prepareSendMessagesRequest: ({ id, messages }) => {\n\n9        return {\n\n10          body: {\n\n11            id,\n\n12            message: messages[messages.length - 1],\n\n13          },\n\n14        };\n\n15      },\n\n16    }),\n\n17  });\n\n18\n\n19  // ... rest of your component\n\n20}\n```\n\nThe corresponding API route receives the custom request format:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1export async function POST(req: Request) {\n\n2  const { id, message } = await req.json();\n\n3\n\n4  // Load existing messages and add the new one\n\n5  const messages = await loadMessages(id);\n\n6  messages.push(message);\n\n7\n\n8  const result = streamText({\n\n9    model: \"anthropic/claude-sonnet-4.5\",\n\n10    messages: await convertToModelMessages(messages),\n\n11  });\n\n12\n\n13  return result.toUIMessageStreamResponse();\n\n14}\n```\n\n### [Advanced: Trigger-based routing](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#advanced-trigger-based-routing)\n\nFor more complex scenarios like message regeneration, you can use trigger-based routing:\n\napp/page.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4export default function Chat() {\n\n5  const { messages, sendMessage, regenerate } = useChat({\n\n6    id: 'my-chat',\n\n7    transport: new DefaultChatTransport({\n\n8      prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n\n9        if (trigger === 'submit-user-message') {\n\n10          return {\n\n11            body: {\n\n12              trigger: 'submit-user-message',\n\n13              id,\n\n14              message: messages[messages.length - 1],\n\n15              messageId,\n\n16            },\n\n17          };\n\n18        } else if (trigger === 'regenerate-assistant-message') {\n\n19          return {\n\n20            body: {\n\n21              trigger: 'regenerate-assistant-message',\n\n22              id,\n\n23              messageId,\n\n24            },\n\n25          };\n\n26        }\n\n27        throw new Error(`Unsupported trigger: ${trigger}`);\n\n28      },\n\n29    }),\n\n30  });\n\n31\n\n32  // ... rest of your component\n\n33}\n```\n\nThe corresponding API route would handle different triggers:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1export async function POST(req: Request) {\n\n2  const { trigger, id, message, messageId } = await req.json();\n\n3\n\n4  const chat = await readChat(id);\n\n5  let messages = chat.messages;\n\n6\n\n7  if (trigger === 'submit-user-message') {\n\n8    // Handle new user message\n\n9    messages = [...messages, message];\n\n10  } else if (trigger === 'regenerate-assistant-message') {\n\n11    // Handle message regeneration - remove messages after messageId\n\n12    const messageIndex = messages.findIndex(m => m.id === messageId);\n\n13    if (messageIndex !== -1) {\n\n14      messages = messages.slice(0, messageIndex);\n\n15    }\n\n16  }\n\n17\n\n18  const result = streamText({\n\n19    model: \"anthropic/claude-sonnet-4.5\",\n\n20    messages: await convertToModelMessages(messages),\n\n21  });\n\n22\n\n23  return result.toUIMessageStreamResponse();\n\n24}\n```\n\nTo learn more about building custom transports, refer to the [Transport API documentation](https://ai-sdk.dev/docs/ai-sdk-ui/transport).\n\n## [Controlling the response stream](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#controlling-the-response-stream)\n\nWith `streamText`, you can control how error messages and usage information are sent back to the client.\n\n### [Error Messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#error-messages)\n\nBy default, the error message is masked for security reasons.\nThe default error message is \"An error occurred.\"\nYou can forward error messages or send your own error message by providing a `getErrorMessage` function:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse({\n\n12    onError: error => {\n\n13      if (error == null) {\n\n14        return 'unknown error';\n\n15      }\n\n16\n\n17      if (typeof error === 'string') {\n\n18        return error;\n\n19      }\n\n20\n\n21      if (error instanceof Error) {\n\n22        return error.message;\n\n23      }\n\n24\n\n25      return JSON.stringify(error);\n\n26    },\n\n27  });\n\n28}\n```\n\n### [Usage Information](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#usage-information)\n\nTrack token consumption and resource usage with [message metadata](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata):\n\n1. Define a custom metadata type with usage fields (optional, for type safety)\n2. Attach usage data using `messageMetadata` in your response\n3. Display usage metrics in your UI components\n\nUsage data is attached as metadata to messages and becomes available once the model completes its response generation.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import {\n\n3  convertToModelMessages,\n\n4  streamText,\n\n5  UIMessage,\n\n6  type LanguageModelUsage,\n\n7} from 'ai';\n\n8\n\n9// Create a new metadata type (optional for type-safety)\n\n10type MyMetadata = {\n\n11  totalUsage: LanguageModelUsage;\n\n12};\n\n13\n\n14// Create a new custom message type with your own metadata\n\n15export type MyUIMessage = UIMessage<MyMetadata>;\n\n16\n\n17export async function POST(req: Request) {\n\n18  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n19\n\n20  const result = streamText({\n\n21    model: \"anthropic/claude-sonnet-4.5\",\n\n22    messages: await convertToModelMessages(messages),\n\n23  });\n\n24\n\n25  return result.toUIMessageStreamResponse({\n\n26    originalMessages: messages,\n\n27    messageMetadata: ({ part }) => {\n\n28      // Send total usage when generation is finished\n\n29      if (part.type === 'finish') {\n\n30        return { totalUsage: part.totalUsage };\n\n31      }\n\n32    },\n\n33  });\n\n34}\n```\n\nThen, on the client, you can access the message-level metadata.\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import type { MyUIMessage } from './api/chat/route';\n\n5import { DefaultChatTransport } from 'ai';\n\n6\n\n7export default function Chat() {\n\n8  // Use custom message type defined on the server (optional for type-safety)\n\n9  const { messages } = useChat<MyUIMessage>({\n\n10    transport: new DefaultChatTransport({\n\n11      api: '/api/chat',\n\n12    }),\n\n13  });\n\n14\n\n15  return (\n\n16    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n17      {messages.map(m => (\n\n18        <div key={m.id} className=\"whitespace-pre-wrap\">\n\n19          {m.role === 'user' ? 'User: ' : 'AI: '}\n\n20          {m.parts.map(part => {\n\n21            if (part.type === 'text') {\n\n22              return part.text;\n\n23            }\n\n24          })}\n\n25          {/* Render usage via metadata */}\n\n26          {m.metadata?.totalUsage && (\n\n27            <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>\n\n28          )}\n\n29        </div>\n\n30      ))}\n\n31    </div>\n\n32  );\n\n33}\n```\n\nYou can also access your metadata from the `onFinish` callback of `useChat`:\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import type { MyUIMessage } from './api/chat/route';\n\n5import { DefaultChatTransport } from 'ai';\n\n6\n\n7export default function Chat() {\n\n8  // Use custom message type defined on the server (optional for type-safety)\n\n9  const { messages } = useChat<MyUIMessage>({\n\n10    transport: new DefaultChatTransport({\n\n11      api: '/api/chat',\n\n12    }),\n\n13    onFinish: ({ message }) => {\n\n14      // Access message metadata via onFinish callback\n\n15      console.log(message.metadata?.totalUsage);\n\n16    },\n\n17  });\n\n18}\n```\n\n### [Text Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#text-streams)\n\n`useChat` can handle plain text streams by setting the `streamProtocol` option to `text`:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { TextStreamChatTransport } from 'ai';\n\n5\n\n6export default function Chat() {\n\n7  const { messages } = useChat({\n\n8    transport: new TextStreamChatTransport({\n\n9      api: '/api/chat',\n\n10    }),\n\n11  });\n\n12\n\n13  return <>...</>;\n\n14}\n```\n\nThis configuration also works with other backend servers that stream plain text.\nCheck out the [stream protocol guide](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol) for more information.\n\nWhen using `TextStreamChatTransport`, tool calls, usage information and finish\nreasons are not available.\n\n## [Reasoning](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#reasoning)\n\nSome models such as as DeepSeek `deepseek-r1`\nand Anthropic `claude-3-7-sonnet-20250219` support reasoning tokens.\nThese tokens are typically sent before the message content.\nYou can forward them to the client with the `sendReasoning` option:\n\napp/api/chat/route.ts\n\n```ts\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: 'deepseek/deepseek-r1',\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse({\n\n12    sendReasoning: true,\n\n13  });\n\n14}\n```\n\nOn the client side, you can access the reasoning parts of the message object.\n\nReasoning parts have a `text` property that contains the reasoning content.\n\napp/page.tsx\n\n```tsx\n1messages.map(message => (\n\n2  <div key={message.id}>\n\n3    {message.role === 'user' ? 'User: ' : 'AI: '}\n\n4    {message.parts.map((part, index) => {\n\n5      // text parts:\n\n6      if (part.type === 'text') {\n\n7        return <div key={index}>{part.text}</div>;\n\n8      }\n\n9\n\n10      // reasoning parts:\n\n11      if (part.type === 'reasoning') {\n\n12        return <pre key={index}>{part.text}</pre>;\n\n13      }\n\n14    })}\n\n15  </div>\n\n16));\n```\n\n## [Sources](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#sources)\n\nSome providers such as [Perplexity](https://ai-sdk.dev/providers/ai-sdk-providers/perplexity#sources) and\n[Google Generative AI](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\n\nCurrently sources are limited to web pages that ground the response.\nYou can forward them to the client with the `sendSources` option:\n\napp/api/chat/route.ts\n\n```ts\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: 'perplexity/sonar-pro',\n\n8    messages: await convertToModelMessages(messages),\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse({\n\n12    sendSources: true,\n\n13  });\n\n14}\n```\n\nOn the client side, you can access source parts of the message object.\nThere are two types of sources: `source-url` for web pages and `source-document` for documents.\nHere is an example that renders both types of sources:\n\napp/page.tsx\n\n```tsx\n1messages.map(message => (\n\n2  <div key={message.id}>\n\n3    {message.role === 'user' ? 'User: ' : 'AI: '}\n\n4\n\n5    {/* Render URL sources */}\n\n6    {message.parts\n\n7      .filter(part => part.type === 'source-url')\n\n8      .map(part => (\n\n9        <span key={`source-${part.id}`}>\n\n10          [\\\n\\\n11          <a href={part.url} target=\"_blank\">\\\n\\\n12            {part.title ?? new URL(part.url).hostname}\\\n\\\n13          </a>\\\n\\\n14          ]\n\n15        </span>\n\n16      ))}\n\n17\n\n18    {/* Render document sources */}\n\n19    {message.parts\n\n20      .filter(part => part.type === 'source-document')\n\n21      .map(part => (\n\n22        <span key={`source-${part.id}`}>\n\n23          [<span>{part.title ?? `Document ${part.id}`}</span>]\n\n24        </span>\n\n25      ))}\n\n26  </div>\n\n27));\n```\n\n## [Image Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#image-generation)\n\nSome models such as Google `gemini-2.5-flash-image-preview` support image generation.\nWhen images are generated, they are exposed as files to the client.\nOn the client side, you can access file parts of the message object\nand render them as images.\n\napp/page.tsx\n\n```tsx\n1messages.map(message => (\n\n2  <div key={message.id}>\n\n3    {message.role === 'user' ? 'User: ' : 'AI: '}\n\n4    {message.parts.map((part, index) => {\n\n5      if (part.type === 'text') {\n\n6        return <div key={index}>{part.text}</div>;\n\n7      } else if (part.type === 'file' && part.mediaType.startsWith('image/')) {\n\n8        return <img key={index} src={part.url} alt=\"Generated image\" />;\n\n9      }\n\n10    })}\n\n11  </div>\n\n12));\n```\n\n## [Attachments](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#attachments)\n\nThe `useChat` hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.\n\nThere are two ways to send files with a message: using a `FileList` object from file inputs or using an array of file objects.\n\n### [FileList](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#filelist)\n\nBy using `FileList`, you can send multiple files as attachments along with a message using the file input element. The `useChat` hook will automatically convert them into data URLs and send them to the AI provider.\n\nCurrently, only `image/*` and `text/*` content types get automatically\nconverted into [multi-modal content\\\\\nparts](https://ai-sdk.dev/docs/foundations/prompts#multi-modal-messages). You will need to\nhandle other content types manually.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useRef, useState } from 'react';\n\n5\n\n6export default function Page() {\n\n7  const { messages, sendMessage, status } = useChat();\n\n8\n\n9  const [input, setInput] = useState('');\n\n10  const [files, setFiles] = useState<FileList | undefined>(undefined);\n\n11  const fileInputRef = useRef<HTMLInputElement>(null);\n\n12\n\n13  return (\n\n14    <div>\n\n15      <div>\n\n16        {messages.map(message => (\n\n17          <div key={message.id}>\n\n18            <div>{`${message.role}: `}</div>\n\n19\n\n20            <div>\n\n21              {message.parts.map((part, index) => {\n\n22                if (part.type === 'text') {\n\n23                  return <span key={index}>{part.text}</span>;\n\n24                }\n\n25\n\n26                if (\n\n27                  part.type === 'file' &&\n\n28                  part.mediaType?.startsWith('image/')\n\n29                ) {\n\n30                  return <img key={index} src={part.url} alt={part.filename} />;\n\n31                }\n\n32\n\n33                return null;\n\n34              })}\n\n35            </div>\n\n36          </div>\n\n37        ))}\n\n38      </div>\n\n39\n\n40      <form\n\n41        onSubmit={event => {\n\n42          event.preventDefault();\n\n43          if (input.trim()) {\n\n44            sendMessage({\n\n45              text: input,\n\n46              files,\n\n47            });\n\n48            setInput('');\n\n49            setFiles(undefined);\n\n50\n\n51            if (fileInputRef.current) {\n\n52              fileInputRef.current.value = '';\n\n53            }\n\n54          }\n\n55        }}\n\n56      >\n\n57        <input\n\n58          type=\"file\"\n\n59          onChange={event => {\n\n60            if (event.target.files) {\n\n61              setFiles(event.target.files);\n\n62            }\n\n63          }}\n\n64          multiple\n\n65          ref={fileInputRef}\n\n66        />\n\n67        <input\n\n68          value={input}\n\n69          placeholder=\"Send message...\"\n\n70          onChange={e => setInput(e.target.value)}\n\n71          disabled={status !== 'ready'}\n\n72        />\n\n73      </form>\n\n74    </div>\n\n75  );\n\n76}\n```\n\n### [File Objects](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#file-objects)\n\nYou can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5import { FileUIPart } from 'ai';\n\n6\n\n7export default function Page() {\n\n8  const { messages, sendMessage, status } = useChat();\n\n9\n\n10  const [input, setInput] = useState('');\n\n11  const [files] = useState<FileUIPart[]>([\\\n\\\n12    {\\\n\\\n13      type: 'file',\\\n\\\n14      filename: 'earth.png',\\\n\\\n15      mediaType: 'image/png',\\\n\\\n16      url: 'https://example.com/earth.png',\\\n\\\n17    },\\\n\\\n18    {\\\n\\\n19      type: 'file',\\\n\\\n20      filename: 'moon.png',\\\n\\\n21      mediaType: 'image/png',\\\n\\\n22      url: 'data:image/png;base64,iVBORw0KGgo...',\\\n\\\n23    },\\\n\\\n24  ]);\n\n25\n\n26  return (\n\n27    <div>\n\n28      <div>\n\n29        {messages.map(message => (\n\n30          <div key={message.id}>\n\n31            <div>{`${message.role}: `}</div>\n\n32\n\n33            <div>\n\n34              {message.parts.map((part, index) => {\n\n35                if (part.type === 'text') {\n\n36                  return <span key={index}>{part.text}</span>;\n\n37                }\n\n38\n\n39                if (\n\n40                  part.type === 'file' &&\n\n41                  part.mediaType?.startsWith('image/')\n\n42                ) {\n\n43                  return <img key={index} src={part.url} alt={part.filename} />;\n\n44                }\n\n45\n\n46                return null;\n\n47              })}\n\n48            </div>\n\n49          </div>\n\n50        ))}\n\n51      </div>\n\n52\n\n53      <form\n\n54        onSubmit={event => {\n\n55          event.preventDefault();\n\n56          if (input.trim()) {\n\n57            sendMessage({\n\n58              text: input,\n\n59              files,\n\n60            });\n\n61            setInput('');\n\n62          }\n\n63        }}\n\n64      >\n\n65        <input\n\n66          value={input}\n\n67          placeholder=\"Send message...\"\n\n68          onChange={e => setInput(e.target.value)}\n\n69          disabled={status !== 'ready'}\n\n70        />\n\n71      </form>\n\n72    </div>\n\n73  );\n\n74}\n```\n\n## [Type Inference for Tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#type-inference-for-tools)\n\nWhen working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.\n\n### [InferUITool](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#inferuitool)\n\nThe `InferUITool` type helper infers the input and output types of a single tool for use in UI messages:\n\n```tsx\n1import { InferUITool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const weatherTool = {\n\n5  description: 'Get the current weather',\n\n6  inputSchema: z.object({\n\n7    location: z.string().describe('The city and state'),\n\n8  }),\n\n9  execute: async ({ location }) => {\n\n10    return `The weather in ${location} is sunny.`;\n\n11  },\n\n12};\n\n13\n\n14// Infer the types from the tool\n\n15type WeatherUITool = InferUITool<typeof weatherTool>;\n\n16// This creates a type with:\n\n17// {\n\n18//   input: { location: string };\n\n19//   output: string;\n\n20// }\n```\n\n### [InferUITools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#inferuitools)\n\nThe `InferUITools` type helper infers the input and output types of a `ToolSet`:\n\n```tsx\n1import { InferUITools, ToolSet } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4const tools = {\n\n5  weather: {\n\n6    description: 'Get the current weather',\n\n7    inputSchema: z.object({\n\n8      location: z.string().describe('The city and state'),\n\n9    }),\n\n10    execute: async ({ location }) => {\n\n11      return `The weather in ${location} is sunny.`;\n\n12    },\n\n13  },\n\n14  calculator: {\n\n15    description: 'Perform basic arithmetic',\n\n16    inputSchema: z.object({\n\n17      operation: z.enum(['add', 'subtract', 'multiply', 'divide']),\n\n18      a: z.number(),\n\n19      b: z.number(),\n\n20    }),\n\n21    execute: async ({ operation, a, b }) => {\n\n22      switch (operation) {\n\n23        case 'add':\n\n24          return a + b;\n\n25        case 'subtract':\n\n26          return a - b;\n\n27        case 'multiply':\n\n28          return a * b;\n\n29        case 'divide':\n\n30          return a / b;\n\n31      }\n\n32    },\n\n33  },\n\n34} satisfies ToolSet;\n\n35\n\n36// Infer the types from the tool set\n\n37type MyUITools = InferUITools<typeof tools>;\n\n38// This creates a type with:\n\n39// {\n\n40//   weather: { input: { location: string }; output: string };\n\n41//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };\n\n42// }\n```\n\n### [Using Inferred Types](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot\\#using-inferred-types)\n\nYou can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:\n\n```tsx\n1import { InferUITools, UIMessage, UIDataTypes } from 'ai';\n\n2\n\n3type MyUITools = InferUITools<typeof tools>;\n\n4type MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;\n```\n\nPass the custom type to `useChat` or `createUIMessageStream`:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { createUIMessageStream } from 'ai';\n\n3import type { MyUIMessage } from './types';\n\n4\n\n5// With useChat\n\n6const { messages } = useChat<MyUIMessage>();\n\n7\n\n8// With createUIMessageStream\n\n9const stream = createUIMessageStream<MyUIMessage>(/* ... */);\n```\n\nThis provides full type safety for tool inputs and outputs on the client and server.\n\nOn this page\n\n[Chatbot](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#chatbot)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#example)\n\n[Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#customized-ui)\n\n[Status](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#status)\n\n[Error State](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#error-state)\n\n[Modify messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#modify-messages)\n\n[Cancellation and regeneration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#cancellation-and-regeneration)\n\n[Throttling UI Updates](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#throttling-ui-updates)\n\n[Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#event-callbacks)\n\n[Request Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#request-configuration)\n\n[Custom headers, body, and credentials](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#custom-headers-body-and-credentials)\n\n[Hook-Level Configuration (Applied to all requests)](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#hook-level-configuration-applied-to-all-requests)\n\n[Dynamic Hook-Level Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#dynamic-hook-level-configuration)\n\n[Request-Level Configuration (Recommended)](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#request-level-configuration-recommended)\n\n[Setting custom body fields per request](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#setting-custom-body-fields-per-request)\n\n[Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#message-metadata)\n\n[Transport Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#transport-configuration)\n\n[Advanced: Trigger-based routing](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#advanced-trigger-based-routing)\n\n[Controlling the response stream](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#controlling-the-response-stream)\n\n[Error Messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#error-messages)\n\n[Usage Information](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#usage-information)\n\n[Text Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#text-streams)\n\n[Reasoning](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#reasoning)\n\n[Sources](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#sources)\n\n[Image Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#image-generation)\n\n[Attachments](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#attachments)\n\n[FileList](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#filelist)\n\n[File Objects](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#file-objects)\n\n[Type Inference for Tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#type-inference-for-tools)\n\n[InferUITool](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#inferuitool)\n\n[InferUITools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#inferuitools)\n\n[Using Inferred Types](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#using-inferred-types)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot"
      },
      {
        "title": "Chatbot Message Persistence",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
        "content": "# [Chatbot Message Persistence](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#chatbot-message-persistence)\n\nBeing able to store and load chat messages is crucial for most AI chatbots.\nIn this guide, we'll show how to implement message persistence with `useChat` and `streamText`.\n\nThis guide does not cover authorization, error handling, or other real-world\nconsiderations. It is intended to be a simple example of how to implement\nmessage persistence.\n\n## [Starting a new chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#starting-a-new-chat)\n\nWhen the user navigates to the chat page without providing a chat ID,\nwe need to create a new chat and redirect to the chat page with the new chat ID.\n\napp/chat/page.tsx\n\n```tsx\n1import { redirect } from 'next/navigation';\n\n2import { createChat } from '@util/chat-store';\n\n3\n\n4export default async function Page() {\n\n5  const id = await createChat(); // create a new chat\n\n6  redirect(`/chat/${id}`); // redirect to chat page, see below\n\n7}\n```\n\nOur example chat store implementation uses files to store the chat messages.\nIn a real-world application, you would use a database or a cloud storage service,\nand get the chat ID from the database.\nThat being said, the function interfaces are designed to be easily replaced with other implementations.\n\nutil/chat-store.ts\n\n```tsx\n1import { generateId } from 'ai';\n\n2import { existsSync, mkdirSync } from 'fs';\n\n3import { writeFile } from 'fs/promises';\n\n4import path from 'path';\n\n5\n\n6export async function createChat(): Promise<string> {\n\n7  const id = generateId(); // generate a unique chat ID\n\n8  await writeFile(getChatFile(id), '[]'); // create an empty chat file\n\n9  return id;\n\n10}\n\n11\n\n12function getChatFile(id: string): string {\n\n13  const chatDir = path.join(process.cwd(), '.chats');\n\n14  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\n\n15  return path.join(chatDir, `${id}.json`);\n\n16}\n```\n\n## [Loading an existing chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#loading-an-existing-chat)\n\nWhen the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.\n\nThe `loadChat` function in our file-based chat store is implemented as follows:\n\nutil/chat-store.ts\n\n```tsx\n1import { UIMessage } from 'ai';\n\n2import { readFile } from 'fs/promises';\n\n3\n\n4export async function loadChat(id: string): Promise<UIMessage[]> {\n\n5  return JSON.parse(await readFile(getChatFile(id), 'utf8'));\n\n6}\n\n7\n\n8// ... rest of the file\n```\n\n## [Validating messages on the server](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#validating-messages-on-the-server)\n\nWhen processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using `validateUIMessages` before sending them to the model.\n\n### [Validation with tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#validation-with-tools)\n\nWhen your messages include tool calls, validate them against your tool definitions:\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  convertToModelMessages,\n\n3  streamText,\n\n4  UIMessage,\n\n5  validateUIMessages,\n\n6  tool,\n\n7} from 'ai';\n\n8import { z } from 'zod';\n\n9import { loadChat, saveChat } from '@util/chat-store';\n\n10import { openai } from '@ai-sdk/openai';\n\n11import { dataPartsSchema, metadataSchema } from '@util/schemas';\n\n12\n\n13// Define your tools\n\n14const tools = {\n\n15  weather: tool({\n\n16    description: 'Get weather information',\n\n17    parameters: z.object({\n\n18      location: z.string(),\n\n19      units: z.enum(['celsius', 'fahrenheit']),\n\n20    }),\n\n21    execute: async ({ location, units }) => {\n\n22      /* tool implementation */\n\n23    },\n\n24  }),\n\n25  // other tools\n\n26};\n\n27\n\n28export async function POST(req: Request) {\n\n29  const { message, id } = await req.json();\n\n30\n\n31  // Load previous messages from database\n\n32  const previousMessages = await loadChat(id);\n\n33\n\n34  // Append new message to previousMessages messages\n\n35  const messages = [...previousMessages, message];\n\n36\n\n37  // Validate loaded messages against\n\n38  // tools, data parts schema, and metadata schema\n\n39  const validatedMessages = await validateUIMessages({\n\n40    messages,\n\n41    tools, // Ensures tool calls in messages match current schemas\n\n42    dataPartsSchema,\n\n43    metadataSchema,\n\n44  });\n\n45\n\n46  const result = streamText({\n\n47    model: 'openai/gpt-5-mini',\n\n48    messages: convertToModelMessages(validatedMessages),\n\n49    tools,\n\n50  });\n\n51\n\n52  return result.toUIMessageStreamResponse({\n\n53    originalMessages: messages,\n\n54    onFinish: ({ messages }) => {\n\n55      saveChat({ chatId: id, messages });\n\n56    },\n\n57  });\n\n58}\n```\n\n### [Handling validation errors](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#handling-validation-errors)\n\nHandle validation errors gracefully when messages from the database don't match current schemas:\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  convertToModelMessages,\n\n3  streamText,\n\n4  validateUIMessages,\n\n5  TypeValidationError,\n\n6} from 'ai';\n\n7import { type MyUIMessage } from '@/types';\n\n8\n\n9export async function POST(req: Request) {\n\n10  const { message, id } = await req.json();\n\n11\n\n12  // Load and validate messages from database\n\n13  let validatedMessages: MyUIMessage[];\n\n14\n\n15  try {\n\n16    const previousMessages = await loadMessagesFromDB(id);\n\n17    validatedMessages = await validateUIMessages({\n\n18      // append the new message to the previous messages:\n\n19      messages: [...previousMessages, message],\n\n20      tools,\n\n21      metadataSchema,\n\n22    });\n\n23  } catch (error) {\n\n24    if (error instanceof TypeValidationError) {\n\n25      // Log validation error for monitoring\n\n26      console.error('Database messages validation failed:', error);\n\n27      // Could implement message migration or filtering here\n\n28      // For now, start with empty history\n\n29      validatedMessages = [];\n\n30    } else {\n\n31      throw error;\n\n32    }\n\n33  }\n\n34\n\n35  // Continue with validated messages...\n\n36}\n```\n\n## [Displaying the chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#displaying-the-chat)\n\nOnce messages are loaded from storage, you can display them in your chat UI. Here's how to set up the page component and the chat display:\n\napp/chat/\\[id\\]/page.tsx\n\n```tsx\n1import { loadChat } from '@util/chat-store';\n\n2import Chat from '@ui/chat';\n\n3\n\n4export default async function Page(props: { params: Promise<{ id: string }> }) {\n\n5  const { id } = await props.params;\n\n6  const messages = await loadChat(id);\n\n7  return <Chat id={id} initialMessages={messages} />;\n\n8}\n```\n\nThe chat component uses the `useChat` hook to manage the conversation:\n\nui/chat.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { UIMessage, useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport } from 'ai';\n\n5import { useState } from 'react';\n\n6\n\n7export default function Chat({\n\n8  id,\n\n9  initialMessages,\n\n10}: { id?: string | undefined; initialMessages?: UIMessage[] } = {}) {\n\n11  const [input, setInput] = useState('');\n\n12  const { sendMessage, messages } = useChat({\n\n13    id, // use the provided chat ID\n\n14    messages: initialMessages, // load initial messages\n\n15    transport: new DefaultChatTransport({\n\n16      api: '/api/chat',\n\n17    }),\n\n18  });\n\n19\n\n20  const handleSubmit = (e: React.FormEvent) => {\n\n21    e.preventDefault();\n\n22    if (input.trim()) {\n\n23      sendMessage({ text: input });\n\n24      setInput('');\n\n25    }\n\n26  };\n\n27\n\n28  // simplified rendering code, extend as needed:\n\n29  return (\n\n30    <div>\n\n31      {messages.map(m => (\n\n32        <div key={m.id}>\n\n33          {m.role === 'user' ? 'User: ' : 'AI: '}\n\n34          {m.parts\n\n35            .map(part => (part.type === 'text' ? part.text : ''))\n\n36            .join('')}\n\n37        </div>\n\n38      ))}\n\n39\n\n40      <form onSubmit={handleSubmit}>\n\n41        <input\n\n42          value={input}\n\n43          onChange={e => setInput(e.target.value)}\n\n44          placeholder=\"Type a message...\"\n\n45        />\n\n46        <button type=\"submit\">Send</button>\n\n47      </form>\n\n48    </div>\n\n49  );\n\n50}\n```\n\n## [Storing messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#storing-messages)\n\n`useChat` sends the chat id and the messages to the backend.\n\nThe `useChat` message format is different from the `ModelMessage` format. The\n`useChat` message format is designed for frontend display, and contains\nadditional fields such as `id` and `createdAt`. We recommend storing the\nmessages in the `useChat` message format.\n\nWhen loading messages from storage that contain tools, metadata, or custom data\nparts, validate them using `validateUIMessages` before processing (see the\n[validation section](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#validating-messages-from-database) above).\n\nStoring messages is done in the `onFinish` callback of the `toUIMessageStreamResponse` function.\n`onFinish` receives the complete messages including the new AI response as `UIMessage[]`.\n\napp/api/chat/route.ts\n\n```tsx\n1import { openai } from '@ai-sdk/openai';\n\n2import { saveChat } from '@util/chat-store';\n\n3import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n4\n\n5export async function POST(req: Request) {\n\n6  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n\n7    await req.json();\n\n8\n\n9  const result = streamText({\n\n10    model: 'openai/gpt-5-mini',\n\n11    messages: await convertToModelMessages(messages),\n\n12  });\n\n13\n\n14  return result.toUIMessageStreamResponse({\n\n15    originalMessages: messages,\n\n16    onFinish: ({ messages }) => {\n\n17      saveChat({ chatId, messages });\n\n18    },\n\n19  });\n\n20}\n```\n\nThe actual storage of the messages is done in the `saveChat` function, which in\nour file-based chat store is implemented as follows:\n\nutil/chat-store.ts\n\n```tsx\n1import { UIMessage } from 'ai';\n\n2import { writeFile } from 'fs/promises';\n\n3\n\n4export async function saveChat({\n\n5  chatId,\n\n6  messages,\n\n7}: {\n\n8  chatId: string;\n\n9  messages: UIMessage[];\n\n10}): Promise<void> {\n\n11  const content = JSON.stringify(messages, null, 2);\n\n12  await writeFile(getChatFile(chatId), content);\n\n13}\n\n14\n\n15// ... rest of the file\n```\n\n## [Message IDs](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#message-ids)\n\nIn addition to a chat ID, each message has an ID.\nYou can use this message ID to e.g. manipulate individual messages.\n\n### [Client-side vs Server-side ID Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#client-side-vs-server-side-id-generation)\n\nBy default, message IDs are generated client-side:\n\n- User message IDs are generated by the `useChat` hook on the client\n- AI response message IDs are generated by `streamText` on the server\n\nFor applications without persistence, client-side ID generation works perfectly.\nHowever, **for persistence, you need server-side generated IDs** to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.\n\n### [Setting Up Server-side ID Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#setting-up-server-side-id-generation)\n\nWhen implementing persistence, you have two options for generating server-side IDs:\n\n1. **Using `generateMessageId` in `toUIMessageStreamResponse`**\n2. **Setting IDs in your start message part with `createUIMessageStream`**\n\n#### [Option 1: Using `generateMessageId` in `toUIMessageStreamResponse`](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#option-1-using-generatemessageid-in-touimessagestreamresponse)\n\nYou can control the ID format by providing ID generators using [`createIdGenerator()`](https://ai-sdk.dev/docs/reference/ai-sdk-core/create-id-generator):\n\napp/api/chat/route.ts\n\n```tsx\n1import { createIdGenerator, streamText } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  // ...\n\n5  const result = streamText({\n\n6    // ...\n\n7  });\n\n8\n\n9  return result.toUIMessageStreamResponse({\n\n10    originalMessages: messages,\n\n11    // Generate consistent server-side IDs for persistence:\n\n12    generateMessageId: createIdGenerator({\n\n13      prefix: 'msg',\n\n14      size: 16,\n\n15    }),\n\n16    onFinish: ({ messages }) => {\n\n17      saveChat({ chatId, messages });\n\n18    },\n\n19  });\n\n20}\n```\n\n#### [Option 2: Setting IDs with `createUIMessageStream`](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#option-2-setting-ids-with-createuimessagestream)\n\nAlternatively, you can use `createUIMessageStream` to control the message ID by writing a start message part:\n\napp/api/chat/route.ts\n\n```tsx\n1import {\n\n2  generateId,\n\n3  streamText,\n\n4  createUIMessageStream,\n\n5  createUIMessageStreamResponse,\n\n6} from 'ai';\n\n7\n\n8export async function POST(req: Request) {\n\n9  const { messages, chatId } = await req.json();\n\n10\n\n11  const stream = createUIMessageStream({\n\n12    execute: ({ writer }) => {\n\n13      // Write start message part with custom ID\n\n14      writer.write({\n\n15        type: 'start',\n\n16        messageId: generateId(), // Generate server-side ID for persistence\n\n17      });\n\n18\n\n19      const result = streamText({\n\n20        model: 'openai/gpt-5-mini',\n\n21        messages: await convertToModelMessages(messages),\n\n22      });\n\n23\n\n24      writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part\n\n25    },\n\n26    originalMessages: messages,\n\n27    onFinish: ({ responseMessage }) => {\n\n28      // save your chat here\n\n29    },\n\n30  });\n\n31\n\n32  return createUIMessageStreamResponse({ stream });\n\n33}\n```\n\nFor client-side applications that don't require persistence, you can still customize client-side ID generation:\n\nui/chat.tsx\n\n```tsx\n1import { createIdGenerator } from 'ai';\n\n2import { useChat } from '@ai-sdk/react';\n\n3\n\n4const { ... } = useChat({\n\n5  generateId: createIdGenerator({\n\n6    prefix: 'msgc',\n\n7    size: 16,\n\n8  }),\n\n9  // ...\n\n10});\n```\n\n## [Sending only the last message](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#sending-only-the-last-message)\n\nOnce you have implemented message persistence, you might want to send only the last message to the server.\nThis reduces the amount of data sent to the server on each request and can improve performance.\n\nTo achieve this, you can provide a `prepareSendMessagesRequest` function to the transport.\nThis function receives the messages and the chat ID, and returns the request body to be sent to the server.\n\nui/chat.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4const {\n\n5  // ...\n\n6} = useChat({\n\n7  // ...\n\n8  transport: new DefaultChatTransport({\n\n9    api: '/api/chat',\n\n10    // only send the last message to the server:\n\n11    prepareSendMessagesRequest({ messages, id }) {\n\n12      return { body: { message: messages[messages.length - 1], id } };\n\n13    },\n\n14  }),\n\n15});\n```\n\nOn the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:\n\napp/api/chat/route.ts\n\n```tsx\n1import { convertToModelMessages, UIMessage, validateUIMessages } from 'ai';\n\n2// import your tools and schemas\n\n3\n\n4export async function POST(req: Request) {\n\n5  // get the last message from the client:\n\n6  const { message, id } = await req.json();\n\n7\n\n8  // load the previous messages from the server:\n\n9  const previousMessages = await loadChat(id);\n\n10\n\n11  // validate messages if they contain tools, metadata, or data parts:\n\n12  const validatedMessages = await validateUIMessages({\n\n13    // append the new message to the previous messages:\n\n14    messages: [...previousMessages, message],\n\n15    tools, // if using tools\n\n16    metadataSchema, // if using custom metadata\n\n17    dataSchemas, // if using custom data parts\n\n18  });\n\n19\n\n20  const result = streamText({\n\n21    // ...\n\n22    messages: convertToModelMessages(validatedMessages),\n\n23  });\n\n24\n\n25  return result.toUIMessageStreamResponse({\n\n26    originalMessages: validatedMessages,\n\n27    onFinish: ({ messages }) => {\n\n28      saveChat({ chatId: id, messages });\n\n29    },\n\n30  });\n\n31}\n```\n\n## [Handling client disconnects](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence\\#handling-client-disconnects)\n\nBy default, the AI SDK `streamText` function uses backpressure to the language model provider to prevent\nthe consumption of tokens that are not yet requested.\n\nHowever, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue,\nthe stream from the LLM will be aborted and the conversation may end up in a broken state.\n\nAssuming that you have a [storage solution](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#storing-messages) in place, you can use the `consumeStream` method to consume the stream on the backend,\nand then save the result as usual.\n`consumeStream` effectively removes the backpressure,\nmeaning that the result is stored even when the client has already disconnected.\n\napp/api/chat/route.ts\n\n```tsx\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2import { saveChat } from '@util/chat-store';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n\n6    await req.json();\n\n7\n\n8  const result = streamText({\n\n9    model,\n\n10    messages: await convertToModelMessages(messages),\n\n11  });\n\n12\n\n13  // consume the stream to ensure it runs to completion & triggers onFinish\n\n14  // even when the client response is aborted:\n\n15  result.consumeStream(); // no await\n\n16\n\n17  return result.toUIMessageStreamResponse({\n\n18    originalMessages: messages,\n\n19    onFinish: ({ messages }) => {\n\n20      saveChat({ chatId, messages });\n\n21    },\n\n22  });\n\n23}\n```\n\nWhen the client reloads the page after a disconnect, the chat will be restored from the storage solution.\n\nIn production applications, you would also track the state of the request (in\nprogress, complete) in your stored messages and use it on the client to cover\nthe case where the client reloads the page after a disconnection, but the\nstreaming is not yet complete.\n\nFor more robust handling of disconnects, you may want to add resumability on disconnects. Check out the [Chatbot Resume Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams) documentation to learn more.\n\nOn this page\n\n[Chatbot Message Persistence](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#chatbot-message-persistence)\n\n[Starting a new chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#starting-a-new-chat)\n\n[Loading an existing chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#loading-an-existing-chat)\n\n[Validating messages on the server](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#validating-messages-on-the-server)\n\n[Validation with tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#validation-with-tools)\n\n[Handling validation errors](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#handling-validation-errors)\n\n[Displaying the chat](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#displaying-the-chat)\n\n[Storing messages](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#storing-messages)\n\n[Message IDs](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#message-ids)\n\n[Client-side vs Server-side ID Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#client-side-vs-server-side-id-generation)\n\n[Setting Up Server-side ID Generation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#setting-up-server-side-id-generation)\n\n[Option 1: Using generateMessageId in toUIMessageStreamResponse](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#option-1-using-generatemessageid-in-touimessagestreamresponse)\n\n[Option 2: Setting IDs with createUIMessageStream](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#option-2-setting-ids-with-createuimessagestream)\n\n[Sending only the last message](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#sending-only-the-last-message)\n\n[Handling client disconnects](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence#handling-client-disconnects)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence"
      },
      {
        "title": "Chatbot Resume Streams",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams",
        "content": "# [Chatbot Resume Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#chatbot-resume-streams)\n\n`useChat` supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.\n\nStream resumption is not compatible with abort functionality. Closing a tab or\nrefreshing the page triggers an abort signal that will break the resumption\nmechanism. Do not use `resume: true` if you need abort functionality in your\napplication. See\n[troubleshooting](https://ai-sdk.dev/docs/troubleshooting/abort-breaks-resumable-streams) for\nmore details.\n\n## [How stream resumption works](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#how-stream-resumption-works)\n\nStream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.\n\n**The AI SDK provides:**\n\n- A `resume` option in `useChat` that automatically reconnects to active streams\n- Access to the outgoing stream through the `consumeSseStream` callback\n- Automatic HTTP requests to your resume endpoints\n\n**You build:**\n\n- Storage to track which stream belongs to each chat\n- Redis to store the UIMessage stream\n- Two API endpoints: POST to create streams, GET to resume them\n- Integration with [`resumable-stream`](https://www.npmjs.com/package/resumable-stream) to manage Redis storage\n\n## [Prerequisites](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#prerequisites)\n\nTo implement resumable streams in your chat application, you need:\n\n1. **The `resumable-stream` package** \\- Handles the publisher/subscriber mechanism for streams\n2. **A Redis instance** \\- Stores stream data (e.g. [Redis through Vercel](https://vercel.com/marketplace/redis))\n3. **A persistence layer** \\- Tracks which stream ID is active for each chat (e.g. database)\n\n## [Implementation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#implementation)\n\n### [1\\. Client-side: Enable stream resumption](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#1-client-side-enable-stream-resumption)\n\nUse the `resume` option in the `useChat` hook to enable stream resumption. When `resume` is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:\n\napp/chat/\\[chatId\\]/chat.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport, type UIMessage } from 'ai';\n\n5\n\n6export function Chat({\n\n7  chatData,\n\n8  resume = false,\n\n9}: {\n\n10  chatData: { id: string; messages: UIMessage[] };\n\n11  resume?: boolean;\n\n12}) {\n\n13  const { messages, sendMessage, status } = useChat({\n\n14    id: chatData.id,\n\n15    messages: chatData.messages,\n\n16    resume, // Enable automatic stream resumption\n\n17    transport: new DefaultChatTransport({\n\n18      // You must send the id of the chat\n\n19      prepareSendMessagesRequest: ({ id, messages }) => {\n\n20        return {\n\n21          body: {\n\n22            id,\n\n23            message: messages[messages.length - 1],\n\n24          },\n\n25        };\n\n26      },\n\n27    }),\n\n28  });\n\n29\n\n30  return <div>{/* Your chat UI */}</div>;\n\n31}\n```\n\nYou must send the chat ID with each request (see\n`prepareSendMessagesRequest`).\n\nWhen you enable `resume`, the `useChat` hook makes a `GET` request to `/api/chat/[id]/stream` on mount to check for and resume any active streams.\n\nLet's start by creating the POST handler to create the resumable stream.\n\n### [2\\. Create the POST handler](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#2-create-the-post-handler)\n\nThe POST handler creates resumable streams using the `consumeSseStream` callback:\n\napp/api/chat/route.ts\n\n```ts\n1import { openai } from '@ai-sdk/openai';\n\n2import { readChat, saveChat } from '@util/chat-store';\n\n3import {\n\n4  convertToModelMessages,\n\n5  generateId,\n\n6  streamText,\n\n7  type UIMessage,\n\n8} from 'ai';\n\n9import { after } from 'next/server';\n\n10import { createResumableStreamContext } from 'resumable-stream';\n\n11\n\n12export async function POST(req: Request) {\n\n13  const {\n\n14    message,\n\n15    id,\n\n16  }: {\n\n17    message: UIMessage | undefined;\n\n18    id: string;\n\n19  } = await req.json();\n\n20\n\n21  const chat = await readChat(id);\n\n22  let messages = chat.messages;\n\n23\n\n24  messages = [...messages, message!];\n\n25\n\n26  // Clear any previous active stream and save the user message\n\n27  saveChat({ id, messages, activeStreamId: null });\n\n28\n\n29  const result = streamText({\n\n30    model: 'openai/gpt-5-mini',\n\n31    messages: await convertToModelMessages(messages),\n\n32  });\n\n33\n\n34  return result.toUIMessageStreamResponse({\n\n35    originalMessages: messages,\n\n36    generateMessageId: generateId,\n\n37    onFinish: ({ messages }) => {\n\n38      // Clear the active stream when finished\n\n39      saveChat({ id, messages, activeStreamId: null });\n\n40    },\n\n41    async consumeSseStream({ stream }) {\n\n42      const streamId = generateId();\n\n43\n\n44      // Create a resumable stream from the SSE stream\n\n45      const streamContext = createResumableStreamContext({ waitUntil: after });\n\n46      await streamContext.createNewResumableStream(streamId, () => stream);\n\n47\n\n48      // Update the chat with the active stream ID\n\n49      saveChat({ id, activeStreamId: streamId });\n\n50    },\n\n51  });\n\n52}\n```\n\n### [3\\. Implement the GET handler](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#3-implement-the-get-handler)\n\nCreate a GET handler at `/api/chat/[id]/stream` that:\n\n1. Reads the chat ID from the route params\n2. Loads the chat data to check for an active stream\n3. Returns 204 (No Content) if no stream is active\n4. Resumes the existing stream if one is found\n\napp/api/chat/\\[id\\]/stream/route.ts\n\n```ts\n1import { readChat } from '@util/chat-store';\n\n2import { UI_MESSAGE_STREAM_HEADERS } from 'ai';\n\n3import { after } from 'next/server';\n\n4import { createResumableStreamContext } from 'resumable-stream';\n\n5\n\n6export async function GET(\n\n7  _: Request,\n\n8  { params }: { params: Promise<{ id: string }> },\n\n9) {\n\n10  const { id } = await params;\n\n11\n\n12  const chat = await readChat(id);\n\n13\n\n14  if (chat.activeStreamId == null) {\n\n15    // no content response when there is no active stream\n\n16    return new Response(null, { status: 204 });\n\n17  }\n\n18\n\n19  const streamContext = createResumableStreamContext({\n\n20    waitUntil: after,\n\n21  });\n\n22\n\n23  return new Response(\n\n24    await streamContext.resumeExistingStream(chat.activeStreamId),\n\n25    { headers: UI_MESSAGE_STREAM_HEADERS },\n\n26  );\n\n27}\n```\n\nThe `after` function from Next.js allows work to continue after the response\nhas been sent. This ensures that the resumable stream persists in Redis even\nafter the initial response is returned to the client, enabling reconnection\nlater.\n\n## [How it works](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#how-it-works)\n\n### [Request lifecycle](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#request-lifecycle)\n\n![Diagram showing the architecture and lifecycle of resumable stream requests](https://e742qlubrjnjqpp0.public.blob.vercel-storage.com/resume-stream-diagram.png)\n\nThe diagram above shows the complete lifecycle of a resumable stream:\n\n1. **Stream creation**: When you send a new message, the POST handler uses `streamText` to generate the response. The `consumeSseStream` callback creates a resumable stream with a unique ID and stores it in Redis through the `resumable-stream` package\n2. **Stream tracking**: Your persistence layer saves the `activeStreamId` in the chat data\n3. **Client reconnection**: When the client reconnects (page reload), the `resume` option triggers a GET request to `/api/chat/[id]/stream`\n4. **Stream recovery**: The GET handler checks for an `activeStreamId` and uses `resumeExistingStream` to reconnect. If no active stream exists, it returns a 204 (No Content) response\n5. **Completion cleanup**: When the stream finishes, the `onFinish` callback clears the `activeStreamId` by setting it to `null`\n\n## [Customize the resume endpoint](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#customize-the-resume-endpoint)\n\nBy default, the `useChat` hook makes a GET request to `/api/chat/[id]/stream` when resuming. Customize this endpoint, credentials, and headers, using the `prepareReconnectToStreamRequest` option in `DefaultChatTransport`:\n\napp/chat/\\[chatId\\]/chat.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4export function Chat({ chatData, resume }) {\n\n5  const { messages, sendMessage } = useChat({\n\n6    id: chatData.id,\n\n7    messages: chatData.messages,\n\n8    resume,\n\n9    transport: new DefaultChatTransport({\n\n10      // Customize reconnect settings (optional)\n\n11      prepareReconnectToStreamRequest: ({ id }) => {\n\n12        return {\n\n13          api: `/api/chat/${id}/stream`, // Default pattern\n\n14          // Or use a different pattern:\n\n15          // api: `/api/streams/${id}/resume`,\n\n16          // api: `/api/resume-chat?id=${id}`,\n\n17          credentials: 'include', // Include cookies/auth\n\n18          headers: {\n\n19            Authorization: 'Bearer token',\n\n20            'X-Custom-Header': 'value',\n\n21          },\n\n22        };\n\n23      },\n\n24    }),\n\n25  });\n\n26\n\n27  return <div>{/* Your chat UI */}</div>;\n\n28}\n```\n\nThis lets you:\n\n- Match your existing API route structure\n- Add query parameters or custom paths\n- Integrate with different backend architectures\n\n## [Important considerations](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams\\#important-considerations)\n\n- **Incompatibility with abort**: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use `resume: true` if you need abort functionality in your application\n- **Stream expiration**: Streams in Redis expire after a set time (configurable in the `resumable-stream` package)\n- **Multiple clients**: Multiple clients can connect to the same stream simultaneously\n- **Error handling**: When no active stream exists, the GET handler returns a 204 (No Content) status code\n- **Security**: Ensure proper authentication and authorization for both creating and resuming streams\n- **Race conditions**: Clear the `activeStreamId` when starting a new stream to prevent resuming outdated streams\n\n[View Example on GitHub](https://github.com/vercel/ai/blob/main/examples/next)\n\nOn this page\n\n[Chatbot Resume Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#chatbot-resume-streams)\n\n[How stream resumption works](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#how-stream-resumption-works)\n\n[Prerequisites](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#prerequisites)\n\n[Implementation](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#implementation)\n\n[1\\. Client-side: Enable stream resumption](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#1-client-side-enable-stream-resumption)\n\n[2\\. Create the POST handler](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#2-create-the-post-handler)\n\n[3\\. Implement the GET handler](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#3-implement-the-get-handler)\n\n[How it works](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#how-it-works)\n\n[Request lifecycle](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#request-lifecycle)\n\n[Customize the resume endpoint](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#customize-the-resume-endpoint)\n\n[Important considerations](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams#important-considerations)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams"
      },
      {
        "title": "Chatbot Tool Usage",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
        "content": "# [Chatbot Tool Usage](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#chatbot-tool-usage)\n\nWith [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) and [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text), you can use tools in your chatbot application.\nThe AI SDK supports three types of tools in this context:\n\n1. Automatically executed server-side tools\n2. Automatically executed client-side tools\n3. Tools that require user interaction, such as confirmation dialogs\n\nThe flow is as follows:\n\n1. The user enters a message in the chat UI.\n2. The message is sent to the API route.\n3. In your server side route, the language model generates tool calls during the `streamText` call.\n4. All tool calls are forwarded to the client.\n5. Server-side tools are executed using their `execute` method and their results are forwarded to the client.\n6. Client-side tools that should be automatically executed are handled with the `onToolCall` callback.\nYou must call `addToolOutput` to provide the tool result.\n7. Client-side tool that require user interactions can be displayed in the UI.\nThe tool calls and results are available as tool invocation parts in the `parts` property of the last assistant message.\n8. When the user interaction is done, `addToolOutput` can be used to add the tool result to the chat.\n9. The chat can be configured to automatically submit when all tool results are available using `sendAutomaticallyWhen`.\nThis triggers another iteration of this flow.\n\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts.\nA tool part is at first a tool call, and then it becomes a tool result when the tool is executed.\nThe tool result contains all information about the tool call as well as the result of the tool execution.\n\nTool result submission can be configured using the `sendAutomaticallyWhen`\noption. You can use the `lastAssistantMessageIsCompleteWithToolCalls` helper\nto automatically submit when all tool results are available. This simplifies\nthe client-side code while still allowing full control when needed.\n\n## [Example](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#example)\n\nIn this example, we'll use three tools:\n\n- `getWeatherInformation`: An automatically executed server-side tool that returns the weather in a given city.\n- `askForConfirmation`: A user-interaction client-side tool that asks the user for confirmation.\n- `getLocation`: An automatically executed client-side tool that returns a random city.\n\n### [API route](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#api-route)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4// Allow streaming responses up to 30 seconds\n\n5export const maxDuration = 30;\n\n6\n\n7export async function POST(req: Request) {\n\n8  const { messages }: { messages: UIMessage[] } = await req.json();\n\n9\n\n10  const result = streamText({\n\n11    model: \"anthropic/claude-sonnet-4.5\",\n\n12    messages: await convertToModelMessages(messages),\n\n13    tools: {\n\n14      // server-side tool with execute function:\n\n15      getWeatherInformation: {\n\n16        description: 'show the weather in a given city to the user',\n\n17        inputSchema: z.object({ city: z.string() }),\n\n18        execute: async ({}: { city: string }) => {\n\n19          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n\n20          return weatherOptions[\\\n\\\n21            Math.floor(Math.random() * weatherOptions.length)\\\n\\\n22          ];\n\n23        },\n\n24      },\n\n25      // client-side tool that starts user interaction:\n\n26      askForConfirmation: {\n\n27        description: 'Ask the user for confirmation.',\n\n28        inputSchema: z.object({\n\n29          message: z.string().describe('The message to ask for confirmation.'),\n\n30        }),\n\n31      },\n\n32      // client-side tool that is automatically executed on the client:\n\n33      getLocation: {\n\n34        description:\n\n35          'Get the user location. Always ask for confirmation before using this tool.',\n\n36        inputSchema: z.object({}),\n\n37      },\n\n38    },\n\n39  });\n\n40\n\n41  return result.toUIMessageStreamResponse();\n\n42}\n```\n\n### [Client-side page](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#client-side-page)\n\nThe client-side page uses the `useChat` hook to create a chatbot application with real-time message streaming.\nTool calls are displayed in the chat UI as typed tool parts.\nPlease make sure to render the messages using the `parts` property of the message.\n\nThere are three things worth mentioning:\n\n1. The [`onToolCall`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#on-tool-call) callback is used to handle client-side tools that should be automatically executed.\nIn this example, the `getLocation` tool is a client-side tool that returns a random city.\nYou call `addToolOutput` to provide the result (without `await` to avoid potential deadlocks).\n\n\n\n\n\n\nAlways check `if (toolCall.dynamic)` first in your `onToolCall` handler.\nWithout this check, TypeScript will throw an error like: `Type 'string' is   not assignable to type '\"toolName1\" | \"toolName2\"'` when you try to use\n`toolCall.toolName` in `addToolOutput`.\n\n2. The [`sendAutomaticallyWhen`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#send-automatically-when) option with `lastAssistantMessageIsCompleteWithToolCalls` helper automatically submits when all tool results are available.\n\n3. The `parts` array of assistant messages contains tool parts with typed names like `tool-askForConfirmation`.\nThe client-side tool `askForConfirmation` is displayed in the UI.\nIt asks the user for confirmation and displays the result once the user confirms or denies the execution.\nThe result is added to the chat using `addToolOutput` with the `tool` parameter for type safety.\n\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import {\n\n5  DefaultChatTransport,\n\n6  lastAssistantMessageIsCompleteWithToolCalls,\n\n7} from 'ai';\n\n8import { useState } from 'react';\n\n9\n\n10export default function Chat() {\n\n11  const { messages, sendMessage, addToolOutput } = useChat({\n\n12    transport: new DefaultChatTransport({\n\n13      api: '/api/chat',\n\n14    }),\n\n15\n\n16    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n17\n\n18    // run client-side tools that are automatically executed:\n\n19    async onToolCall({ toolCall }) {\n\n20      // Check if it's a dynamic tool first for proper type narrowing\n\n21      if (toolCall.dynamic) {\n\n22        return;\n\n23      }\n\n24\n\n25      if (toolCall.toolName === 'getLocation') {\n\n26        const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n\n27\n\n28        // No await - avoids potential deadlocks\n\n29        addToolOutput({\n\n30          tool: 'getLocation',\n\n31          toolCallId: toolCall.toolCallId,\n\n32          output: cities[Math.floor(Math.random() * cities.length)],\n\n33        });\n\n34      }\n\n35    },\n\n36  });\n\n37  const [input, setInput] = useState('');\n\n38\n\n39  return (\n\n40    <>\n\n41      {messages?.map(message => (\n\n42        <div key={message.id}>\n\n43          <strong>{`${message.role}: `}</strong>\n\n44          {message.parts.map(part => {\n\n45            switch (part.type) {\n\n46              // render text parts as simple text:\n\n47              case 'text':\n\n48                return part.text;\n\n49\n\n50              // for tool parts, use the typed tool part names:\n\n51              case 'tool-askForConfirmation': {\n\n52                const callId = part.toolCallId;\n\n53\n\n54                switch (part.state) {\n\n55                  case 'input-streaming':\n\n56                    return (\n\n57                      <div key={callId}>Loading confirmation request...</div>\n\n58                    );\n\n59                  case 'input-available':\n\n60                    return (\n\n61                      <div key={callId}>\n\n62                        {part.input.message}\n\n63                        <div>\n\n64                          <button\n\n65                            onClick={() =>\n\n66                              addToolOutput({\n\n67                                tool: 'askForConfirmation',\n\n68                                toolCallId: callId,\n\n69                                output: 'Yes, confirmed.',\n\n70                              })\n\n71                            }\n\n72                          >\n\n73                            Yes\n\n74                          </button>\n\n75                          <button\n\n76                            onClick={() =>\n\n77                              addToolOutput({\n\n78                                tool: 'askForConfirmation',\n\n79                                toolCallId: callId,\n\n80                                output: 'No, denied',\n\n81                              })\n\n82                            }\n\n83                          >\n\n84                            No\n\n85                          </button>\n\n86                        </div>\n\n87                      </div>\n\n88                    );\n\n89                  case 'output-available':\n\n90                    return (\n\n91                      <div key={callId}>\n\n92                        Location access allowed: {part.output}\n\n93                      </div>\n\n94                    );\n\n95                  case 'output-error':\n\n96                    return <div key={callId}>Error: {part.errorText}</div>;\n\n97                }\n\n98                break;\n\n99              }\n\n100\n\n101              case 'tool-getLocation': {\n\n102                const callId = part.toolCallId;\n\n103\n\n104                switch (part.state) {\n\n105                  case 'input-streaming':\n\n106                    return (\n\n107                      <div key={callId}>Preparing location request...</div>\n\n108                    );\n\n109                  case 'input-available':\n\n110                    return <div key={callId}>Getting location...</div>;\n\n111                  case 'output-available':\n\n112                    return <div key={callId}>Location: {part.output}</div>;\n\n113                  case 'output-error':\n\n114                    return (\n\n115                      <div key={callId}>\n\n116                        Error getting location: {part.errorText}\n\n117                      </div>\n\n118                    );\n\n119                }\n\n120                break;\n\n121              }\n\n122\n\n123              case 'tool-getWeatherInformation': {\n\n124                const callId = part.toolCallId;\n\n125\n\n126                switch (part.state) {\n\n127                  // example of pre-rendering streaming tool inputs:\n\n128                  case 'input-streaming':\n\n129                    return (\n\n130                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\n\n131                    );\n\n132                  case 'input-available':\n\n133                    return (\n\n134                      <div key={callId}>\n\n135                        Getting weather information for {part.input.city}...\n\n136                      </div>\n\n137                    );\n\n138                  case 'output-available':\n\n139                    return (\n\n140                      <div key={callId}>\n\n141                        Weather in {part.input.city}: {part.output}\n\n142                      </div>\n\n143                    );\n\n144                  case 'output-error':\n\n145                    return (\n\n146                      <div key={callId}>\n\n147                        Error getting weather for {part.input.city}:{' '}\n\n148                        {part.errorText}\n\n149                      </div>\n\n150                    );\n\n151                }\n\n152                break;\n\n153              }\n\n154            }\n\n155          })}\n\n156          <br />\n\n157        </div>\n\n158      ))}\n\n159\n\n160      <form\n\n161        onSubmit={e => {\n\n162          e.preventDefault();\n\n163          if (input.trim()) {\n\n164            sendMessage({ text: input });\n\n165            setInput('');\n\n166          }\n\n167        }}\n\n168      >\n\n169        <input value={input} onChange={e => setInput(e.target.value)} />\n\n170      </form>\n\n171    </>\n\n172  );\n\n173}\n```\n\n### [Error handling](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#error-handling)\n\nSometimes an error may occur during client-side tool execution. Use the `addToolOutput` method with a `state` of `output-error` and `errorText` value instead of `output` record the error.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import {\n\n5  DefaultChatTransport,\n\n6  lastAssistantMessageIsCompleteWithToolCalls,\n\n7} from 'ai';\n\n8import { useState } from 'react';\n\n9\n\n10export default function Chat() {\n\n11  const { messages, sendMessage, addToolOutput } = useChat({\n\n12    transport: new DefaultChatTransport({\n\n13      api: '/api/chat',\n\n14    }),\n\n15\n\n16    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n17\n\n18    // run client-side tools that are automatically executed:\n\n19    async onToolCall({ toolCall }) {\n\n20      // Check if it's a dynamic tool first for proper type narrowing\n\n21      if (toolCall.dynamic) {\n\n22        return;\n\n23      }\n\n24\n\n25      if (toolCall.toolName === 'getWeatherInformation') {\n\n26        try {\n\n27          const weather = await getWeatherInformation(toolCall.input);\n\n28\n\n29          // No await - avoids potential deadlocks\n\n30          addToolOutput({\n\n31            tool: 'getWeatherInformation',\n\n32            toolCallId: toolCall.toolCallId,\n\n33            output: weather,\n\n34          });\n\n35        } catch (err) {\n\n36          addToolOutput({\n\n37            tool: 'getWeatherInformation',\n\n38            toolCallId: toolCall.toolCallId,\n\n39            state: 'output-error',\n\n40            errorText: 'Unable to get the weather information',\n\n41          });\n\n42        }\n\n43      }\n\n44    },\n\n45  });\n\n46}\n```\n\n## [Tool Execution Approval](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#tool-execution-approval)\n\nTool execution approval lets you require user confirmation before a server-side tool runs. Unlike [client-side tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#example) that execute in the browser, tools with approval still execute on the server—but only after the user approves.\n\nUse tool execution approval when you want to:\n\n- Confirm sensitive operations (payments, deletions, external API calls)\n- Let users review tool inputs before execution\n- Add human oversight to automated workflows\n\nFor tools that need to run in the browser (updating UI state, accessing browser APIs), use client-side tools instead.\n\n### [Server Setup](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#server-setup)\n\nEnable approval by setting `needsApproval` on your tool. See [Tool Execution Approval](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling#tool-execution-approval) for configuration options including dynamic approval based on input.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { streamText, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages,\n\n10    tools: {\n\n11      getWeather: tool({\n\n12        description: 'Get the weather in a location',\n\n13        inputSchema: z.object({\n\n14          city: z.string(),\n\n15        }),\n\n16        needsApproval: true,\n\n17        execute: async ({ city }) => {\n\n18          const weather = await fetchWeather(city);\n\n19          return weather;\n\n20        },\n\n21      }),\n\n22    },\n\n23  });\n\n24\n\n25  return result.toUIMessageStreamResponse();\n\n26}\n```\n\n### [Client-Side Approval UI](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#client-side-approval-ui)\n\nWhen a tool requires approval, the tool part state is `approval-requested`. Use `addToolApprovalResponse` to approve or deny:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4\n\n5export default function Chat() {\n\n6  const { messages, addToolApprovalResponse } = useChat();\n\n7\n\n8  return (\n\n9    <>\n\n10      {messages.map(message => (\n\n11        <div key={message.id}>\n\n12          {message.parts.map(part => {\n\n13            if (part.type === 'tool-getWeather') {\n\n14              switch (part.state) {\n\n15                case 'approval-requested':\n\n16                  return (\n\n17                    <div key={part.toolCallId}>\n\n18                      <p>Get weather for {part.input.city}?</p>\n\n19                      <button\n\n20                        onClick={() =>\n\n21                          addToolApprovalResponse({\n\n22                            id: part.approval.id,\n\n23                            approved: true,\n\n24                          })\n\n25                        }\n\n26                      >\n\n27                        Approve\n\n28                      </button>\n\n29                      <button\n\n30                        onClick={() =>\n\n31                          addToolApprovalResponse({\n\n32                            id: part.approval.id,\n\n33                            approved: false,\n\n34                          })\n\n35                        }\n\n36                      >\n\n37                        Deny\n\n38                      </button>\n\n39                    </div>\n\n40                  );\n\n41                case 'output-available':\n\n42                  return (\n\n43                    <div key={part.toolCallId}>\n\n44                      Weather in {part.input.city}: {part.output}\n\n45                    </div>\n\n46                  );\n\n47              }\n\n48            }\n\n49            // Handle other part types...\n\n50          })}\n\n51        </div>\n\n52      ))}\n\n53    </>\n\n54  );\n\n55}\n```\n\n### [Auto-Submit After Approval](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#auto-submit-after-approval)\n\nIf nothing happens after you approve a tool execution, make sure you either\ncall `sendMessage` manually or configure `sendAutomaticallyWhen` on the\n`useChat` hook.\n\nUse `lastAssistantMessageIsCompleteWithApprovalResponses` to automatically continue the conversation after approvals:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { lastAssistantMessageIsCompleteWithApprovalResponses } from 'ai';\n\n3\n\n4const { messages, addToolApprovalResponse } = useChat({\n\n5  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithApprovalResponses,\n\n6});\n```\n\n## [Dynamic Tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#dynamic-tools)\n\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic `dynamic-tool` type instead of specific tool types:\n\napp/page.tsx\n\n```tsx\n1{\n\n2  message.parts.map((part, index) => {\n\n3    switch (part.type) {\n\n4      // Static tools with specific (`tool-${toolName}`) types\n\n5      case 'tool-getWeatherInformation':\n\n6        return <WeatherDisplay part={part} />;\n\n7\n\n8      // Dynamic tools use generic `dynamic-tool` type\n\n9      case 'dynamic-tool':\n\n10        return (\n\n11          <div key={index}>\n\n12            <h4>Tool: {part.toolName}</h4>\n\n13            {part.state === 'input-streaming' && (\n\n14              <pre>{JSON.stringify(part.input, null, 2)}</pre>\n\n15            )}\n\n16            {part.state === 'output-available' && (\n\n17              <pre>{JSON.stringify(part.output, null, 2)}</pre>\n\n18            )}\n\n19            {part.state === 'output-error' && (\n\n20              <div>Error: {part.errorText}</div>\n\n21            )}\n\n22          </div>\n\n23        );\n\n24    }\n\n25  });\n\n26}\n```\n\nDynamic tools are useful when integrating with:\n\n- MCP (Model Context Protocol) tools without schemas\n- User-defined functions loaded at runtime\n- External tool providers\n\n## [Tool call streaming](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#tool-call-streaming)\n\nTool call streaming is **enabled by default** in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1export async function POST(req: Request) {\n\n2  const { messages }: { messages: UIMessage[] } = await req.json();\n\n3\n\n4  const result = streamText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    messages: await convertToModelMessages(messages),\n\n7    // toolCallStreaming is enabled by default in v5\n\n8    // ...\n\n9  });\n\n10\n\n11  return result.toUIMessageStreamResponse();\n\n12}\n```\n\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream.\nThey are available through the `useChat` hook.\nThe typed tool parts of assistant messages will also contain partial tool calls.\nYou can use the `state` property of the tool part to render the correct UI.\n\napp/page.tsx\n\n```tsx\n1export default function Chat() {\n\n2  // ...\n\n3  return (\n\n4    <>\n\n5      {messages?.map(message => (\n\n6        <div key={message.id}>\n\n7          {message.parts.map(part => {\n\n8            switch (part.type) {\n\n9              case 'tool-askForConfirmation':\n\n10              case 'tool-getLocation':\n\n11              case 'tool-getWeatherInformation':\n\n12                switch (part.state) {\n\n13                  case 'input-streaming':\n\n14                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n\n15                  case 'input-available':\n\n16                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n\n17                  case 'output-available':\n\n18                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\n\n19                  case 'output-error':\n\n20                    return <div>Error: {part.errorText}</div>;\n\n21                }\n\n22            }\n\n23          })}\n\n24        </div>\n\n25      ))}\n\n26    </>\n\n27  );\n\n28}\n```\n\n## [Step start parts](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#step-start-parts)\n\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages.\nIf you want to display boundaries between tool calls, you can use the `step-start` parts as follows:\n\napp/page.tsx\n\n```tsx\n1// ...\n\n2// where you render the message parts:\n\n3message.parts.map((part, index) => {\n\n4  switch (part.type) {\n\n5    case 'step-start':\n\n6      // show step boundaries as horizontal lines:\n\n7      return index > 0 ? (\n\n8        <div key={index} className=\"text-gray-500\">\n\n9          <hr className=\"my-2 border-gray-300\" />\n\n10        </div>\n\n11      ) : null;\n\n12    case 'text':\n\n13    // ...\n\n14    case 'tool-askForConfirmation':\n\n15    case 'tool-getLocation':\n\n16    case 'tool-getWeatherInformation':\n\n17    // ...\n\n18  }\n\n19});\n\n20// ...\n```\n\n## [Server-side Multi-Step Calls](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#server-side-multi-step-calls)\n\nYou can also use multi-step calls on the server-side with `streamText`.\nThis works when all invoked tools have an `execute` function on the server side.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```tsx\n1import { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages }: { messages: UIMessage[] } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages: await convertToModelMessages(messages),\n\n10    tools: {\n\n11      getWeatherInformation: {\n\n12        description: 'show the weather in a given city to the user',\n\n13        inputSchema: z.object({ city: z.string() }),\n\n14        // tool has execute function:\n\n15        execute: async ({}: { city: string }) => {\n\n16          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n\n17          return weatherOptions[\\\n\\\n18            Math.floor(Math.random() * weatherOptions.length)\\\n\\\n19          ];\n\n20        },\n\n21      },\n\n22    },\n\n23    stopWhen: stepCountIs(5),\n\n24  });\n\n25\n\n26  return result.toUIMessageStreamResponse();\n\n27}\n```\n\n## [Errors](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage\\#errors)\n\nLanguage models can make errors when calling tools.\nBy default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\n\nTo surface the errors, you can use the `onError` function when calling `toUIMessageResponse`.\n\n```tsx\n1export function errorHandler(error: unknown) {\n\n2  if (error == null) {\n\n3    return 'unknown error';\n\n4  }\n\n5\n\n6  if (typeof error === 'string') {\n\n7    return error;\n\n8  }\n\n9\n\n10  if (error instanceof Error) {\n\n11    return error.message;\n\n12  }\n\n13\n\n14  return JSON.stringify(error);\n\n15}\n```\n\n```tsx\n1const result = streamText({\n\n2  // ...\n\n3});\n\n4\n\n5return result.toUIMessageStreamResponse({\n\n6  onError: errorHandler,\n\n7});\n```\n\nIn case you are using `createUIMessageResponse`, you can use the `onError` function when calling `toUIMessageResponse`:\n\n```tsx\n1const response = createUIMessageResponse({\n\n2  // ...\n\n3  async execute(dataStream) {\n\n4    // ...\n\n5  },\n\n6  onError: error => `Custom error: ${error.message}`,\n\n7});\n```\n\nOn this page\n\n[Chatbot Tool Usage](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#chatbot-tool-usage)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#example)\n\n[API route](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#api-route)\n\n[Client-side page](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#client-side-page)\n\n[Error handling](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#error-handling)\n\n[Tool Execution Approval](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#tool-execution-approval)\n\n[Server Setup](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#server-setup)\n\n[Client-Side Approval UI](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#client-side-approval-ui)\n\n[Auto-Submit After Approval](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#auto-submit-after-approval)\n\n[Dynamic Tools](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#dynamic-tools)\n\n[Tool call streaming](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#tool-call-streaming)\n\n[Step start parts](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#step-start-parts)\n\n[Server-side Multi-Step Calls](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#server-side-multi-step-calls)\n\n[Errors](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#errors)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage"
      },
      {
        "title": "Generative User Interfaces",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
        "content": "# [Generative User Interfaces](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#generative-user-interfaces)\n\nGenerative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and \"generate UI\". This creates a more engaging and AI-native experience for users.\n\nWhat is the weather in SF?\n\ngetWeather(\"San Francisco\")\n\nThursday, March 7\n\n47°\n\nsunny\n\n7am\n\n48°\n\n8am\n\n50°\n\n9am\n\n52°\n\n10am\n\n54°\n\n11am\n\n56°\n\n12pm\n\n58°\n\n1pm\n\n60°\n\nThanks!\n\nAt the core of generative UI are [tools](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling), which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.\n\nGenerative UI is the process of connecting the results of a tool call to a React component. Here's how it works:\n\n1. You provide the model with a prompt or conversation history, along with a set of tools.\n2. Based on the context, the model may decide to call a tool.\n3. If a tool is called, it will execute and return data.\n4. This data can then be passed to a React component for rendering.\n\nBy passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.\n\n## [Build a Generative UI Chat Interface](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#build-a-generative-ui-chat-interface)\n\nLet's create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.\n\n### [Basic Chat Implementation](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#basic-chat-implementation)\n\nStart with a basic chat implementation using the `useChat` hook:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Page() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage } = useChat();\n\n9\n\n10  const handleSubmit = (e: React.FormEvent) => {\n\n11    e.preventDefault();\n\n12    sendMessage({ text: input });\n\n13    setInput('');\n\n14  };\n\n15\n\n16  return (\n\n17    <div>\n\n18      {messages.map(message => (\n\n19        <div key={message.id}>\n\n20          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n\n21          <div>\n\n22            {message.parts.map((part, index) => {\n\n23              if (part.type === 'text') {\n\n24                return <span key={index}>{part.text}</span>;\n\n25              }\n\n26              return null;\n\n27            })}\n\n28          </div>\n\n29        </div>\n\n30      ))}\n\n31\n\n32      <form onSubmit={handleSubmit}>\n\n33        <input\n\n34          value={input}\n\n35          onChange={e => setInput(e.target.value)}\n\n36          placeholder=\"Type a message...\"\n\n37        />\n\n38        <button type=\"submit\">Send</button>\n\n39      </form>\n\n40    </div>\n\n41  );\n\n42}\n```\n\nTo handle the chat requests and model responses, set up an API route:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\n\n2\n\n3export async function POST(request: Request) {\n\n4  const { messages }: { messages: UIMessage[] } = await request.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    system: 'You are a friendly assistant!',\n\n9    messages: await convertToModelMessages(messages),\n\n10    stopWhen: stepCountIs(5),\n\n11  });\n\n12\n\n13  return result.toUIMessageStreamResponse();\n\n14}\n```\n\nThis API route uses the `streamText` function to process chat messages and stream the model's responses back to the client.\n\n### [Create a Tool](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#create-a-tool)\n\nBefore enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.\n\nCreate a new file called `ai/tools.ts` with the following content:\n\nai/tools.ts\n\n```ts\n1import { tool as createTool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4export const weatherTool = createTool({\n\n5  description: 'Display the weather for a location',\n\n6  inputSchema: z.object({\n\n7    location: z.string().describe('The location to get the weather for'),\n\n8  }),\n\n9  execute: async function ({ location }) {\n\n10    await new Promise(resolve => setTimeout(resolve, 2000));\n\n11    return { weather: 'Sunny', temperature: 75, location };\n\n12  },\n\n13});\n\n14\n\n15export const tools = {\n\n16  displayWeather: weatherTool,\n\n17};\n```\n\nIn this file, you've created a tool called `weatherTool`. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.\n\n### [Update the API Route](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#update-the-api-route)\n\nUpdate the API route to include the tool you've defined:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\n\n2import { tools } from '@/ai/tools';\n\n3\n\n4export async function POST(request: Request) {\n\n5  const { messages }: { messages: UIMessage[] } = await request.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    system: 'You are a friendly assistant!',\n\n10    messages: await convertToModelMessages(messages),\n\n11    stopWhen: stepCountIs(5),\n\n12    tools,\n\n13  });\n\n14\n\n15  return result.toUIMessageStreamResponse();\n\n16}\n```\n\nNow that you've defined the tool and added it to your `streamText` call, let's build a React component to display the weather information it returns.\n\n### [Create UI Components](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#create-ui-components)\n\nCreate a new file called `components/weather.tsx`:\n\ncomponents/weather.tsx\n\n```tsx\n1type WeatherProps = {\n\n2  temperature: number;\n\n3  weather: string;\n\n4  location: string;\n\n5};\n\n6\n\n7export const Weather = ({ temperature, weather, location }: WeatherProps) => {\n\n8  return (\n\n9    <div>\n\n10      <h2>Current Weather for {location}</h2>\n\n11      <p>Condition: {weather}</p>\n\n12      <p>Temperature: {temperature}°C</p>\n\n13    </div>\n\n14  );\n\n15};\n```\n\nThis component will display the weather information for a given location. It takes three props: `temperature`, `weather`, and `location` (exactly what the `weatherTool` returns).\n\n### [Render the Weather Component](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#render-the-weather-component)\n\nNow that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.\n\nTo check if the model has called a tool, you can check the `parts` array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: `tool-${toolName}` instead of generic types.\n\nUpdate your `page.tsx` file:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5import { Weather } from '@/components/weather';\n\n6\n\n7export default function Page() {\n\n8  const [input, setInput] = useState('');\n\n9  const { messages, sendMessage } = useChat();\n\n10\n\n11  const handleSubmit = (e: React.FormEvent) => {\n\n12    e.preventDefault();\n\n13    sendMessage({ text: input });\n\n14    setInput('');\n\n15  };\n\n16\n\n17  return (\n\n18    <div>\n\n19      {messages.map(message => (\n\n20        <div key={message.id}>\n\n21          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n\n22          <div>\n\n23            {message.parts.map((part, index) => {\n\n24              if (part.type === 'text') {\n\n25                return <span key={index}>{part.text}</span>;\n\n26              }\n\n27\n\n28              if (part.type === 'tool-displayWeather') {\n\n29                switch (part.state) {\n\n30                  case 'input-available':\n\n31                    return <div key={index}>Loading weather...</div>;\n\n32                  case 'output-available':\n\n33                    return (\n\n34                      <div key={index}>\n\n35                        <Weather {...part.output} />\n\n36                      </div>\n\n37                    );\n\n38                  case 'output-error':\n\n39                    return <div key={index}>Error: {part.errorText}</div>;\n\n40                  default:\n\n41                    return null;\n\n42                }\n\n43              }\n\n44\n\n45              return null;\n\n46            })}\n\n47          </div>\n\n48        </div>\n\n49      ))}\n\n50\n\n51      <form onSubmit={handleSubmit}>\n\n52        <input\n\n53          value={input}\n\n54          onChange={e => setInput(e.target.value)}\n\n55          placeholder=\"Type a message...\"\n\n56        />\n\n57        <button type=\"submit\">Send</button>\n\n58      </form>\n\n59    </div>\n\n60  );\n\n61}\n```\n\nIn this updated code snippet, you:\n\n1. Use manual input state management with `useState` instead of the built-in `input` and `handleInputChange`.\n2. Use `sendMessage` instead of `handleSubmit` to send messages.\n3. Check the `parts` array of each message for different content types.\n4. Handle tool parts with type `tool-displayWeather` and their different states (`input-available`, `output-available`, `output-error`).\n\nThis approach allows you to dynamically render UI components based on the model's responses, creating a more interactive and context-aware chat experience.\n\n## [Expanding Your Generative UI Application](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#expanding-your-generative-ui-application)\n\nYou can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:\n\n### [Adding More Tools](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces\\#adding-more-tools)\n\nTo add more tools, simply define them in your `ai/tools.ts` file:\n\n```ts\n1// Add a new stock tool\n\n2export const stockTool = createTool({\n\n3  description: 'Get price for a stock',\n\n4  inputSchema: z.object({\n\n5    symbol: z.string().describe('The stock symbol to get the price for'),\n\n6  }),\n\n7  execute: async function ({ symbol }) {\n\n8    // Simulated API call\n\n9    await new Promise(resolve => setTimeout(resolve, 2000));\n\n10    return { symbol, price: 100 };\n\n11  },\n\n12});\n\n13\n\n14// Update the tools object\n\n15export const tools = {\n\n16  displayWeather: weatherTool,\n\n17  getStockPrice: stockTool,\n\n18};\n```\n\nNow, create a new file called `components/stock.tsx`:\n\n```tsx\n1type StockProps = {\n\n2  price: number;\n\n3  symbol: string;\n\n4};\n\n5\n\n6export const Stock = ({ price, symbol }: StockProps) => {\n\n7  return (\n\n8    <div>\n\n9      <h2>Stock Information</h2>\n\n10      <p>Symbol: {symbol}</p>\n\n11      <p>Price: ${price}</p>\n\n12    </div>\n\n13  );\n\n14};\n```\n\nFinally, update your `page.tsx` file to include the new Stock component:\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5import { Weather } from '@/components/weather';\n\n6import { Stock } from '@/components/stock';\n\n7\n\n8export default function Page() {\n\n9  const [input, setInput] = useState('');\n\n10  const { messages, sendMessage } = useChat();\n\n11\n\n12  const handleSubmit = (e: React.FormEvent) => {\n\n13    e.preventDefault();\n\n14    sendMessage({ text: input });\n\n15    setInput('');\n\n16  };\n\n17\n\n18  return (\n\n19    <div>\n\n20      {messages.map(message => (\n\n21        <div key={message.id}>\n\n22          <div>{message.role}</div>\n\n23          <div>\n\n24            {message.parts.map((part, index) => {\n\n25              if (part.type === 'text') {\n\n26                return <span key={index}>{part.text}</span>;\n\n27              }\n\n28\n\n29              if (part.type === 'tool-displayWeather') {\n\n30                switch (part.state) {\n\n31                  case 'input-available':\n\n32                    return <div key={index}>Loading weather...</div>;\n\n33                  case 'output-available':\n\n34                    return (\n\n35                      <div key={index}>\n\n36                        <Weather {...part.output} />\n\n37                      </div>\n\n38                    );\n\n39                  case 'output-error':\n\n40                    return <div key={index}>Error: {part.errorText}</div>;\n\n41                  default:\n\n42                    return null;\n\n43                }\n\n44              }\n\n45\n\n46              if (part.type === 'tool-getStockPrice') {\n\n47                switch (part.state) {\n\n48                  case 'input-available':\n\n49                    return <div key={index}>Loading stock price...</div>;\n\n50                  case 'output-available':\n\n51                    return (\n\n52                      <div key={index}>\n\n53                        <Stock {...part.output} />\n\n54                      </div>\n\n55                    );\n\n56                  case 'output-error':\n\n57                    return <div key={index}>Error: {part.errorText}</div>;\n\n58                  default:\n\n59                    return null;\n\n60                }\n\n61              }\n\n62\n\n63              return null;\n\n64            })}\n\n65          </div>\n\n66        </div>\n\n67      ))}\n\n68\n\n69      <form onSubmit={handleSubmit}>\n\n70        <input\n\n71          type=\"text\"\n\n72          value={input}\n\n73          onChange={e => setInput(e.target.value)}\n\n74        />\n\n75        <button type=\"submit\">Send</button>\n\n76      </form>\n\n77    </div>\n\n78  );\n\n79}\n```\n\nBy following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.\n\nOn this page\n\n[Generative User Interfaces](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#generative-user-interfaces)\n\n[Build a Generative UI Chat Interface](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#build-a-generative-ui-chat-interface)\n\n[Basic Chat Implementation](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#basic-chat-implementation)\n\n[Create a Tool](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#create-a-tool)\n\n[Update the API Route](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#update-the-api-route)\n\n[Create UI Components](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#create-ui-components)\n\n[Render the Weather Component](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#render-the-weather-component)\n\n[Expanding Your Generative UI Application](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#expanding-your-generative-ui-application)\n\n[Adding More Tools](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces#adding-more-tools)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces"
      },
      {
        "title": "Completion",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
        "content": "# [Completion](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#completion)\n\nThe `useCompletion` hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\n\nThe `useCompletion` hook is now part of the `@ai-sdk/react` package.\n\nIn this guide, you will learn how to use the `useCompletion` hook in your application to generate text completions and stream them in real-time to your users.\n\n## [Example](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#example)\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useCompletion } from '@ai-sdk/react';\n\n4\n\n5export default function Page() {\n\n6  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n\n7    api: '/api/completion',\n\n8  });\n\n9\n\n10  return (\n\n11    <form onSubmit={handleSubmit}>\n\n12      <input\n\n13        name=\"prompt\"\n\n14        value={input}\n\n15        onChange={handleInputChange}\n\n16        id=\"input\"\n\n17      />\n\n18      <button type=\"submit\">Submit</button>\n\n19      <div>{completion}</div>\n\n20    </form>\n\n21  );\n\n22}\n```\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/completion/route.ts\n\n```ts\n1import { streamText } from 'ai';\n\n2\n\n3// Allow streaming responses up to 30 seconds\n\n4export const maxDuration = 30;\n\n5\n\n6export async function POST(req: Request) {\n\n7  const { prompt }: { prompt: string } = await req.json();\n\n8\n\n9  const result = streamText({\n\n10    model: \"anthropic/claude-sonnet-4.5\",\n\n11    prompt,\n\n12  });\n\n13\n\n14  return result.toUIMessageStreamResponse();\n\n15}\n```\n\nIn the `Page` component, the `useCompletion` hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.\n\nThis enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\n\n## [Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#customized-ui)\n\n`useCompletion` also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.\n\n### [Loading and error states](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#loading-and-error-states)\n\nTo show a loading spinner while the chatbot is processing the user's message, you can use the `isLoading` state returned by the `useCompletion` hook:\n\n```tsx\n1const { isLoading, ... } = useCompletion()\n\n2\n\n3return(\n\n4  <>\n\n5    {isLoading ? <Spinner /> : null}\n\n6  </>\n\n7)\n```\n\nSimilarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:\n\n```tsx\n1const { error, ... } = useCompletion()\n\n2\n\n3useEffect(() => {\n\n4  if (error) {\n\n5    toast.error(error.message)\n\n6  }\n\n7}, [error])\n\n8\n\n9// Or display the error message in the UI:\n\n10return (\n\n11  <>\n\n12    {error ? <div>{error.message}</div> : null}\n\n13  </>\n\n14)\n```\n\n### [Controlled input](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#controlled-input)\n\nIn the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.\n\nThe following example demonstrates how to use more granular APIs like `setInput` with your custom input and submit button components:\n\n```tsx\n1const { input, setInput } = useCompletion();\n\n2\n\n3return (\n\n4  <>\n\n5    <MyCustomInput value={input} onChange={value => setInput(value)} />\n\n6  </>\n\n7);\n```\n\n### [Cancelation](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#cancelation)\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useCompletion` hook.\n\n```tsx\n1const { stop, isLoading, ... } = useCompletion()\n\n2\n\n3return (\n\n4  <>\n\n5    <button onClick={stop} disabled={!isLoading}>Stop</button>\n\n6  </>\n\n7)\n```\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.\n\n### [Throttling UI Updates](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#throttling-ui-updates)\n\nThis feature is currently only available for React.\n\nBy default, the `useCompletion` hook will trigger a render every time a new chunk is received.\nYou can throttle the UI updates with the `experimental_throttle` option.\n\npage.tsx\n\n```tsx\n1const { completion, ... } = useCompletion({\n\n2  // Throttle the completion and data updates to 50ms:\n\n3  experimental_throttle: 50\n\n4})\n```\n\n## [Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#event-callbacks)\n\n`useCompletion` also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\n```tsx\n1const { ... } = useCompletion({\n\n2  onResponse: (response: Response) => {\n\n3    console.log('Received response from server:', response)\n\n4  },\n\n5  onFinish: (prompt: string, completion: string) => {\n\n6    console.log('Finished streaming completion:', completion)\n\n7  },\n\n8  onError: (error: Error) => {\n\n9    console.error('An error occurred:', error)\n\n10  },\n\n11})\n```\n\nIt's worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\n## [Configure Request Options](https://ai-sdk.dev/docs/ai-sdk-ui/completion\\#configure-request-options)\n\nBy default, the `useCompletion` hook sends a HTTP POST request to the `/api/completion` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useCompletion` hook:\n\n```tsx\n1const { messages, input, handleInputChange, handleSubmit } = useCompletion({\n\n2  api: '/api/custom-completion',\n\n3  headers: {\n\n4    Authorization: 'your_token',\n\n5  },\n\n6  body: {\n\n7    user_id: '123',\n\n8  },\n\n9  credentials: 'same-origin',\n\n10});\n```\n\nIn this example, the `useCompletion` hook sends a POST request to the `/api/completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.\n\nOn this page\n\n[Completion](https://ai-sdk.dev/docs/ai-sdk-ui/completion#completion)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-ui/completion#example)\n\n[Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/completion#customized-ui)\n\n[Loading and error states](https://ai-sdk.dev/docs/ai-sdk-ui/completion#loading-and-error-states)\n\n[Controlled input](https://ai-sdk.dev/docs/ai-sdk-ui/completion#controlled-input)\n\n[Cancelation](https://ai-sdk.dev/docs/ai-sdk-ui/completion#cancelation)\n\n[Throttling UI Updates](https://ai-sdk.dev/docs/ai-sdk-ui/completion#throttling-ui-updates)\n\n[Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/completion#event-callbacks)\n\n[Configure Request Options](https://ai-sdk.dev/docs/ai-sdk-ui/completion#configure-request-options)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/completion"
      },
      {
        "title": "Object Generation",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
        "content": "# [Object Generation](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#object-generation)\n\n`useObject` is an experimental feature and only available in React, Svelte,\nand Vue.\n\nThe [`useObject`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) hook allows you to create interfaces that represent a structured JSON object that is being streamed.\n\nIn this guide, you will learn how to use the `useObject` hook in your application to generate UIs for structured data on the fly.\n\n## [Example](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#example)\n\nThe example shows a small notifications demo app that generates fake notifications in real-time.\n\n### [Schema](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#schema)\n\nIt is helpful to set up the schema in a separate file that is imported on both the client and server.\n\napp/api/notifications/schema.ts\n\n```ts\n1import { z } from 'zod';\n\n2\n\n3// define a schema for the notifications\n\n4export const notificationSchema = z.object({\n\n5  notifications: z.array(\n\n6    z.object({\n\n7      name: z.string().describe('Name of a fictional person.'),\n\n8      message: z.string().describe('Message. Do not use emojis or links.'),\n\n9    }),\n\n10  ),\n\n11});\n```\n\n### [Client](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#client)\n\nThe client uses [`useObject`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.\n\nThe results are partial and are displayed as they are received.\nPlease note the code for handling `undefined` values in the JSX.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { experimental_useObject as useObject } from '@ai-sdk/react';\n\n4import { notificationSchema } from './api/notifications/schema';\n\n5\n\n6export default function Page() {\n\n7  const { object, submit } = useObject({\n\n8    api: '/api/notifications',\n\n9    schema: notificationSchema,\n\n10  });\n\n11\n\n12  return (\n\n13    <>\n\n14      <button onClick={() => submit('Messages during finals week.')}>\n\n15        Generate notifications\n\n16      </button>\n\n17\n\n18      {object?.notifications?.map((notification, index) => (\n\n19        <div key={index}>\n\n20          <p>{notification?.name}</p>\n\n21          <p>{notification?.message}</p>\n\n22        </div>\n\n23      ))}\n\n24    </>\n\n25  );\n\n26}\n```\n\n### [Server](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#server)\n\nOn the server, we use [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) with [`Output.object()`](https://ai-sdk.dev/docs/reference/ai-sdk-core/output#output-object) to stream the object generation process.\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/notifications/route.ts\n\n```typescript\n1import { streamText, Output } from 'ai';\n\n2import { notificationSchema } from './schema';\n\n3\n\n4// Allow streaming responses up to 30 seconds\n\n5export const maxDuration = 30;\n\n6\n\n7export async function POST(req: Request) {\n\n8  const context = await req.json();\n\n9\n\n10  const result = streamText({\n\n11    model: \"anthropic/claude-sonnet-4.5\",\n\n12    output: Output.object({ schema: notificationSchema }),\n\n13    prompt:\n\n14      `Generate 3 notifications for a messages app in this context:` + context,\n\n15  });\n\n16\n\n17  return result.toTextStreamResponse();\n\n18}\n```\n\n## [Enum Output Mode](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#enum-output-mode)\n\nWhen you need to classify or categorize input into predefined options, you can use the `enum` output mode with `useObject`. This requires a specific schema structure where the object has `enum` as a key with `z.enum` containing your possible values.\n\n### [Example: Text Classification](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#example-text-classification)\n\nThis example shows how to build a simple text classifier that categorizes statements as true or false.\n\n#### [Client](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#client-1)\n\nWhen using `useObject` with enum output mode, your schema must be an object with `enum` as the key:\n\napp/classify/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { experimental_useObject as useObject } from '@ai-sdk/react';\n\n4import { z } from 'zod';\n\n5\n\n6export default function ClassifyPage() {\n\n7  const { object, submit, isLoading } = useObject({\n\n8    api: '/api/classify',\n\n9    schema: z.object({ enum: z.enum(['true', 'false']) }),\n\n10  });\n\n11\n\n12  return (\n\n13    <>\n\n14      <button onClick={() => submit('The earth is flat')} disabled={isLoading}>\n\n15        Classify statement\n\n16      </button>\n\n17\n\n18      {object && <div>Classification: {object.enum}</div>}\n\n19    </>\n\n20  );\n\n21}\n```\n\n#### [Server](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#server-1)\n\nOn the server, use `streamText` with `Output.choice()` to stream the classification result:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/classify/route.ts\n\n```typescript\n1import { streamText, Output } from 'ai';\n\n2\n\n3export async function POST(req: Request) {\n\n4  const context = await req.json();\n\n5\n\n6  const result = streamText({\n\n7    model: \"anthropic/claude-sonnet-4.5\",\n\n8    output: Output.choice({ options: ['true', 'false'] }),\n\n9    prompt: `Classify this statement as true or false: ${context}`,\n\n10  });\n\n11\n\n12  return result.toTextStreamResponse();\n\n13}\n```\n\n## [Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#customized-ui)\n\n`useObject` also provides ways to show loading and error states:\n\n### [Loading State](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#loading-state)\n\nThe `isLoading` state returned by the `useObject` hook can be used for several\npurposes:\n\n- To show a loading spinner while the object is generated.\n- To disable the submit button.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useObject } from '@ai-sdk/react';\n\n4\n\n5export default function Page() {\n\n6  const { isLoading, object, submit } = useObject({\n\n7    api: '/api/notifications',\n\n8    schema: notificationSchema,\n\n9  });\n\n10\n\n11  return (\n\n12    <>\n\n13      {isLoading && <Spinner />}\n\n14\n\n15      <button\n\n16        onClick={() => submit('Messages during finals week.')}\n\n17        disabled={isLoading}\n\n18      >\n\n19        Generate notifications\n\n20      </button>\n\n21\n\n22      {object?.notifications?.map((notification, index) => (\n\n23        <div key={index}>\n\n24          <p>{notification?.name}</p>\n\n25          <p>{notification?.message}</p>\n\n26        </div>\n\n27      ))}\n\n28    </>\n\n29  );\n\n30}\n```\n\n### [Stop Handler](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#stop-handler)\n\nThe `stop` function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useObject } from '@ai-sdk/react';\n\n4\n\n5export default function Page() {\n\n6  const { isLoading, stop, object, submit } = useObject({\n\n7    api: '/api/notifications',\n\n8    schema: notificationSchema,\n\n9  });\n\n10\n\n11  return (\n\n12    <>\n\n13      {isLoading && (\n\n14        <button type=\"button\" onClick={() => stop()}>\n\n15          Stop\n\n16        </button>\n\n17      )}\n\n18\n\n19      <button onClick={() => submit('Messages during finals week.')}>\n\n20        Generate notifications\n\n21      </button>\n\n22\n\n23      {object?.notifications?.map((notification, index) => (\n\n24        <div key={index}>\n\n25          <p>{notification?.name}</p>\n\n26          <p>{notification?.message}</p>\n\n27        </div>\n\n28      ))}\n\n29    </>\n\n30  );\n\n31}\n```\n\n### [Error State](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#error-state)\n\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\nIt can be used to display an error message, or to disable the submit button:\n\nWe recommend showing a generic error message to the user, such as \"Something\nwent wrong.\" This is a good practice to avoid leaking information from the\nserver.\n\n```tsx\n1'use client';\n\n2\n\n3import { useObject } from '@ai-sdk/react';\n\n4\n\n5export default function Page() {\n\n6  const { error, object, submit } = useObject({\n\n7    api: '/api/notifications',\n\n8    schema: notificationSchema,\n\n9  });\n\n10\n\n11  return (\n\n12    <>\n\n13      {error && <div>An error occurred.</div>}\n\n14\n\n15      <button onClick={() => submit('Messages during finals week.')}>\n\n16        Generate notifications\n\n17      </button>\n\n18\n\n19      {object?.notifications?.map((notification, index) => (\n\n20        <div key={index}>\n\n21          <p>{notification?.name}</p>\n\n22          <p>{notification?.message}</p>\n\n23        </div>\n\n24      ))}\n\n25    </>\n\n26  );\n\n27}\n```\n\n## [Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#event-callbacks)\n\n`useObject` provides optional event callbacks that you can use to handle life-cycle events.\n\n- `onFinish`: Called when the object generation is completed.\n- `onError`: Called when an error occurs during the fetch request.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { experimental_useObject as useObject } from '@ai-sdk/react';\n\n4import { notificationSchema } from './api/notifications/schema';\n\n5\n\n6export default function Page() {\n\n7  const { object, submit } = useObject({\n\n8    api: '/api/notifications',\n\n9    schema: notificationSchema,\n\n10    onFinish({ object, error }) {\n\n11      // typed object, undefined if schema validation fails:\n\n12      console.log('Object generation completed:', object);\n\n13\n\n14      // error, undefined if schema validation succeeds:\n\n15      console.log('Schema validation error:', error);\n\n16    },\n\n17    onError(error) {\n\n18      // error during fetch request:\n\n19      console.error('An error occurred:', error);\n\n20    },\n\n21  });\n\n22\n\n23  return (\n\n24    <div>\n\n25      <button onClick={() => submit('Messages during finals week.')}>\n\n26        Generate notifications\n\n27      </button>\n\n28\n\n29      {object?.notifications?.map((notification, index) => (\n\n30        <div key={index}>\n\n31          <p>{notification?.name}</p>\n\n32          <p>{notification?.message}</p>\n\n33        </div>\n\n34      ))}\n\n35    </div>\n\n36  );\n\n37}\n```\n\n## [Configure Request Options](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation\\#configure-request-options)\n\nYou can configure the API endpoint, optional headers and credentials using the `api`, `headers` and `credentials` settings.\n\n```tsx\n1const { submit, object } = useObject({\n\n2  api: '/api/use-object',\n\n3  headers: {\n\n4    'X-Custom-Header': 'CustomValue',\n\n5  },\n\n6  credentials: 'include',\n\n7  schema: yourSchema,\n\n8});\n```\n\nOn this page\n\n[Object Generation](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#object-generation)\n\n[Example](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#example)\n\n[Schema](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#schema)\n\n[Client](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#client)\n\n[Server](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#server)\n\n[Enum Output Mode](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#enum-output-mode)\n\n[Example: Text Classification](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#example-text-classification)\n\n[Client](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#client-1)\n\n[Server](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#server-1)\n\n[Customized UI](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#customized-ui)\n\n[Loading State](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#loading-state)\n\n[Stop Handler](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#stop-handler)\n\n[Error State](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#error-state)\n\n[Event Callbacks](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#event-callbacks)\n\n[Configure Request Options](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation#configure-request-options)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation"
      },
      {
        "title": "Streaming Custom Data",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
        "content": "# [Streaming Custom Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#streaming-custom-data)\n\nIt is often useful to send additional data alongside the model's response.\nFor example, you may want to send status information, the message ids after storing them,\nor references to content that the language model is referring to.\n\nThe AI SDK provides several helpers that allows you to stream additional data to the client\nand attach it to the `UIMessage` parts array:\n\n- `createUIMessageStream`: creates a data stream\n- `createUIMessageStreamResponse`: creates a response object that streams data\n- `pipeUIMessageStreamToResponse`: pipes a data stream to a server response object\n\nThe data is streamed as part of the response stream using Server-Sent Events.\n\n## [Setting Up Type-Safe Data Streaming](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#setting-up-type-safe-data-streaming)\n\nFirst, define your custom message type with data part schemas for type safety:\n\nai/types.ts\n\n```tsx\n1import { UIMessage } from 'ai';\n\n2\n\n3// Define your custom message type with data part schemas\n\n4export type MyUIMessage = UIMessage<\n\n5  never, // metadata type\n\n6  {\n\n7    weather: {\n\n8      city: string;\n\n9      weather?: string;\n\n10      status: 'loading' | 'success';\n\n11    };\n\n12    notification: {\n\n13      message: string;\n\n14      level: 'info' | 'warning' | 'error';\n\n15    };\n\n16  } // data parts type\n\n17>;\n```\n\n## [Streaming Data from the Server](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#streaming-data-from-the-server)\n\nIn your server-side route handler, you can create a `UIMessageStream` and then pass it to `createUIMessageStreamResponse`:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\nroute.ts\n\n```tsx\n1import { openai } from '@ai-sdk/openai';\n\n2import {\n\n3  createUIMessageStream,\n\n4  createUIMessageStreamResponse,\n\n5  streamText,\n\n6  convertToModelMessages,\n\n7} from 'ai';\n\n8import type { MyUIMessage } from '@/ai/types';\n\n9\n\n10export async function POST(req: Request) {\n\n11  const { messages } = await req.json();\n\n12\n\n13  const stream = createUIMessageStream<MyUIMessage>({\n\n14    execute: ({ writer }) => {\n\n15      // 1. Send initial status (transient - won't be added to message history)\n\n16      writer.write({\n\n17        type: 'data-notification',\n\n18        data: { message: 'Processing your request...', level: 'info' },\n\n19        transient: true, // This part won't be added to message history\n\n20      });\n\n21\n\n22      // 2. Send sources (useful for RAG use cases)\n\n23      writer.write({\n\n24        type: 'source',\n\n25        value: {\n\n26          type: 'source',\n\n27          sourceType: 'url',\n\n28          id: 'source-1',\n\n29          url: 'https://weather.com',\n\n30          title: 'Weather Data Source',\n\n31        },\n\n32      });\n\n33\n\n34      // 3. Send data parts with loading state\n\n35      writer.write({\n\n36        type: 'data-weather',\n\n37        id: 'weather-1',\n\n38        data: { city: 'San Francisco', status: 'loading' },\n\n39      });\n\n40\n\n41      const result = streamText({\n\n42        model: \"anthropic/claude-sonnet-4.5\",\n\n43        messages: await convertToModelMessages(messages),\n\n44        onFinish() {\n\n45          // 4. Update the same data part (reconciliation)\n\n46          writer.write({\n\n47            type: 'data-weather',\n\n48            id: 'weather-1', // Same ID = update existing part\n\n49            data: {\n\n50              city: 'San Francisco',\n\n51              weather: 'sunny',\n\n52              status: 'success',\n\n53            },\n\n54          });\n\n55\n\n56          // 5. Send completion notification (transient)\n\n57          writer.write({\n\n58            type: 'data-notification',\n\n59            data: { message: 'Request completed', level: 'info' },\n\n60            transient: true, // Won't be added to message history\n\n61          });\n\n62        },\n\n63      });\n\n64\n\n65      writer.merge(result.toUIMessageStream());\n\n66    },\n\n67  });\n\n68\n\n69  return createUIMessageStreamResponse({ stream });\n\n70}\n```\n\nYou can also send stream data from custom backends, e.g. Python / FastAPI,\nusing the [UI Message Stream\\\\\nProtocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#ui-message-stream-protocol).\n\n## [Types of Streamable Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#types-of-streamable-data)\n\n### [Data Parts (Persistent)](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#data-parts-persistent)\n\nRegular data parts are added to the message history and appear in `message.parts`:\n\n```tsx\n1writer.write({\n\n2  type: 'data-weather',\n\n3  id: 'weather-1', // Optional: enables reconciliation\n\n4  data: { city: 'San Francisco', status: 'loading' },\n\n5});\n```\n\n### [Sources](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#sources)\n\nSources are useful for RAG implementations where you want to show which documents or URLs were referenced:\n\n```tsx\n1writer.write({\n\n2  type: 'source',\n\n3  value: {\n\n4    type: 'source',\n\n5    sourceType: 'url',\n\n6    id: 'source-1',\n\n7    url: 'https://example.com',\n\n8    title: 'Example Source',\n\n9  },\n\n10});\n```\n\n### [Transient Data Parts (Ephemeral)](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#transient-data-parts-ephemeral)\n\nTransient parts are sent to the client but not added to the message history. They are only accessible via the `onData` useChat handler:\n\n```tsx\n1// server\n\n2writer.write({\n\n3  type: 'data-notification',\n\n4  data: { message: 'Processing...', level: 'info' },\n\n5  transient: true, // Won't be added to message history\n\n6});\n\n7\n\n8// client\n\n9const [notification, setNotification] = useState();\n\n10\n\n11const { messages } = useChat({\n\n12  onData: ({ data, type }) => {\n\n13    if (type === 'data-notification') {\n\n14      setNotification({ message: data.message, level: data.level });\n\n15    }\n\n16  },\n\n17});\n```\n\n## [Data Part Reconciliation](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#data-part-reconciliation)\n\nWhen you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:\n\n- **Collaborative artifacts** \\- Update code, documents, or designs in real-time\n- **Progressive data loading** \\- Show loading states that transform into final results\n- **Live status updates** \\- Update progress bars, counters, or status indicators\n- **Interactive components** \\- Build UI elements that evolve based on user interaction\n\nThe reconciliation happens automatically - simply use the same `id` when writing to the stream.\n\n## [Processing Data on the Client](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#processing-data-on-the-client)\n\n### [Using the onData Callback](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#using-the-ondata-callback)\n\nThe `onData` callback is essential for handling streaming data, especially transient parts:\n\npage.tsx\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import type { MyUIMessage } from '@/ai/types';\n\n3\n\n4const { messages } = useChat<MyUIMessage>({\n\n5  api: '/api/chat',\n\n6  onData: dataPart => {\n\n7    // Handle all data parts as they arrive (including transient parts)\n\n8    console.log('Received data part:', dataPart);\n\n9\n\n10    // Handle different data part types\n\n11    if (dataPart.type === 'data-weather') {\n\n12      console.log('Weather update:', dataPart.data);\n\n13    }\n\n14\n\n15    // Handle transient notifications (ONLY available here, not in message.parts)\n\n16    if (dataPart.type === 'data-notification') {\n\n17      showToast(dataPart.data.message, dataPart.data.level);\n\n18    }\n\n19  },\n\n20});\n```\n\n**Important:** Transient data parts are **only** available through the `onData` callback. They will not appear in the `message.parts` array since they're not added to message history.\n\n### [Rendering Persistent Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#rendering-persistent-data-parts)\n\nYou can filter and render data parts from the message parts array:\n\npage.tsx\n\n```tsx\n1const result = (\n\n2  <>\n\n3    {messages?.map(message => (\n\n4      <div key={message.id}>\n\n5        {/* Render weather data parts */}\n\n6        {message.parts\n\n7          .filter(part => part.type === 'data-weather')\n\n8          .map((part, index) => (\n\n9            <div key={index} className=\"weather-widget\">\n\n10              {part.data.status === 'loading' ? (\n\n11                <>Getting weather for {part.data.city}...</>\n\n12              ) : (\n\n13                <>\n\n14                  Weather in {part.data.city}: {part.data.weather}\n\n15                </>\n\n16              )}\n\n17            </div>\n\n18          ))}\n\n19\n\n20        {/* Render text content */}\n\n21        {message.parts\n\n22          .filter(part => part.type === 'text')\n\n23          .map((part, index) => (\n\n24            <div key={index}>{part.text}</div>\n\n25          ))}\n\n26\n\n27        {/* Render sources */}\n\n28        {message.parts\n\n29          .filter(part => part.type === 'source')\n\n30          .map((part, index) => (\n\n31            <div key={index} className=\"source\">\n\n32              Source: <a href={part.url}>{part.title}</a>\n\n33            </div>\n\n34          ))}\n\n35      </div>\n\n36    ))}\n\n37  </>\n\n38);\n```\n\n### [Complete Example](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#complete-example)\n\npage.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5import type { MyUIMessage } from '@/ai/types';\n\n6\n\n7export default function Chat() {\n\n8  const [input, setInput] = useState('');\n\n9\n\n10  const { messages, sendMessage } = useChat<MyUIMessage>({\n\n11    api: '/api/chat',\n\n12    onData: dataPart => {\n\n13      // Handle transient notifications\n\n14      if (dataPart.type === 'data-notification') {\n\n15        console.log('Notification:', dataPart.data.message);\n\n16      }\n\n17    },\n\n18  });\n\n19\n\n20  const handleSubmit = (e: React.FormEvent) => {\n\n21    e.preventDefault();\n\n22    sendMessage({ text: input });\n\n23    setInput('');\n\n24  };\n\n25\n\n26  return (\n\n27    <>\n\n28      {messages?.map(message => (\n\n29        <div key={message.id}>\n\n30          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n31\n\n32          {/* Render weather data */}\n\n33          {message.parts\n\n34            .filter(part => part.type === 'data-weather')\n\n35            .map((part, index) => (\n\n36              <span key={index} className=\"weather-update\">\n\n37                {part.data.status === 'loading' ? (\n\n38                  <>Getting weather for {part.data.city}...</>\n\n39                ) : (\n\n40                  <>\n\n41                    Weather in {part.data.city}: {part.data.weather}\n\n42                  </>\n\n43                )}\n\n44              </span>\n\n45            ))}\n\n46\n\n47          {/* Render text content */}\n\n48          {message.parts\n\n49            .filter(part => part.type === 'text')\n\n50            .map((part, index) => (\n\n51              <div key={index}>{part.text}</div>\n\n52            ))}\n\n53        </div>\n\n54      ))}\n\n55\n\n56      <form onSubmit={handleSubmit}>\n\n57        <input\n\n58          value={input}\n\n59          onChange={e => setInput(e.target.value)}\n\n60          placeholder=\"Ask about the weather...\"\n\n61        />\n\n62        <button type=\"submit\">Send</button>\n\n63      </form>\n\n64    </>\n\n65  );\n\n66}\n```\n\n## [Use Cases](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#use-cases)\n\n- **RAG Applications** \\- Stream sources and retrieved documents\n- **Real-time Status** \\- Show loading states and progress updates\n- **Collaborative Tools** \\- Stream live updates to shared artifacts\n- **Analytics** \\- Send usage data without cluttering message history\n- **Notifications** \\- Display temporary alerts and status messages\n\n## [Message Metadata vs Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#message-metadata-vs-data-parts)\n\nBoth [message metadata](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata) and data parts allow you to send additional information alongside messages, but they serve different purposes:\n\n### [Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#message-metadata)\n\nMessage metadata is best for **message-level information** that describes the message as a whole:\n\n- Attached at the message level via `message.metadata`\n- Sent using the `messageMetadata` callback in `toUIMessageStreamResponse`\n- Ideal for: timestamps, model info, token usage, user context\n- Type-safe with custom metadata types\n\n```ts\n1// Server: Send metadata about the message\n\n2return result.toUIMessageStreamResponse({\n\n3  messageMetadata: ({ part }) => {\n\n4    if (part.type === 'finish') {\n\n5      return {\n\n6        model: part.response.modelId,\n\n7        totalTokens: part.totalUsage.totalTokens,\n\n8        createdAt: Date.now(),\n\n9      };\n\n10    }\n\n11  },\n\n12});\n```\n\n### [Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data\\#data-parts)\n\nData parts are best for streaming **dynamic arbitrary data**:\n\n- Added to the message parts array via `message.parts`\n- Streamed using `createUIMessageStream` and `writer.write()`\n- Can be reconciled/updated using the same ID\n- Support transient parts that don't persist\n- Ideal for: dynamic content, loading states, interactive components\n\n```ts\n1// Server: Stream data as part of message content\n\n2writer.write({\n\n3  type: 'data-weather',\n\n4  id: 'weather-1',\n\n5  data: { city: 'San Francisco', status: 'loading' },\n\n6});\n```\n\nFor more details on message metadata, see the [Message Metadata documentation](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata).\n\nOn this page\n\n[Streaming Custom Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#streaming-custom-data)\n\n[Setting Up Type-Safe Data Streaming](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#setting-up-type-safe-data-streaming)\n\n[Streaming Data from the Server](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#streaming-data-from-the-server)\n\n[Types of Streamable Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#types-of-streamable-data)\n\n[Data Parts (Persistent)](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#data-parts-persistent)\n\n[Sources](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#sources)\n\n[Transient Data Parts (Ephemeral)](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#transient-data-parts-ephemeral)\n\n[Data Part Reconciliation](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#data-part-reconciliation)\n\n[Processing Data on the Client](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#processing-data-on-the-client)\n\n[Using the onData Callback](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#using-the-ondata-callback)\n\n[Rendering Persistent Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#rendering-persistent-data-parts)\n\n[Complete Example](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#complete-example)\n\n[Use Cases](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#use-cases)\n\n[Message Metadata vs Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#message-metadata-vs-data-parts)\n\n[Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#message-metadata)\n\n[Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#data-parts)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data"
      },
      {
        "title": "Error Handling and warnings",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
        "content": "# [Error Handling and warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#error-handling-and-warnings)\n\n## [Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#warnings)\n\nThe AI SDK shows warnings when something might not work as expected.\nThese warnings help you fix problems before they cause errors.\n\n### [When Warnings Appear](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#when-warnings-appear)\n\nWarnings are shown in the browser console when:\n\n- **Unsupported features**: You use a feature or setting that is not supported by the AI model (e.g., certain options or parameters).\n- **Compatibility warnings**: A feature is used in a compatibility mode, which might work differently or less optimally than intended.\n- **Other warnings**: The AI model reports another type of issue, such as general problems or advisory messages.\n\n### [Warning Messages](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#warning-messages)\n\nAll warnings start with \"AI SDK Warning:\" so you can easily find them. For example:\n\n```typescript\n1AI SDK Warning: The feature \"temperature\" is not supported by this model\n```\n\n### [Turning Off Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#turning-off-warnings)\n\nBy default, warnings are shown in the console. You can control this behavior:\n\n#### [Turn Off All Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#turn-off-all-warnings)\n\nSet a global variable to turn off warnings completely:\n\n```ts\n1globalThis.AI_SDK_LOG_WARNINGS = false;\n```\n\n#### [Custom Warning Handler](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#custom-warning-handler)\n\nYou can also provide your own function to handle warnings.\nIt receives provider id, model id, and a list of warnings.\n\n```ts\n1globalThis.AI_SDK_LOG_WARNINGS = ({ warnings, provider, model }) => {\n\n2  // Handle warnings your own way\n\n3};\n```\n\n## [Error Handling](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#error-handling)\n\n### [Error Helper Object](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#error-helper-object)\n\nEach AI SDK UI hook also returns an [error](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#error) object that you can use to render the error in your UI.\nYou can use the error object to show an error message, disable the submit button, or show a retry button.\n\nWe recommend showing a generic error message to the user, such as \"Something\nwent wrong.\" This is a good practice to avoid leaking information from the\nserver.\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage, error, regenerate } = useChat();\n\n9\n\n10  const handleSubmit = (e: React.FormEvent) => {\n\n11    e.preventDefault();\n\n12    sendMessage({ text: input });\n\n13    setInput('');\n\n14  };\n\n15\n\n16  return (\n\n17    <div>\n\n18      {messages.map(m => (\n\n19        <div key={m.id}>\n\n20          {m.role}:{' '}\n\n21          {m.parts\n\n22            .filter(part => part.type === 'text')\n\n23            .map(part => part.text)\n\n24            .join('')}\n\n25        </div>\n\n26      ))}\n\n27\n\n28      {error && (\n\n29        <>\n\n30          <div>An error occurred.</div>\n\n31          <button type=\"button\" onClick={() => regenerate()}>\n\n32            Retry\n\n33          </button>\n\n34        </>\n\n35      )}\n\n36\n\n37      <form onSubmit={handleSubmit}>\n\n38        <input\n\n39          value={input}\n\n40          onChange={e => setInput(e.target.value)}\n\n41          disabled={error != null}\n\n42        />\n\n43      </form>\n\n44    </div>\n\n45  );\n\n46}\n```\n\n#### [Alternative: replace last message](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#alternative-replace-last-message)\n\nAlternatively you can write a custom submit handler that replaces the last message when an error is present.\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { sendMessage, error, messages, setMessages } = useChat();\n\n9\n\n10  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\n\n11    event.preventDefault();\n\n12\n\n13    if (error != null) {\n\n14      setMessages(messages.slice(0, -1)); // remove last message\n\n15    }\n\n16\n\n17    sendMessage({ text: input });\n\n18    setInput('');\n\n19  }\n\n20\n\n21  return (\n\n22    <div>\n\n23      {messages.map(m => (\n\n24        <div key={m.id}>\n\n25          {m.role}:{' '}\n\n26          {m.parts\n\n27            .filter(part => part.type === 'text')\n\n28            .map(part => part.text)\n\n29            .join('')}\n\n30        </div>\n\n31      ))}\n\n32\n\n33      {error && <div>An error occurred.</div>}\n\n34\n\n35      <form onSubmit={customSubmit}>\n\n36        <input value={input} onChange={e => setInput(e.target.value)} />\n\n37      </form>\n\n38    </div>\n\n39  );\n\n40}\n```\n\n### [Error Handling Callback](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#error-handling-callback)\n\nErrors can be processed by passing an [`onError`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#on-error) callback function as an option to the [`useChat`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion) hooks.\nThe callback function receives an error object as an argument.\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2\n\n3export default function Page() {\n\n4  const {\n\n5    /* ... */\n\n6  } = useChat({\n\n7    // handle error:\n\n8    onError: error => {\n\n9      console.error(error);\n\n10    },\n\n11  });\n\n12}\n```\n\n### [Injecting Errors for Testing](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling\\#injecting-errors-for-testing)\n\nYou might want to create errors for testing.\nYou can easily do so by throwing an error in your route handler:\n\n```ts\n1export async function POST(req: Request) {\n\n2  throw new Error('This is a test error');\n\n3}\n```\n\nOn this page\n\n[Error Handling and warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#error-handling-and-warnings)\n\n[Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#warnings)\n\n[When Warnings Appear](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#when-warnings-appear)\n\n[Warning Messages](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#warning-messages)\n\n[Turning Off Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#turning-off-warnings)\n\n[Turn Off All Warnings](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#turn-off-all-warnings)\n\n[Custom Warning Handler](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#custom-warning-handler)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#error-handling)\n\n[Error Helper Object](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#error-helper-object)\n\n[Alternative: replace last message](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#alternative-replace-last-message)\n\n[Error Handling Callback](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#error-handling-callback)\n\n[Injecting Errors for Testing](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling#injecting-errors-for-testing)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling"
      },
      {
        "title": "Transport",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/transport",
        "content": "# [Transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#transport)\n\nThe `useChat` transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.\n\n## [Default Transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#default-transport)\n\nBy default, `useChat` uses HTTP POST requests to send messages to `/api/chat`:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2\n\n3// Uses default HTTP transport\n\n4const { messages, sendMessage } = useChat();\n```\n\nThis is equivalent to:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4const { messages, sendMessage } = useChat({\n\n5  transport: new DefaultChatTransport({\n\n6    api: '/api/chat',\n\n7  }),\n\n8});\n```\n\n## [Custom Transport Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#custom-transport-configuration)\n\nConfigure the default transport with custom options:\n\n```tsx\n1import { useChat } from '@ai-sdk/react';\n\n2import { DefaultChatTransport } from 'ai';\n\n3\n\n4const { messages, sendMessage } = useChat({\n\n5  transport: new DefaultChatTransport({\n\n6    api: '/api/custom-chat',\n\n7    headers: {\n\n8      Authorization: 'Bearer your-token',\n\n9      'X-API-Version': '2024-01',\n\n10    },\n\n11    credentials: 'include',\n\n12  }),\n\n13});\n```\n\n### [Dynamic Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#dynamic-configuration)\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\n```tsx\n1const { messages, sendMessage } = useChat({\n\n2  transport: new DefaultChatTransport({\n\n3    api: '/api/chat',\n\n4    headers: () => ({\n\n5      Authorization: `Bearer ${getAuthToken()}`,\n\n6      'X-User-ID': getCurrentUserId(),\n\n7    }),\n\n8    body: () => ({\n\n9      sessionId: getCurrentSessionId(),\n\n10      preferences: getUserPreferences(),\n\n11    }),\n\n12    credentials: () => 'include',\n\n13  }),\n\n14});\n```\n\n### [Request Transformation](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#request-transformation)\n\nTransform requests before sending to your API:\n\n```tsx\n1const { messages, sendMessage } = useChat({\n\n2  transport: new DefaultChatTransport({\n\n3    api: '/api/chat',\n\n4    prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n\n5      return {\n\n6        headers: {\n\n7          'X-Session-ID': id,\n\n8        },\n\n9        body: {\n\n10          messages: messages.slice(-10), // Only send last 10 messages\n\n11          trigger,\n\n12          messageId,\n\n13        },\n\n14      };\n\n15    },\n\n16  }),\n\n17});\n```\n\n## [Building Custom Transports](https://ai-sdk.dev/docs/ai-sdk-ui/transport\\#building-custom-transports)\n\nTo understand how to build your own transport, refer to the source code of the default implementation:\n\n- **[DefaultChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/default-chat-transport.ts)** \\- The complete default HTTP transport implementation\n- **[HttpChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/http-chat-transport.ts)** \\- Base HTTP transport with request handling\n- **[ChatTransport Interface](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/chat-transport.ts)** \\- The transport interface you need to implement\n\nThese implementations show you exactly how to:\n\n- Handle the `sendMessages` method\n- Process UI message streams\n- Transform requests and responses\n- Handle errors and connection management\n\nThe transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.\n\nOn this page\n\n[Transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport#transport)\n\n[Default Transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport#default-transport)\n\n[Custom Transport Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/transport#custom-transport-configuration)\n\n[Dynamic Configuration](https://ai-sdk.dev/docs/ai-sdk-ui/transport#dynamic-configuration)\n\n[Request Transformation](https://ai-sdk.dev/docs/ai-sdk-ui/transport#request-transformation)\n\n[Building Custom Transports](https://ai-sdk.dev/docs/ai-sdk-ui/transport#building-custom-transports)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/transport",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/transport",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/transport"
      },
      {
        "title": "Reading UI Message Streams",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams",
        "content": "# [Reading UI Message Streams](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams\\#reading-ui-message-streams)\n\n`UIMessage` streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).\n\nThe `readUIMessageStream` helper transforms a stream of `UIMessageChunk` objects into an `AsyncIterableStream` of `UIMessage` objects, allowing you to process messages as they're being constructed.\n\n## [Basic Usage](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams\\#basic-usage)\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { readUIMessageStream, streamText } from 'ai';\n\n2\n\n3async function main() {\n\n4  const result = streamText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    prompt: 'Write a short story about a robot.',\n\n7  });\n\n8\n\n9  for await (const uiMessage of readUIMessageStream({\n\n10    stream: result.toUIMessageStream(),\n\n11  })) {\n\n12    console.log('Current message state:', uiMessage);\n\n13  }\n\n14}\n```\n\n## [Tool Calls Integration](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams\\#tool-calls-integration)\n\nHandle streaming responses that include tool calls:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { readUIMessageStream, streamText, tool } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4async function handleToolCalls() {\n\n5  const result = streamText({\n\n6    model: \"anthropic/claude-sonnet-4.5\",\n\n7    tools: {\n\n8      weather: tool({\n\n9        description: 'Get the weather in a location',\n\n10        inputSchema: z.object({\n\n11          location: z.string().describe('The location to get the weather for'),\n\n12        }),\n\n13        execute: ({ location }) => ({\n\n14          location,\n\n15          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n\n16        }),\n\n17      }),\n\n18    },\n\n19    prompt: 'What is the weather in Tokyo?',\n\n20  });\n\n21\n\n22  for await (const uiMessage of readUIMessageStream({\n\n23    stream: result.toUIMessageStream(),\n\n24  })) {\n\n25    // Handle different part types\n\n26    uiMessage.parts.forEach(part => {\n\n27      switch (part.type) {\n\n28        case 'text':\n\n29          console.log('Text:', part.text);\n\n30          break;\n\n31        case 'tool-call':\n\n32          console.log('Tool called:', part.toolName, 'with args:', part.args);\n\n33          break;\n\n34        case 'tool-result':\n\n35          console.log('Tool result:', part.result);\n\n36          break;\n\n37      }\n\n38    });\n\n39  }\n\n40}\n```\n\n## [Resuming Conversations](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams\\#resuming-conversations)\n\nResume streaming from a previous message state:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\n```tsx\n1import { readUIMessageStream, streamText } from 'ai';\n\n2\n\n3async function resumeConversation(lastMessage: UIMessage) {\n\n4  const result = streamText({\n\n5    model: \"anthropic/claude-sonnet-4.5\",\n\n6    messages: [\\\n\\\n7      { role: 'user', content: 'Continue our previous conversation.' },\\\n\\\n8    ],\n\n9  });\n\n10\n\n11  // Resume from the last message\n\n12  for await (const uiMessage of readUIMessageStream({\n\n13    stream: result.toUIMessageStream(),\n\n14    message: lastMessage, // Resume from this message\n\n15  })) {\n\n16    console.log('Resumed message:', uiMessage);\n\n17  }\n\n18}\n```\n\nOn this page\n\n[Reading UI Message Streams](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams#reading-ui-message-streams)\n\n[Basic Usage](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams#basic-usage)\n\n[Tool Calls Integration](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams#tool-calls-integration)\n\n[Resuming Conversations](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams#resuming-conversations)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams"
      },
      {
        "title": "Message Metadata",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata",
        "content": "# [Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#message-metadata)\n\nMessage metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.\n\n## [Overview](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#overview)\n\nMessage metadata differs from [data parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data) in that it's attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.\n\n## [Getting Started](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#getting-started)\n\nHere's a simple example of using message metadata to track timestamps and model information:\n\n### [Defining Metadata Types](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#defining-metadata-types)\n\nFirst, define your metadata type for type safety:\n\napp/types.ts\n\n```tsx\n1import { UIMessage } from 'ai';\n\n2import { z } from 'zod';\n\n3\n\n4// Define your metadata schema\n\n5export const messageMetadataSchema = z.object({\n\n6  createdAt: z.number().optional(),\n\n7  model: z.string().optional(),\n\n8  totalTokens: z.number().optional(),\n\n9});\n\n10\n\n11export type MessageMetadata = z.infer<typeof messageMetadataSchema>;\n\n12\n\n13// Create a typed UIMessage\n\n14export type MyUIMessage = UIMessage<MessageMetadata>;\n```\n\n### [Sending Metadata from the Server](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#sending-metadata-from-the-server)\n\nUse the `messageMetadata` callback in `toUIMessageStreamResponse` to send metadata at different streaming stages:\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { convertToModelMessages, streamText } from 'ai';\n\n2import type { MyUIMessage } from '@/types';\n\n3\n\n4export async function POST(req: Request) {\n\n5  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n6\n\n7  const result = streamText({\n\n8    model: \"anthropic/claude-sonnet-4.5\",\n\n9    messages: await convertToModelMessages(messages),\n\n10  });\n\n11\n\n12  return result.toUIMessageStreamResponse({\n\n13    originalMessages: messages, // pass this in for type-safe return objects\n\n14    messageMetadata: ({ part }) => {\n\n15      // Send metadata when streaming starts\n\n16      if (part.type === 'start') {\n\n17        return {\n\n18          createdAt: Date.now(),\n\n19          model: 'your-model-id',\n\n20        };\n\n21      }\n\n22\n\n23      // Send additional metadata when streaming completes\n\n24      if (part.type === 'finish') {\n\n25        return {\n\n26          totalTokens: part.totalUsage.totalTokens,\n\n27        };\n\n28      }\n\n29    },\n\n30  });\n\n31}\n```\n\nTo enable type-safe metadata return object in `messageMetadata`, pass in the\n`originalMessages` parameter typed to your UIMessage type.\n\n### [Accessing Metadata on the Client](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#accessing-metadata-on-the-client)\n\nAccess metadata through the `message.metadata` property:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { DefaultChatTransport } from 'ai';\n\n5import type { MyUIMessage } from '@/types';\n\n6\n\n7export default function Chat() {\n\n8  const { messages } = useChat<MyUIMessage>({\n\n9    transport: new DefaultChatTransport({\n\n10      api: '/api/chat',\n\n11    }),\n\n12  });\n\n13\n\n14  return (\n\n15    <div>\n\n16      {messages.map(message => (\n\n17        <div key={message.id}>\n\n18          <div>\n\n19            {message.role === 'user' ? 'User: ' : 'AI: '}\n\n20            {message.metadata?.createdAt && (\n\n21              <span className=\"text-sm text-gray-500\">\n\n22                {new Date(message.metadata.createdAt).toLocaleTimeString()}\n\n23              </span>\n\n24            )}\n\n25          </div>\n\n26\n\n27          {/* Render message content */}\n\n28          {message.parts.map((part, index) =>\n\n29            part.type === 'text' ? <div key={index}>{part.text}</div> : null,\n\n30          )}\n\n31\n\n32          {/* Display additional metadata */}\n\n33          {message.metadata?.totalTokens && (\n\n34            <div className=\"text-xs text-gray-400\">\n\n35              {message.metadata.totalTokens} tokens\n\n36            </div>\n\n37          )}\n\n38        </div>\n\n39      ))}\n\n40    </div>\n\n41  );\n\n42}\n```\n\nFor streaming arbitrary data that changes during generation, consider using\n[data parts](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data) instead.\n\n## [Common Use Cases](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#common-use-cases)\n\nMessage metadata is ideal for:\n\n- **Timestamps**: When messages were created or completed\n- **Model Information**: Which AI model was used\n- **Token Usage**: Track costs and usage limits\n- **User Context**: User IDs, session information\n- **Performance Metrics**: Generation time, time to first token\n- **Quality Indicators**: Finish reason, confidence scores\n\n## [See Also](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata\\#see-also)\n\n- [Chatbot Guide](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot#message-metadata) \\- Message metadata in the context of building chatbots\n- [Streaming Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data#message-metadata-vs-data-parts) \\- Comparison with data parts\n- [UIMessage Reference](https://ai-sdk.dev/docs/reference/ai-sdk-core/ui-message) \\- Complete UIMessage type reference\n\nOn this page\n\n[Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#message-metadata)\n\n[Overview](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#overview)\n\n[Getting Started](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#getting-started)\n\n[Defining Metadata Types](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#defining-metadata-types)\n\n[Sending Metadata from the Server](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#sending-metadata-from-the-server)\n\n[Accessing Metadata on the Client](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#accessing-metadata-on-the-client)\n\n[Common Use Cases](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#common-use-cases)\n\n[See Also](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata#see-also)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata"
      },
      {
        "title": "Stream Protocols",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
        "content": "# [Stream Protocols](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#stream-protocols)\n\nAI SDK UI functions such as `useChat` and `useCompletion` support both text streams and data streams.\nThe stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.\n\nThis page describes both protocols and how to use them in the backend and frontend.\n\nYou can use this information to develop custom backends and frontends for your use case, e.g.,\nto provide compatible API endpoints that are implemented in a different language such as Python.\n\nFor instance, here's an example using [FastAPI](https://github.com/vercel/ai/tree/main/examples/next-fastapi) as a backend.\n\n## [Text Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-stream-protocol)\n\nA text stream contains chunks in plain text, that are streamed to the frontend.\nEach chunk is then appended together to form a full text response.\n\nText streams are supported by `useChat`, `useCompletion`, and `useObject`.\nWhen you use `useChat` or `useCompletion`, you need to enable text streaming\nby setting the `streamProtocol` options to `text`.\n\nYou can generate text streams with `streamText` in the backend.\nWhen you call `toTextStreamResponse()` on the result object,\na streaming HTTP response is returned.\n\nText streams only support basic text data. If you need to stream other types\nof data such as tool calls, use data streams.\n\n### [Text Stream Example](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-stream-example)\n\nHere is a Next.js example that uses the text stream protocol:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { TextStreamChatTransport } from 'ai';\n\n5import { useState } from 'react';\n\n6\n\n7export default function Chat() {\n\n8  const [input, setInput] = useState('');\n\n9  const { messages, sendMessage } = useChat({\n\n10    transport: new TextStreamChatTransport({ api: '/api/chat' }),\n\n11  });\n\n12\n\n13  return (\n\n14    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n15      {messages.map(message => (\n\n16        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n17          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n18          {message.parts.map((part, i) => {\n\n19            switch (part.type) {\n\n20              case 'text':\n\n21                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n22            }\n\n23          })}\n\n24        </div>\n\n25      ))}\n\n26\n\n27      <form\n\n28        onSubmit={e => {\n\n29          e.preventDefault();\n\n30          sendMessage({ text: input });\n\n31          setInput('');\n\n32        }}\n\n33      >\n\n34        <input\n\n35          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n36          value={input}\n\n37          placeholder=\"Say something...\"\n\n38          onChange={e => setInput(e.currentTarget.value)}\n\n39        />\n\n40      </form>\n\n41    </div>\n\n42  );\n\n43}\n```\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2\n\n3// Allow streaming responses up to 30 seconds\n\n4export const maxDuration = 30;\n\n5\n\n6export async function POST(req: Request) {\n\n7  const { messages }: { messages: UIMessage[] } = await req.json();\n\n8\n\n9  const result = streamText({\n\n10    model: \"anthropic/claude-sonnet-4.5\",\n\n11    messages: await convertToModelMessages(messages),\n\n12  });\n\n13\n\n14  return result.toTextStreamResponse();\n\n15}\n```\n\n## [Data Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#data-stream-protocol)\n\nA data stream follows a special protocol that the AI SDK provides to send information to the frontend.\n\nThe data stream protocol uses Server-Sent Events (SSE) format for improved standardization, keep-alive through ping, reconnect capabilities, and better cache handling.\n\nWhen you provide data streams from a custom backend, you need to set the\n`x-vercel-ai-ui-message-stream` header to `v1`.\n\nThe following stream parts are currently supported:\n\n### [Message Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#message-start-part)\n\nIndicates the beginning of a new message with metadata.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"start\",\"messageId\":\"...\"}\n```\n\n### [Text Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-parts)\n\nText content is streamed using a start/delta/end pattern with unique IDs for each text block.\n\n#### [Text Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-start-part)\n\nIndicates the beginning of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"text-start\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\n```\n\n#### [Text Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-delta-part)\n\nContains incremental text content for the text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"text-delta\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\",\"delta\":\"Hello\"}\n```\n\n#### [Text End Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#text-end-part)\n\nIndicates the completion of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"text-end\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\n```\n\n### [Reasoning Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#reasoning-parts)\n\nReasoning content is streamed using a start/delta/end pattern with unique IDs for each reasoning block.\n\n#### [Reasoning Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#reasoning-start-part)\n\nIndicates the beginning of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"reasoning-start\",\"id\":\"reasoning_123\"}\n```\n\n#### [Reasoning Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#reasoning-delta-part)\n\nContains incremental reasoning content for the reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"reasoning-delta\",\"id\":\"reasoning_123\",\"delta\":\"This is some reasoning\"}\n```\n\n#### [Reasoning End Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#reasoning-end-part)\n\nIndicates the completion of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"reasoning-end\",\"id\":\"reasoning_123\"}\n```\n\n### [Source Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#source-parts)\n\nSource parts provide references to external content sources.\n\n#### [Source URL Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#source-url-part)\n\nReferences to external URLs.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"source-url\",\"sourceId\":\"https://example.com\",\"url\":\"https://example.com\"}\n```\n\n#### [Source Document Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#source-document-part)\n\nReferences to documents or files.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"source-document\",\"sourceId\":\"https://example.com\",\"mediaType\":\"file\",\"title\":\"Title\"}\n```\n\n### [File Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#file-part)\n\nThe file parts contain references to files with their media type.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"file\",\"url\":\"https://example.com/file.png\",\"mediaType\":\"image/png\"}\n```\n\n### [Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#data-parts)\n\nCustom data parts allow streaming of arbitrary structured data with type-specific handling.\n\nFormat: Server-Sent Event with JSON object where the type includes a custom suffix\n\nExample:\n\n```typescript\n1data: {\"type\":\"data-weather\",\"data\":{\"location\":\"SF\",\"temperature\":100}}\n```\n\nThe `data-*` type pattern allows you to define custom data types that your frontend can handle specifically.\n\n### [Error Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#error-part)\n\nThe error parts are appended to the message as they are received.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"error\",\"errorText\":\"error message\"}\n```\n\n### [Tool Input Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#tool-input-start-part)\n\nIndicates the beginning of tool input streaming.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"tool-input-start\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\"}\n```\n\n### [Tool Input Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#tool-input-delta-part)\n\nIncremental chunks of tool input as it's being generated.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"tool-input-delta\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"inputTextDelta\":\"San Francisco\"}\n```\n\n### [Tool Input Available Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#tool-input-available-part)\n\nIndicates that tool input is complete and ready for execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"tool-input-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\",\"input\":{\"city\":\"San Francisco\"}}\n```\n\n### [Tool Output Available Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#tool-output-available-part)\n\nContains the result of tool execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"tool-output-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"output\":{\"city\":\"San Francisco\",\"weather\":\"sunny\"}}\n```\n\n### [Start Step Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#start-step-part)\n\nA part indicating the start of a step.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"start-step\"}\n```\n\n### [Finish Step Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#finish-step-part)\n\nA part indicating that a step (i.e., one LLM API call in the backend) has been completed.\n\nThis part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in `useChat` at the same time.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"finish-step\"}\n```\n\n### [Finish Message Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#finish-message-part)\n\nA part indicating the completion of a message.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\n```typescript\n1data: {\"type\":\"finish\"}\n```\n\n### [Stream Termination](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#stream-termination)\n\nThe stream ends with a special `[DONE]` marker.\n\nFormat: Server-Sent Event with literal `[DONE]`\n\nExample:\n\n```typescript\n1data: [DONE]\n```\n\nThe data stream protocol is supported\nby `useChat` and `useCompletion` on the frontend and used by default.\n`useCompletion` only supports the `text` and `data` stream parts.\n\nOn the backend, you can use `toUIMessageStreamResponse()` from the `streamText` result object to return a streaming HTTP response.\n\n### [UI Message Stream Example](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol\\#ui-message-stream-example)\n\nHere is a Next.js example that uses the UI message stream protocol:\n\napp/page.tsx\n\n```tsx\n1'use client';\n\n2\n\n3import { useChat } from '@ai-sdk/react';\n\n4import { useState } from 'react';\n\n5\n\n6export default function Chat() {\n\n7  const [input, setInput] = useState('');\n\n8  const { messages, sendMessage } = useChat();\n\n9\n\n10  return (\n\n11    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n\n12      {messages.map(message => (\n\n13        <div key={message.id} className=\"whitespace-pre-wrap\">\n\n14          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n15          {message.parts.map((part, i) => {\n\n16            switch (part.type) {\n\n17              case 'text':\n\n18                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n\n19            }\n\n20          })}\n\n21        </div>\n\n22      ))}\n\n23\n\n24      <form\n\n25        onSubmit={e => {\n\n26          e.preventDefault();\n\n27          sendMessage({ text: input });\n\n28          setInput('');\n\n29        }}\n\n30      >\n\n31        <input\n\n32          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n\n33          value={input}\n\n34          placeholder=\"Say something...\"\n\n35          onChange={e => setInput(e.currentTarget.value)}\n\n36        />\n\n37      </form>\n\n38    </div>\n\n39  );\n\n40}\n```\n\nGateway\n\nProvider\n\nCustom\n\nClaude Sonnet 4.5\n\napp/api/chat/route.ts\n\n```ts\n1import { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n2\n\n3// Allow streaming responses up to 30 seconds\n\n4export const maxDuration = 30;\n\n5\n\n6export async function POST(req: Request) {\n\n7  const { messages }: { messages: UIMessage[] } = await req.json();\n\n8\n\n9  const result = streamText({\n\n10    model: \"anthropic/claude-sonnet-4.5\",\n\n11    messages: await convertToModelMessages(messages),\n\n12  });\n\n13\n\n14  return result.toUIMessageStreamResponse();\n\n15}\n```\n\nOn this page\n\n[Stream Protocols](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#stream-protocols)\n\n[Text Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-stream-protocol)\n\n[Text Stream Example](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-stream-example)\n\n[Data Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#data-stream-protocol)\n\n[Message Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#message-start-part)\n\n[Text Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-parts)\n\n[Text Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-start-part)\n\n[Text Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-delta-part)\n\n[Text End Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#text-end-part)\n\n[Reasoning Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#reasoning-parts)\n\n[Reasoning Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#reasoning-start-part)\n\n[Reasoning Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#reasoning-delta-part)\n\n[Reasoning End Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#reasoning-end-part)\n\n[Source Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#source-parts)\n\n[Source URL Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#source-url-part)\n\n[Source Document Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#source-document-part)\n\n[File Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#file-part)\n\n[Data Parts](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#data-parts)\n\n[Error Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#error-part)\n\n[Tool Input Start Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#tool-input-start-part)\n\n[Tool Input Delta Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#tool-input-delta-part)\n\n[Tool Input Available Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#tool-input-available-part)\n\n[Tool Output Available Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#tool-output-available-part)\n\n[Start Step Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#start-step-part)\n\n[Finish Step Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#finish-step-part)\n\n[Finish Message Part](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#finish-message-part)\n\n[Stream Termination](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#stream-termination)\n\n[UI Message Stream Example](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#ui-message-stream-example)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_TVwCVPFFKzRYSpeZ5FBQVMjU1Pv4)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol"
      }
    ],
    "ai_sdk_rsc": [
      {
        "title": "AI SDK RSC",
        "title_citation": "https://ai-sdk.dev/docs/ai-sdk-rsc",
        "content": "# [AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc\\#ai-sdk-rsc)\n\nAI SDK RSC is currently experimental. We recommend using [AI SDK\\\\\nUI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\nRSC to UI, see our [migration guide](https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui).\n\n[Overview\\\\\n\\\\\nLearn about AI SDK RSC.](https://ai-sdk.dev/docs/ai-sdk-rsc/overview) [Streaming React Components\\\\\n\\\\\nLearn how to stream React components.](https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-react-components) [Managing Generative UI State\\\\\n\\\\\nLearn how to manage generative UI state.](https://ai-sdk.dev/docs/ai-sdk-rsc/generative-ui-state) [Saving and Restoring States\\\\\n\\\\\nLearn how to save and restore states.](https://ai-sdk.dev/docs/ai-sdk-rsc/saving-and-restoring-states) [Multi-step Interfaces\\\\\n\\\\\nLearn how to build multi-step interfaces.](https://ai-sdk.dev/docs/ai-sdk-rsc/multistep-interfaces) [Streaming Values\\\\\n\\\\\nLearn how to stream values with AI SDK RSC.](https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-values) [Error Handling\\\\\n\\\\\nLearn how to handle errors.](https://ai-sdk.dev/docs/ai-sdk-rsc/error-handling) [Authentication\\\\\n\\\\\nLearn how to authenticate users.](https://ai-sdk.dev/docs/ai-sdk-rsc/authentication)\n\nOn this page\n\n[AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc#ai-sdk-rsc)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/ai-sdk-rsc",
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc",
        "url_citation": "https://ai-sdk.dev/docs/ai-sdk-rsc"
      }
    ],
    "advanced": [
      {
        "title": "Advanced",
        "title_citation": "https://ai-sdk.dev/docs/advanced",
        "content": "# [Advanced](https://ai-sdk.dev/docs/advanced\\#advanced)\n\nThis section covers advanced topics and concepts for the AI SDK and RSC API. Working with LLMs often requires a different mental model compared to traditional software development.\n\nAfter these concepts, you should have a better understanding of the paradigms behind the AI SDK and RSC API, and how to use them to build more AI applications.\n\nOn this page\n\n[Advanced](https://ai-sdk.dev/docs/advanced#advanced)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/advanced",
        "url": "https://ai-sdk.dev/docs/advanced",
        "url_citation": "https://ai-sdk.dev/docs/advanced"
      }
    ],
    "reference": [
      {
        "page_title": "AI SDK Core",
        "page_title_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-core",
        "content": "# [AI SDK Core](https://ai-sdk.dev/docs/reference/ai-sdk-core\\#ai-sdk-core)\n\n[AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core) is a set of functions that allow you to interact with language models and other AI models.\nThese functions are designed to be easy-to-use and flexible, allowing you to generate text, structured data,\nand embeddings from language models and other AI models.\n\nAI SDK Core contains the following main functions:\n\n[generateText()\\\\\n\\\\\nGenerate text and call tools from a language model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text) [streamText()\\\\\n\\\\\nStream text and call tools from a language model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) [generateObject()\\\\\n\\\\\nGenerate structured data from a language model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object) [streamObject()\\\\\n\\\\\nStream structured data from a language model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-object) [embed()\\\\\n\\\\\nGenerate an embedding for a single value using an embedding model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/embed) [embedMany()\\\\\n\\\\\nGenerate embeddings for several values using an embedding model (batch embedding).](https://ai-sdk.dev/docs/reference/ai-sdk-core/embed-many) [generateImage()\\\\\n\\\\\nGenerate images based on a given prompt using an image model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image) [experimental\\_transcribe()\\\\\n\\\\\nGenerate a transcript from an audio file.](https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe) [experimental\\_generateSpeech()\\\\\n\\\\\nGenerate speech audio from text.](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech)\n\nIt also contains the following helper functions:\n\n[tool()\\\\\n\\\\\nType inference helper function for tools.](https://ai-sdk.dev/docs/reference/ai-sdk-core/tool) [createMCPClient()\\\\\n\\\\\nCreates a client for connecting to MCP servers.](https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client) [jsonSchema()\\\\\n\\\\\nCreates AI SDK compatible JSON schema objects.](https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema) [zodSchema()\\\\\n\\\\\nCreates AI SDK compatible Zod schema objects.](https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema) [createProviderRegistry()\\\\\n\\\\\nCreates a registry for using models from multiple providers.](https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry) [cosineSimilarity()\\\\\n\\\\\nCalculates the cosine similarity between two vectors, e.g. embeddings.](https://ai-sdk.dev/docs/reference/ai-sdk-core/cosine-similarity) [simulateReadableStream()\\\\\n\\\\\nCreates a ReadableStream that emits values with configurable delays.](https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-readable-stream) [wrapLanguageModel()\\\\\n\\\\\nWraps a language model with middleware.](https://ai-sdk.dev/docs/reference/ai-sdk-core/wrap-language-model) [extractReasoningMiddleware()\\\\\n\\\\\nExtracts reasoning from the generated text and exposes it as a \\`reasoning\\` property on the result.](https://ai-sdk.dev/docs/reference/ai-sdk-core/extract-reasoning-middleware) [simulateStreamingMiddleware()\\\\\n\\\\\nSimulates streaming behavior with responses from non-streaming language models.](https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-streaming-middleware) [defaultSettingsMiddleware()\\\\\n\\\\\nApplies default settings to a language model.](https://ai-sdk.dev/docs/reference/ai-sdk-core/default-settings-middleware) [smoothStream()\\\\\n\\\\\nSmooths text streaming output.](https://ai-sdk.dev/docs/reference/ai-sdk-core/smooth-stream) [generateId()\\\\\n\\\\\nHelper function for generating unique IDs](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-id) [createIdGenerator()\\\\\n\\\\\nCreates an ID generator](https://ai-sdk.dev/docs/reference/ai-sdk-core/create-id-generator)\n\nOn this page\n\n[AI SDK Core](https://ai-sdk.dev/docs/reference/ai-sdk-core#ai-sdk-core)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-core",
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core",
        "url_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-core"
      },
      {
        "page_title": "AI SDK UI",
        "page_title_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-ui",
        "content": "# [AI SDK UI](https://ai-sdk.dev/docs/reference/ai-sdk-ui\\#ai-sdk-ui)\n\n[AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui) is designed to help you build interactive chat, completion, and assistant applications with ease.\nIt is framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.\n\nAI SDK UI contains the following hooks:\n\n[useChat\\\\\n\\\\\nUse a hook to interact with language models in a chat interface.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) [useCompletion\\\\\n\\\\\nUse a hook to interact with language models in a completion interface.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion) [useObject\\\\\n\\\\\nUse a hook for consuming a streamed JSON objects.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) [convertToModelMessages\\\\\n\\\\\nConvert useChat messages to ModelMessages for AI functions.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/convert-to-model-messages) [pruneMessages\\\\\n\\\\\nPrunes model messages from a list of model messages.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/prune-messages) [createUIMessageStream\\\\\n\\\\\nCreate a UI message stream to stream additional data to the client.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-ui-message-stream) [createUIMessageStreamResponse\\\\\n\\\\\nCreate a response object to stream UI messages to the client.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-ui-message-stream-response) [pipeUIMessageStreamToResponse\\\\\n\\\\\nPipe a UI message stream to a Node.js ServerResponse object.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/pipe-ui-message-stream-to-response) [readUIMessageStream\\\\\n\\\\\nTransform a stream of UIMessageChunk objects into an AsyncIterableStream of UIMessage objects.](https://ai-sdk.dev/docs/reference/ai-sdk-ui/read-ui-message-stream)\n\n## [UI Framework Support](https://ai-sdk.dev/docs/reference/ai-sdk-ui\\#ui-framework-support)\n\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/),\n[Angular](https://angular.dev/), and [SolidJS](https://www.solidjs.com/).\n\nHere is a comparison of the supported functions across these frameworks:\n\n|  | [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) | [useCompletion](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion) | [useObject](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object) |\n| --- | --- | --- | --- |\n| React `@ai-sdk/react` |  |  |  |\n| Vue.js `@ai-sdk/vue` |  |  |  |\n| Svelte `@ai-sdk/svelte` | Chat | Completion | StructuredObject |\n| Angular `@ai-sdk/angular` | Chat | Completion | StructuredObject |\n| [SolidJS](https://github.com/kodehort/ai-sdk-solid) (community) |  |  |  |\n\n[Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\nwelcome to implement missing features for non-React frameworks.\n\nOn this page\n\n[AI SDK UI](https://ai-sdk.dev/docs/reference/ai-sdk-ui#ai-sdk-ui)\n\n[UI Framework Support](https://ai-sdk.dev/docs/reference/ai-sdk-ui#ui-framework-support)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-ui",
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui",
        "url_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-ui"
      },
      {
        "page_title": "AI SDK RSC",
        "page_title_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc",
        "content": "# [AI SDK RSC](https://ai-sdk.dev/docs/reference/ai-sdk-rsc\\#ai-sdk-rsc)\n\nAI SDK RSC is currently experimental. We recommend using [AI SDK\\\\\nUI](https://ai-sdk.dev/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\nRSC to UI, see our [migration guide](https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui).\n\n[streamUI\\\\\n\\\\\nUse a helper function that streams React Server Components on tool execution.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/stream-ui) [createAI\\\\\n\\\\\nCreate a context provider that wraps your application and shares state between the client and language model on the server.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-ai) [createStreamableUI\\\\\n\\\\\nCreate a streamable UI component that can be rendered on the server and streamed to the client.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-ui) [createStreamableValue\\\\\n\\\\\nCreate a streamable value that can be rendered on the server and streamed to the client.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-value) [getAIState\\\\\n\\\\\nRead the AI state on the server.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-ai-state) [getMutableAIState\\\\\n\\\\\nRead and update the AI state on the server.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-mutable-ai-state) [useAIState\\\\\n\\\\\nGet the AI state on the client from the context provider.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ai-state) [useUIState\\\\\n\\\\\nGet the UI state on the client from the context provider.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ui-state) [useActions\\\\\n\\\\\nCall server actions from the client.](https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-actions)\n\nOn this page\n\n[AI SDK RSC](https://ai-sdk.dev/docs/reference/ai-sdk-rsc#ai-sdk-rsc)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc",
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc",
        "url_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc"
      },
      {
        "page_title": "Untitled",
        "page_title_citation": "https://ai-sdk.dev/docs/reference/stream-helpers",
        "content": "Search…\n`⌘ K`\n\nFeedback\n\nSign in with Vercel\n\nSign in with Vercel\n\nMenu\n\nv6 (Latest)\n\nAI SDK 6.x\n\n[AI SDK by Vercel](https://ai-sdk.dev/docs/introduction)\n\n[Foundations](https://ai-sdk.dev/docs/foundations)\n\n[Overview](https://ai-sdk.dev/docs/foundations/overview)\n\n[Providers and Models](https://ai-sdk.dev/docs/foundations/providers-and-models)\n\n[Prompts](https://ai-sdk.dev/docs/foundations/prompts)\n\n[Tools](https://ai-sdk.dev/docs/foundations/tools)\n\n[Streaming](https://ai-sdk.dev/docs/foundations/streaming)\n\n[Getting Started](https://ai-sdk.dev/docs/getting-started)\n\n[Choosing a Provider](https://ai-sdk.dev/docs/getting-started/choosing-a-provider)\n\n[Navigating the Library](https://ai-sdk.dev/docs/getting-started/navigating-the-library)\n\n[Next.js App Router](https://ai-sdk.dev/docs/getting-started/nextjs-app-router)\n\n[Next.js Pages Router](https://ai-sdk.dev/docs/getting-started/nextjs-pages-router)\n\n[Svelte](https://ai-sdk.dev/docs/getting-started/svelte)\n\n[Vue.js (Nuxt)](https://ai-sdk.dev/docs/getting-started/nuxt)\n\n[Node.js](https://ai-sdk.dev/docs/getting-started/nodejs)\n\n[Expo](https://ai-sdk.dev/docs/getting-started/expo)\n\n[TanStack Start](https://ai-sdk.dev/docs/getting-started/tanstack-start)\n\n[Agents](https://ai-sdk.dev/docs/agents)\n\n[Overview](https://ai-sdk.dev/docs/agents/overview)\n\n[Building Agents](https://ai-sdk.dev/docs/agents/building-agents)\n\n[Workflow Patterns](https://ai-sdk.dev/docs/agents/workflows)\n\n[Loop Control](https://ai-sdk.dev/docs/agents/loop-control)\n\n[Configuring Call Options](https://ai-sdk.dev/docs/agents/configuring-call-options)\n\n[AI SDK Core](https://ai-sdk.dev/docs/ai-sdk-core)\n\n[Overview](https://ai-sdk.dev/docs/ai-sdk-core/overview)\n\n[Generating Text](https://ai-sdk.dev/docs/ai-sdk-core/generating-text)\n\n[Generating Structured Data](https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data)\n\n[Tool Calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling)\n\n[Model Context Protocol (MCP)](https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools)\n\n[Prompt Engineering](https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering)\n\n[Settings](https://ai-sdk.dev/docs/ai-sdk-core/settings)\n\n[Embeddings](https://ai-sdk.dev/docs/ai-sdk-core/embeddings)\n\n[Reranking](https://ai-sdk.dev/docs/ai-sdk-core/reranking)\n\n[Image Generation](https://ai-sdk.dev/docs/ai-sdk-core/image-generation)\n\n[Transcription](https://ai-sdk.dev/docs/ai-sdk-core/transcription)\n\n[Speech](https://ai-sdk.dev/docs/ai-sdk-core/speech)\n\n[Language Model Middleware](https://ai-sdk.dev/docs/ai-sdk-core/middleware)\n\n[Provider & Model Management](https://ai-sdk.dev/docs/ai-sdk-core/provider-management)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-core/error-handling)\n\n[Testing](https://ai-sdk.dev/docs/ai-sdk-core/testing)\n\n[Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry)\n\n[DevTools](https://ai-sdk.dev/docs/ai-sdk-core/devtools)\n\n[AI SDK UI](https://ai-sdk.dev/docs/ai-sdk-ui)\n\n[Overview](https://ai-sdk.dev/docs/ai-sdk-ui/overview)\n\n[Chatbot](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot)\n\n[Chatbot Message Persistence](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence)\n\n[Chatbot Resume Streams](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams)\n\n[Chatbot Tool Usage](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage)\n\n[Generative User Interfaces](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces)\n\n[Completion](https://ai-sdk.dev/docs/ai-sdk-ui/completion)\n\n[Object Generation](https://ai-sdk.dev/docs/ai-sdk-ui/object-generation)\n\n[Streaming Custom Data](https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data)\n\n[Error Handling](https://ai-sdk.dev/docs/ai-sdk-ui/error-handling)\n\n[Transport](https://ai-sdk.dev/docs/ai-sdk-ui/transport)\n\n[Reading UIMessage Streams](https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams)\n\n[Message Metadata](https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata)\n\n[Stream Protocols](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol)\n\n[AI SDK RSC](https://ai-sdk.dev/docs/ai-sdk-rsc)\n\n[Advanced](https://ai-sdk.dev/docs/advanced)\n\n[Reference](https://ai-sdk.dev/docs/reference)\n\n[AI SDK Core](https://ai-sdk.dev/docs/reference/ai-sdk-core)\n\n[AI SDK UI](https://ai-sdk.dev/docs/reference/ai-sdk-ui)\n\n[AI SDK RSC](https://ai-sdk.dev/docs/reference/ai-sdk-rsc)\n\n[Stream Helpers](https://ai-sdk.dev/docs/reference/stream-helpers)\n\n[AIStream](https://ai-sdk.dev/docs/reference/stream-helpers/ai-stream)\n\n[StreamingTextResponse](https://ai-sdk.dev/docs/reference/stream-helpers/streaming-text-response)\n\n[streamToResponse](https://ai-sdk.dev/docs/reference/stream-helpers/stream-to-response)\n\n[OpenAIStream](https://ai-sdk.dev/docs/reference/stream-helpers/openai-stream)\n\n[AnthropicStream](https://ai-sdk.dev/docs/reference/stream-helpers/anthropic-stream)\n\n[AWSBedrockStream](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-stream)\n\n[AWSBedrockAnthropicStream](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-anthropic-stream)\n\n[AWSBedrockAnthropicMessagesStream](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-messages-stream)\n\n[AWSBedrockCohereStream](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-cohere-stream)\n\n[AWSBedrockLlama2Stream](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-llama-2-stream)\n\n[CohereStream](https://ai-sdk.dev/docs/reference/stream-helpers/cohere-stream)\n\n[GoogleGenerativeAIStream](https://ai-sdk.dev/docs/reference/stream-helpers/google-generative-ai-stream)\n\n[HuggingFaceStream](https://ai-sdk.dev/docs/reference/stream-helpers/hugging-face-stream)\n\n[@ai-sdk/langchain Adapter](https://ai-sdk.dev/docs/reference/stream-helpers/langchain-adapter)\n\n[@ai-sdk/llamaindex Adapter](https://ai-sdk.dev/docs/reference/stream-helpers/llamaindex-adapter)\n\n[MistralStream](https://ai-sdk.dev/docs/reference/stream-helpers/mistral-stream)\n\n[ReplicateStream](https://ai-sdk.dev/docs/reference/stream-helpers/replicate-stream)\n\n[InkeepStream](https://ai-sdk.dev/docs/reference/stream-helpers/inkeep-stream)\n\n[AI SDK Errors](https://ai-sdk.dev/docs/reference/ai-sdk-errors)\n\n[Migration Guides](https://ai-sdk.dev/docs/migration-guides)\n\n[Troubleshooting](https://ai-sdk.dev/docs/troubleshooting)\n\nCopy markdown\n\n[AIStream\\\\\n\\\\\nCreate a readable stream for AI responses.](https://ai-sdk.dev/docs/reference/stream-helpers/ai-stream) [StreamingTextResponse\\\\\n\\\\\nCreate a streaming response for text generations.](https://ai-sdk.dev/docs/reference/stream-helpers/streaming-text-response) [streamtoResponse\\\\\n\\\\\nPipe a ReadableStream to a Node.js ServerResponse object.](https://ai-sdk.dev/docs/reference/stream-helpers/stream-to-response) [OpenAIStream\\\\\n\\\\\nTransforms the response from OpenAI's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/openai-stream) [AnthropicStream\\\\\n\\\\\nTransforms the response from Anthropic's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/anthropic-stream) [AWSBedrockStream\\\\\n\\\\\nTransforms the response from AWS Bedrock's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-stream) [AWSBedrockMessagesStream\\\\\n\\\\\nTransforms the response from AWS Bedrock Message's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-messages-stream) [AWSBedrockCohereStream\\\\\n\\\\\nTransforms the response from AWS Bedrock Cohere's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-cohere-stream) [AWSBedrockLlama-2Stream\\\\\n\\\\\nTransforms the response from AWS Bedrock Llama-2's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-llama-2-stream) [CohereStream\\\\\n\\\\\nTransforms the response from Cohere's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/cohere-stream) [GoogleGenerativeAIStream\\\\\n\\\\\nTransforms the response from Google's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/google-generative-ai-stream) [HuggingFaceStream\\\\\n\\\\\nTransforms the response from Hugging Face's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/hugging-face-stream) [LangChainStream\\\\\n\\\\\nTransforms the response from LangChain's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/langchain-adapter) [MistralStream\\\\\n\\\\\nTransforms the response from Mistral's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/mistral-stream) [ReplicateStream\\\\\n\\\\\nTransforms the response from Replicate's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/replicate-stream) [InkeepsStream\\\\\n\\\\\nTransforms the response from Inkeeps's language models into a readable stream.](https://ai-sdk.dev/docs/reference/stream-helpers/inkeep-stream)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/reference/stream-helpers",
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers",
        "url_citation": "https://ai-sdk.dev/docs/reference/stream-helpers"
      },
      {
        "page_title": "AI SDK Errors",
        "page_title_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-errors",
        "content": "# [AI SDK Errors](https://ai-sdk.dev/docs/reference/ai-sdk-errors\\#ai-sdk-errors)\n\n- [AI\\_APICallError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-api-call-error)\n- [AI\\_DownloadError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-download-error)\n- [AI\\_EmptyResponseBodyError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-empty-response-body-error)\n- [AI\\_InvalidArgumentError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-argument-error)\n- [AI\\_InvalidDataContent](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content)\n- [AI\\_InvalidDataContentError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content-error)\n- [AI\\_InvalidMessageRoleError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-message-role-error)\n- [AI\\_InvalidPromptError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-prompt-error)\n- [AI\\_InvalidResponseDataError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-response-data-error)\n- [AI\\_InvalidToolApprovalError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-approval-error)\n- [AI\\_InvalidToolInputError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error)\n- [AI\\_JSONParseError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-json-parse-error)\n- [AI\\_LoadAPIKeyError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-api-key-error)\n- [AI\\_LoadSettingError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-setting-error)\n- [AI\\_MessageConversionError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-message-conversion-error)\n- [AI\\_NoSpeechGeneratedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-speech-generated-error)\n- [AI\\_NoContentGeneratedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-content-generated-error)\n- [AI\\_NoImageGeneratedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-image-generated-error)\n- [AI\\_NoTranscriptGeneratedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error)\n- [AI\\_NoObjectGeneratedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-object-generated-error)\n- [AI\\_NoOutputSpecifiedError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-output-specified-error)\n- [AI\\_NoSuchModelError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-model-error)\n- [AI\\_NoSuchProviderError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-provider-error)\n- [AI\\_NoSuchToolError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-tool-error)\n- [AI\\_RetryError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-retry-error)\n- [AI\\_ToolCallNotFoundForApprovalError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-not-found-for-approval-error)\n- [AI\\_ToolCallRepairError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-repair-error)\n- [AI\\_TooManyEmbeddingValuesForCallError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error)\n- [AI\\_TypeValidationError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-type-validation-error)\n- [AI\\_UnsupportedFunctionalityError](https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error)\n\nOn this page\n\n[AI SDK Errors](https://ai-sdk.dev/docs/reference/ai-sdk-errors#ai-sdk-errors)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-errors",
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors",
        "url_citation": "https://ai-sdk.dev/docs/reference/ai-sdk-errors"
      }
    ],
    "migration_guides": [
      {
        "title": "Migration Guides",
        "title_citation": "https://ai-sdk.dev/docs/migration-guides",
        "content": "# [Migration Guides](https://ai-sdk.dev/docs/migration-guides\\#migration-guides)\n\n- [Migrate AI SDK 5.x to 6.0](https://ai-sdk.dev/docs/migration-guides/migration-guide-6-0)\n- [Migrate AI SDK 4.x to 5.0](https://ai-sdk.dev/docs/migration-guides/migration-guide-5-0)\n- [Migrate your data to AI SDK 5.0](https://ai-sdk.dev/docs/migration-guides/migration-guide-5-0-data)\n- [Migrate AI SDK 4.1 to 4.2](https://ai-sdk.dev/docs/migration-guides/migration-guide-4-2)\n- [Migrate AI SDK 4.0 to 4.1](https://ai-sdk.dev/docs/migration-guides/migration-guide-4-1)\n- [Migrate AI SDK 3.4 to 4.0](https://ai-sdk.dev/docs/migration-guides/migration-guide-4-0)\n- [Migrate AI SDK 3.3 to 3.4](https://ai-sdk.dev/docs/migration-guides/migration-guide-3-4)\n- [Migrate AI SDK 3.2 to 3.3](https://ai-sdk.dev/docs/migration-guides/migration-guide-3-3)\n- [Migrate AI SDK 3.1 to 3.2](https://ai-sdk.dev/docs/migration-guides/migration-guide-3-2)\n- [Migrate AI SDK 3.0 to 3.1](https://ai-sdk.dev/docs/migration-guides/migration-guide-3-1)\n\n## [Versioning](https://ai-sdk.dev/docs/migration-guides\\#versioning)\n\n- [Versioning](https://ai-sdk.dev/docs/migration-guides/versioning)\n\nOn this page\n\n[Migration Guides](https://ai-sdk.dev/docs/migration-guides#migration-guides)\n\n[Versioning](https://ai-sdk.dev/docs/migration-guides#versioning)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
        "content_citation": "https://ai-sdk.dev/docs/migration-guides",
        "url": "https://ai-sdk.dev/docs/migration-guides",
        "url_citation": "https://ai-sdk.dev/docs/migration-guides"
      }
    ],
    "troubleshooting": {
      "content": "# [Troubleshooting](https://ai-sdk.dev/docs/troubleshooting\\#troubleshooting)\n\nThis section is designed to help you quickly identify and resolve common issues encountered with the AI SDK, ensuring a smoother and more efficient development experience.\n\nReport Issues\n\nFound a bug? We'd love to hear about it in our GitHub issues.\n\n[Open GitHub Issue](https://github.com/vercel/ai/issues/new?assignees=&labels=&projects=&template=1.bug_report.yml)\n\nFeature Requests\n\nWant to suggest a new feature? Share it with us and the community.\n\n[Request Feature](https://github.com/vercel/ai/issues/new?assignees=&labels=&projects=&template=2.feature_request.yml)\n\nAsk the Community\n\nJoin our GitHub discussions to browse for help and best practices.\n\n[Ask a question](https://github.com/vercel/ai/discussions)\n\nMigration Guides\n\nCheck out our migration guides to help you upgrade to the latest version.\n\n[Migration Guides](https://ai-sdk.dev/docs/migration-guides)\n\nOn this page\n\n[Troubleshooting](https://ai-sdk.dev/docs/troubleshooting#troubleshooting)\n\nDeploy and Scale AI Apps with Vercel\n\nDeliver AI experiences globally with one push.\n\nTrusted by industry leaders:\n\n- OpenAI\n- Photoroom\n- ![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-light.c72067fe.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![leonardo-ai Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fleonardo-ai-dark.5fcd9c34.svg&w=384&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n- ![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-light.b48931cb.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)![zapier Logo](https://ai-sdk.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzapier-dark.bb119b02.svg&w=256&q=75&dpl=dpl_9Ji8w1s4RUgyaSeScmNDWLVW9sFb)\n\n[Sign Up](https://vercel.com/signup?utm_source=ai-sdk_site&utm_medium=docs_card&utm_content=sign-up)",
      "content_citation": "https://ai-sdk.dev/docs/troubleshooting",
      "url": "https://ai-sdk.dev/docs/troubleshooting",
      "url_citation": "https://ai-sdk.dev/docs/troubleshooting"
    }
  }
}